{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstmCharModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZJMjOG42dRuxJf8DibzBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/lstmCharModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzZmHEit6L7",
        "outputId": "005a72b2-4095-4b04-af7b-c1b2c57bcd84"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxq_5vKjt9PR",
        "outputId": "7a46bd96-8a8d-45de-e70b-260cb4e278a8"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8700YD7t_ua"
      },
      "source": [
        "# Manually pick cpu device if desired\n",
        "device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfK0gAeuB-0"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUn_O6vuFZV"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5P5eSNzyuJCY",
        "outputId": "bbe58bad-3ef8-46e6-f08b-4e76d52ff629"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "conv_val_data = conv_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13000\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F1344ECE590>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "QW8fDU_nuyqO",
        "outputId": "141b96e0-2ee1-48b4-e21b-425b45bfaae5"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3216222e5cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# used for stopping prediction of recurrent layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<EOS>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_train_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6VrPTAwMBp"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # since lstm is being used but there is always only 1 char, no need to worry about parsing label\n",
        "      # just make one hot vector for char and end token and put them together in tensor\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86QKpTZozE8N"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDXQCzpLO8L"
      },
      "source": [
        "def toChars(nums, dict):\n",
        "  out = []\n",
        "  for num in nums:\n",
        "    out.append(dict[num])\n",
        "  return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASlls_LuDEM"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "class convLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(convLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOt1YmJHfAE"
      },
      "source": [
        "def parsePred(pred):\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  #print(pred)\n",
        "  out = []\n",
        "  for i in pred:\n",
        "    #print(i[0])\n",
        "    out.append(labelDict[i.argmax(0).item()])\n",
        "\n",
        "  return \"\".join(out)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_69r9ZIEeO"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "7uDGucfozFod",
        "outputId": "e09f9f94-d4d7-4ca4-97ef-148a86fc7f20"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACOUlEQVR4nG2TsWuVSRTFf+fe+eKLPjGCARUjFq6uWdAgCoKgmFIDyv4J2mqxssK2wnYilrLNwqIWNlpaKIoiGEFsFBFJJxLBQkT2JeZ9M3eLee9F3ExzZ+aeOefcmTsIRwACA8OQARruDiY1L6/wRuAJGyQAQwinLoaRVM9LWEVbAgcMM8lhbESBTUwONGUGGt9k0FQigPHZB48O42A4JNt65e5uIQwkgKknsXShEgC282avd9K0atMPvMuv9hgCIRv/+1vv2oTLETiSnbix1PtrYqin2Y/lw1GgWsbST/MR97bUC5KM09/K4hHM640lZh6vtM+P+qB80vo/luLZHq/yQnYx2oVjJiTMabpn3saX31RrAuP4i3782TFEAvB9C7k83CGQI6M0pw/659fLQah1w8vclGnn+U1YZMOgs11684RwBbkk5s4ZsY2VVCAiwdTe7DPnnjZ9hRcOz/282bOeXV8uQBgYv6+0bUSONkopkSPnT7emNXgjHHUvfo0oEZEjco5cFuaSDQEC6M5s9PaXUzbWRjO5m38vXy0pB8PRgInGO2Mb1nU6l9p4t3/YOkBCfaV+yn2ClaadOSnef/HimZGHqmRCxq75Nnpn7TuG2oIaWBo7tFjaOzt81GdgqGSPqDT51y7t7cWMYhUQkCFwgjTdVSwVLEYMaaRUMPSxL5Isr3ocARSeffmf6cX7L61V4f/DEKyf3Cgb/bsfAAIHM9ywNQC1jgFqrZxw1bi2QvVQaew7iv8ALCW6vj1Z1DEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F1346AB3F10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted N2 for 4<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBJlsmLndDk"
      },
      "source": [
        "# function to find accuracy for variable size output\n",
        "def findAccuracy(pred, labels):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  #print(pred.shape)\n",
        "  #print(labels.shape)\n",
        "\n",
        "  for p, l in zip(pred, labels):\n",
        "    for b in range(len(p)):\n",
        "      #print(p[b])\n",
        "      if p[b].argmax(0).item() == l[b]:\n",
        "        accuracy += 1\n",
        "\n",
        "  return accuracy/(pred.shape[0] * pred.shape[1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knuiNPbtZq7"
      },
      "source": [
        "# NOT USED\n",
        "\n",
        "# function to define loss for variable size output\n",
        "# works like categorical crossentropy for each of the char predictions\n",
        "def lstmLoss(pred, labels, lossFunc):\n",
        "  pred = pred.permute(1,0,2)\n",
        "  #print(pred.shape)\n",
        "  labels = labels.permute(1,0)\n",
        "  #print(labels.shape)\n",
        "  loss = []\n",
        "  for char, lab in zip(pred, labels):\n",
        "    #print(char.shape)\n",
        "    #print(lab.shape)\n",
        "    loss.append(lossFunc(char, lab))\n",
        "  return loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "LJIj4ZUcngqN",
        "outputId": "45b370e4-ed40-4010-994d-1e35ce0303e9"
      },
      "source": [
        "#LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(LSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.41980 acc: 49.245%\n",
            "[1,  2000] loss: 4.47460 acc: 50.000%\n",
            "[1,  3000] loss: 4.37765 acc: 50.000%\n",
            "[1,  4000] loss: 4.25818 acc: 50.000%\n",
            "[1,  5000] loss: 4.14243 acc: 50.000%\n",
            "[1,  6000] loss: 4.02726 acc: 50.000%\n",
            "[1,  7000] loss: 3.88344 acc: 50.000%\n",
            "[2,  1000] loss: 3.46377 acc: 50.465%\n",
            "[2,  2000] loss: 3.18313 acc: 53.055%\n",
            "[2,  3000] loss: 2.87355 acc: 59.725%\n",
            "[2,  4000] loss: 2.59726 acc: 67.065%\n",
            "[2,  5000] loss: 2.32753 acc: 74.500%\n",
            "[2,  6000] loss: 2.05578 acc: 80.665%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-101441c805bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfindAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c4b405f11ce8>\u001b[0m in \u001b[0;36mfindAccuracy\u001b[0;34m(pred, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;31m#print(p[b])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "odSxfPhhDWBu",
        "outputId": "80290d52-4f36-4e8a-a044-1ebe67118499"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbd0lEQVR4nO3deZRdVZ328e9TlXmADBQhJAFiRNMIJmCYGkSaaAsoBgQBFYyKTTfSzjbQvq6GpltbXb6KtIqAoEEDMiq0C3xlbEQkkECYAggBQhJIUpB5TqV+7x/7hLopar5Vde499XzWuuue8Z5fnaSe2rXPPqcUEZiZWbHU5F2AmZl1P4e7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdKpKkOyTN7O5tzfoKeZy7dRdJ60tmhwBbgO3Z/D9GxOzer6p8kiYCC4HLI+KcvOsx6wi33K3bRMSwHS/gFeCEkmVvBrukfvlV2SWfAlYBp0ka2JsHllTbm8ez4nC4W4+TdLSkJZLOl7QM+IWkkZJ+L6le0qpsenzJPvdJ+lw2/WlJD0j6frbtS5KO6+K2EyXdL2mdpLsk/UTSr9uoXaRw/yawDTih2foZkuZLWitpoaRjs+WjJP1C0qtZHb8rra/ZZ4Skt2fTv5R0maTbJW0A/k7ShyQ9lh1jsaSLmu1/pKQHJa3O1n9a0sGSlpf+cJD0UUmPd+gfzaqew916yx7AKGBv4GzS/71fZPN7AZuAH7ex/6HAc8BuwPeAq7Lg7ey21wIPA6OBi4Az26n7SGA88BvgBuDNvn1JhwDXAP8CjACOAl7OVv+K1DX1LmB34IftHKfUJ4BvAcOBB4ANpB8wI4APAedIOjGrYW/gDuC/gTpgKjA/Ih4B3gD+vuRzz8zqtT6g2n49turVCFwYEVuy+U3AzTtWSvoWcG8b+y+KiCuzbWcBPwXGAMs6uq2kAcDBwPSI2Ao8IOm2duqeCdwREaskXQvcL2n3iFgBnAVcHRF3ZtsuzY45FjgOGB0Rq7J1/9vOcUrdGhF/zqY3A/eVrHtC0nXA+4DfkX4Q3BUR12Xr38heALOAM4A7JI0CPgh8vhN1WBVzy916S31EbN4xI2mIpMslLZK0FrgfGNFGH/ObIR4RG7PJYZ3cdk9gZckygMWtFSxpMPAxYHb2WX8hXUv4RLbJBNKF1uYmZMdZ1cK6jtipJkmHSro368JaA/wT6beStmoA+DVwgqShwKnAnyLitS7WZFXG4W69pfmwrK8B7wQOjYhdSF0aAK11tXSH14BRkoaULJvQxvYnAbsAP5W0LLteMI6mrpnFwKQW9lucHWdEC+s2kLprAJC0RwvbND9X1wK3ARMiYlfgZzSdp9ZqICKWAn8BPkrqkvlVS9tZMTncLS/DSV0zq7Mugwt7+oARsQiYC1wkaYCkw2l2gbSZmcDVwAGkvuypwBHAFEkHAFcBn5E0XVKNpHGSJmet4ztIPxRGSuovaccPr8eBd0maKmkQqd+/PcNJvwlszvr5P1GybjbwfkmnSuonabSkqSXrrwHOy76GWzpwLCsIh7vl5RJgMPA68BDwh1467ieBw0n90v8JXE8aj78TSeOA6cAlEbGs5DUvq3VmRDwMfIZ0sXQNqV997+wjziSNrnkWWAF8GSAi/gpcDNwFPE+6YNqezwMXS1oH/Bvpwi7Z570CHE/6TWglMB+YUrLvb7OaftusO8oKzjcxWZ8m6Xrg2Yjo8d8c8iJpIekmsrvyrsV6j1vu1qdk478nZd0oxwIzSKNOCknSyaQ+/HvyrsV6V7vhLulqSSskPVWybJSkOyU9n72PzJZL0qWSXpD0hKSDerJ4sy7YgzS0cD1wKXBORDyWa0U9RNJ9wGXAuRHRmHM51sva7ZbJLgStB66JiP2zZd8jXeD5jqQLgJERcb6k44EvkPoADwV+FBGH9uhXYGZmb9Fuyz0i7iddqCk1g3SDBNn7iSXLr4nkIdK45bHdVayZmXVMV+9QHVNyM8Qy0p2CkMYAl96AsSRb9pYbJySdTboNnaFDh75n8uTJXSzFzKxvmjdv3usRUdfSurIfPxARIanTQ24i4grgCoBp06bF3Llzyy3FzKxPkbSotXVdHS2zfEd3S/a+Ilu+lJ3v+BufLTMzs17U1XC/jaZbsGcCt5Ys/1Q2auYwYI2fZWFm1vva7ZbJnkB3NLCbpCWk28S/A9wg6SxgEemhRAC3k0bKvABsJN29Z2ZmvazdcI+Ij7eyanoL2wZwbrlFmZlZefw8dzOrXI2N8Mbz8OpjsHElqAZqakFK06otWVYDNf3SdE3/bLof1PZrmq7pDwOHweBRMHhkWtfZerasgU2robY/DNwFBgyDmg72cO/Yf+PK9BmbVkLdO2HEXp0/N+1wuJtZZYiA1a/Aq4/C0kdToL86H7au67ljDto1Bf2QUTBkdNP09q2waVUWwKuaXpvX8NYnMgsGDk9BP3A4DNolmx4GWzdm+65Mgb55NTS/Wfj478Mh/9DtX5rD3cza1rgdtm6AbRvT+47Xtg3QsCWFVeP29L7jVTrf2ADbt6XAbNyWTWfz27elZateToG+8fV0zNoBMGZ/mHIa7HkQjDsIho9t5Rjbd57fvi0ds/RVumzLuhS0G99oCt1NK2H9cljxbJquHZBa9oNHpLAfPSnNDxqRve+a6t6yDjavTe9b1qbw37IufR2rXoL+Q9L+u45r+sExeOTO06NafBx/2RzuZtWiYQu8sRA21Gev17P3FSXT9SlgojG1hCN2DsRoBLLlO7oydureKJmPLNQbNrdbWpfUDkivmn4puN/xQdjzQBj3HhjzLug3sGeO20c43M0q1brlsORhWDwHFj+Suim2N3v0vGph6G4wtC69j9wntS5raoEd/dIqCfFsGpq1gOOtLWDVwIChTa/+Q1L/8oAh2fzQFMBv/pCobblPvKY2C/L+WZj3b9rGeozD3SxvEam1vXoRLH44vZY8nLoqIAXingemftk9D4The2RhXpcFuZ/cbW/lcDfrbo3bm0ZCbHyjqX934+s7d59sqIcNb6T3xm1N+w8bAxMOgYM/BxMOhbFT3EVhneZwN2tPRBrlsD7r316/fOfpDa83hfimbIjbW0ZUZPoPaepG2WUc7DGlZH4sjJuWhsW5y8LK5HA3K7V5bRqKt2RuGr2x7ElYvyyN7Giupl9TX/eQ3VIovzkKIhtaN6RkZMTQutRXbdYLHO7Wd23fBisWZEE+L73qn+PNVvfot6fukV3Hw7DdYeju6X3H9OCR7u+2iuVwt75jfX02+uRhWPJIapk3bErrhoxOXSL7n5zGVO95UGptm1Uph7sV0/YGWPF0U5AvfjjdVAJpKN7Yd8N7Pg3jp6XXiL3dz22F4nC36rd1AyxfAMuegOVPpX7y5U+nOyohjT4ZfzBM+2zqZhk7FfoPyrdmsx7mcLfq8/ID8MpDWYg/le7a3NFPPmhXGHMAHPSpFOjjD/boE+uTHO5WXR6+Em7/epoeuQ/scQAccGp632N/2HWCg9wMh7tVk+fugDvOg3ceDyf9LLXSzaxFDnerDksfhZs+m+7WPPnnHi9u1g4P0rXKt/oVuPa0dKPQx693sJt1gFvuVtk2rYbZH0uPu535PzB8TN4VmVUFh7tVroatcP0ZaTTMmbfA7pPzrsisajjcrTJFwP98EV7+E5x0OUw8Ku+KzKqK+9ytMt33HXj8Ojj6GzDl9LyrMas6DnerPPOvhf/9Dkz9JLzvvLyrMatKDnerLC/eB7d9ASa+Dz58iW9IMusih7tVjoX3wPVnwuh94bRfQb8BeVdkVrUc7pa/CJhzBfz6lPTs9DNu8t2nZmXyaBnL1/Zt6ZECc6+GdxwHJ18JA4fnXZVZ1XO4W342roQbZ8JL98MRX4LpF0JNbd5VmRWCw93yUf9XuO40WLMETrwMpn4i74rMCsXhbr3vhbvhxs+kC6Yzfw97HZp3RWaF4wuq1nsi4KGfwexTYMQE+Id7HOxmPcQtd+sdjY3pj2zMvQomfzg9UmDgsLyrMisst9ytdzzwgxTsf/tFOPVXDnazHuaWu/W85++Ce/4TDvgYfOBi33Vq1gvccreetfIluPksGLM/nHCpg92sl5QV7pK+IulpSU9Juk7SIEkTJc2R9IKk6yX5HvK+auvG9Dx2SI8TGDAk33rM+pAuh7ukccAXgWkRsT9QC5wOfBf4YUS8HVgFnNUdhVqV2fE89uVPw8lXwaiJeVdk1qeU2y3TDxgsqR8wBHgNOAa4KVs/CzixzGNYNXroMnjyRjjmm7Dv+/OuxqzP6XK4R8RS4PvAK6RQXwPMA1ZHREO22RJgXEv7Szpb0lxJc+vr67tahlWilx+AP34zDXk88qt5V2PWJ5XTLTMSmAFMBPYEhgLHdnT/iLgiIqZFxLS6urqulmGVZs1SuPHTMHpSeqxAja/Zm+WhnO+89wMvRUR9RGwDbgGOAEZk3TQA44GlZdZo1aJhC9xwJmzbDKfNhkG75F2RWZ9VTri/AhwmaYgkAdOBBcC9wCnZNjOBW8sr0arG7f8CS+fBSZdB3TvyrsasTyunz30O6cLpo8CT2WddAZwPfFXSC8Bo4KpuqNMq3bxfwqOz4L1fg785Ie9qzPq8su5QjYgLgQubLX4ROKScz7Uqs/JFuP08mDQd/u7/5F2NmeE7VK07/OEbUNsfZvzEf2zDrEI43K08f/0j/PUOeN95sMvYvKsxs4zD3bquYQv84XwYvS8cek7e1ZhZCT8V0rruLz9O/e1n3JL+qpKZVQy33K1r1iyF+7+f7kJ9+/S8qzGzZhzu1jV//CZEI3zw23lXYmYtcLhb5730J3j6FjjyKzBy77yrMbMWONytc7Y3wB3nwYi94Igv5V2NmbXCF1Stcx75OaxYkJ4d039w3tWYWSvccreOW18P934bJh0Dkz+UdzVm1gaHu3Xc3RfBto1w3Pf8t1DNKpzD3TpmyVx47Ndw2Dmw2755V2Nm7XC4W/saG+H2r8OwPdJjBsys4vmCqrXvsV/Bq4/BR6+EgcPzrsbMOsAtd2vb1o1wz3/AXofDAR/Luxoz6yC33K1t82fDhno49RpfRDWrIm65W+u2N8CDl8L4Q1LL3cyqhsPdWrfgd7D6FTjyy261m1UZh7u1LAIeuAR2eye847i8qzGzTnK4W8sW3g3Ln4Qjvgg1/m9iVm38XWst+/OPYPhYj5Axq1IOd3urpY/CS/fDYZ+HfgPzrsbMusDhbm/150tg4K7wnk/nXYmZdZHD3Xb2xkJYcBscfBYM2iXvasysixzutrMHL4XaAekBYWZWtRzu1mTdcph/HUz9BAzbPe9qzKwMDndrMucy2L4V/vYLeVdiZmVyuFuyeS08cjXs9xEYPSnvasysTA53S+b9ErasgSO+nHclZtYNHO4GDVvgoZ/CxKNg3EF5V2Nm3cDhbvDEDbDuNbfazQrE4d7XNTamRw3scQBMOibvasysmzjc+7rnboc3nk+tdj/W16wwHO59WQQ88EMYsTfsd2Le1ZhZNyor3CWNkHSTpGclPSPpcEmjJN0p6fnsfWR3FWvd7OlbYOlceO/XoNZ/cdGsSMptuf8I+ENETAamAM8AFwB3R8S+wN3ZvFWabZvgzgthzAFw4Bl5V2Nm3azL4S5pV+Ao4CqAiNgaEauBGcCsbLNZgH/fr0QP/hjWLIZj/wtqavOuxsy6WTkt94lAPfALSY9J+rmkocCYiHgt22YZMKalnSWdLWmupLn19fVllGGdtvZVeOAH8DcnwMT35l2NmfWAcsK9H3AQcFlEHAhsoFkXTEQEEC3tHBFXRMS0iJhWV1dXRhnWaXdfDI0N8IH/yLsSM+sh5YT7EmBJRMzJ5m8ihf1ySWMBsvcV5ZVo3WrJPHj8uvRXlkZNzLsaM+shXQ73iFgGLJb0zmzRdGABcBswM1s2E7i1rAqt+0TAHy6AobvDUV/Puxoz60Hljn/7AjBb0gDgReAzpB8YN0g6C1gEnFrmMay7PHUzLHkYPvJjGDg872rMrAeVFe4RMR+Y1sKq6eV8rvWArRvhzn+DPd6d/hiHmRWa71zpKx78b1i7FD56pYc+mvUBfvxAX7BmKfz5EthvBuxzRN7VmFkvcLj3BXf/OzRuhw9cnHclZtZLHO5Ft/gReOJ6OPxcGLlP3tWYWS9xuBfZjqGPw8bAe7+adzVm1ot8QbXInrwxPfVxxk899NGsj3HLvagW/QVu/zqMnQpTPp53NWbWyxzuRbTgVrhmBgytg1NnQY3/mc36Gn/XF82cy+GGmTB2Cnz2j76IatZHuc+9KBob4a4L4cFLYfKH4eSfQ//BeVdlZjlxuBdBwxa49dx0AfXgz8Fx3/NdqGZ9nMO92m1eA9efAS/dD9MvhCO/AlLeVZlZzhzu1WztqzD7Y1D/LJx0OUw5Pe+KzKxCONyrUcMWWPoo3Py51HL/5I0w6Zi8qzKzCuJwr2SNjbB6EaxYkF7Ls/c3Xkh/Jm/YGPjM7TD23XlXamYVprjhvr0Bls6DhfdA/TPpVvyK0ayWlmpbtyx1t2xd37RsxN6w+34w+UPp/W1Hw9DderJQM6tSxQr3lS+mMF94b7rAuGUtqAZGTYKaCvtS33LRs9n8kFEw9ZMwZj/Y/V2w+2Q/QsDMOqzCEq+TNq1OIb7wnvRavSgt33UveNdJqR964lEpKM3M+pDqDvc5l8N934YBw1KI/+0XUqCPepuHA5pZn1bd4T7l9BTq46dBbf+8qzEzqxjVHe4j904vMzPbiR8cZmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczK6Cyw11SraTHJP0+m58oaY6kFyRdL2lA+WWamVlndEfL/UvAMyXz3wV+GBFvB1YBZ3XDMczMrBPKCndJ44EPAT/P5gUcA9yUbTILOLGcY5iZWeeV23K/BDgPaMzmRwOrI6Ihm18CjGtpR0lnS5oraW59fX2ZZZiZWakuh7ukDwMrImJeV/aPiCsiYlpETKurq+tqGWZm1oJy/obqEcBHJB0PDAJ2AX4EjJDUL2u9jweWll+mmZl1Rpdb7hHxrxExPiL2AU4H7omITwL3Aqdkm80Ebi27SjMz65SeGOd+PvBVSS+Q+uCv6oFjmJlZG8rplnlTRNwH3JdNvwgc0h2fa2ZmXeM7VM3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkBdDndJEyTdK2mBpKclfSlbPkrSnZKez95Hdl+5ZmbWEeW03BuAr0XEfsBhwLmS9gMuAO6OiH2Bu7N5MzPrRV0O94h4LSIezabXAc8A44AZwKxss1nAieUWaWZmndMtfe6S9gEOBOYAYyLitWzVMmBMK/ucLWmupLn19fXdUYaZmWXKDndJw4CbgS9HxNrSdRERQLS0X0RcERHTImJaXV1duWWYmVmJssJdUn9SsM+OiFuyxcsljc3WjwVWlFeimZl1VjmjZQRcBTwTET8oWXUbMDObngnc2vXyzMysK/qVse8RwJnAk5LmZ8u+AXwHuEHSWcAi4NTySjQzs87qcrhHxAOAWlk9vaufa2Zm5fMdqmZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZAfVIuEs6VtJzkl6QdEFPHMPMzFrX7eEuqRb4CXAcsB/wcUn7dfdxzMysdT3Rcj8EeCEiXoyIrcBvgBk9cBwzM2tFvx74zHHA4pL5JcChzTeSdDZwdja7XtJzXTzebsDrXdw3L665d1RbzdVWL7jm3tJazXu3tkNPhHuHRMQVwBXlfo6kuRExrRtK6jWuuXdUW83VVi+45t7SlZp7oltmKTChZH58tszMzHpJT4T7I8C+kiZKGgCcDtzWA8cxM7NWdHu3TEQ0SPpn4P8BtcDVEfF0dx+nRNldOzlwzb2j2mqutnrBNfeWTtesiOiJQszMLEe+Q9XMrIAc7mZmBVTV4V6NjzmQ9LKkJyXNlzQ373paIulqSSskPVWybJSkOyU9n72PzLPGUq3Ue5Gkpdl5ni/p+DxrbE7SBEn3Slog6WlJX8qWV+R5bqPeij3PkgZJeljS41nN/54tnyhpTpYb12cDPypCGzX/UtJLJed5arsfFhFV+SJdrF0IvA0YADwO7Jd3XR2o+2Vgt7zraKfGo4CDgKdKln0PuCCbvgD4bt51tlPvRcDX866tjZrHAgdl08OBv5Ie11GR57mNeiv2PAMChmXT/YE5wGHADcDp2fKfAefkXWsHav4lcEpnPquaW+5+zEEPiYj7gZXNFs8AZmXTs4ATe7WoNrRSb0WLiNci4tFseh3wDOnu7oo8z23UW7EiWZ/N9s9eARwD3JQtr5hzDG3W3GnVHO4tPeagov+zZQL4o6R52SMYqsWYiHgtm14GjMmzmA76Z0lPZN02FdG90RJJ+wAHklppFX+em9ULFXyeJdVKmg+sAO4k/ba/OiIask0qLjea1xwRO87zt7Lz/ENJA9v7nGoO92p1ZEQcRHpq5rmSjsq7oM6K9DtjpY+hvQyYBEwFXgP+b77ltEzSMOBm4MsRsbZ0XSWe5xbqrejzHBHbI2Iq6U75Q4DJOZfUruY1S9of+FdS7QcDo4Dz2/ucag73qnzMQUQszd5XAL8l/YerBssljQXI3lfkXE+bImJ59k3SCFxJBZ5nSf1JQTk7Im7JFlfseW6p3mo4zwARsRq4FzgcGCFpxw2cFZsbJTUfm3WLRURsAX5BB85zNYd71T3mQNJQScN3TAN/DzzV9l4V4zZgZjY9E7g1x1ratSMgMydRYedZkoCrgGci4gclqyryPLdWbyWfZ0l1kkZk04OBD5CuFdwLnJJtVjHnGFqt+dmSH/giXSNo9zxX9R2q2bCrS2h6zMG3ci6pTZLeRmqtQ3r0w7WVWLOk64CjSY8ZXQ5cCPyONMpgL2ARcGpEVMRFzFbqPZrUVRCkEUr/WNKXnTtJRwJ/Ap4EGrPF3yD1Y1fceW6j3o9ToedZ0rtJF0xrSQ3ZGyLi4uz78Dek7o3HgDOyFnHu2qj5HqCONJpmPvBPJRdeW/6sag53MzNrWTV3y5iZWSsc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAvr/LB0Lrq3m4cgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOuTSe6DnvB"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKIgoqbDw43",
        "outputId": "07d81870-038a-4563-fa71-a68c57947e2b"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.14879 acc: 88.275%\n",
            "[2,  1000] loss: 1.15740 acc: 88.100%\n",
            "[3,  1000] loss: 1.14069 acc: 88.100%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "YNzAl0F9EBqz",
        "outputId": "70487e40-fbf0-4462-d677-ec6f15e5bdd7"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVHwMT7H0wf"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsCornerDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      image = image * cv2.cornerHarris(image, 2, 3, 0.04)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVR__AEdH2VL"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "cornerTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdEzjgrH5oS"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "CornerLSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "ifHMtLREIDt1",
        "outputId": "d8e8edec-fb1b-41e7-d1b5-1ea7b0bd7422"
      },
      "source": [
        "testItem, testLabel = next(iter(cornerTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABRklEQVR4nI2TPU4DMRCFP292owWiDVEkOv5SgkKHuMM2SFtF4q9JhRDiCFwAkChyAFoaJERFAaKAloKWI9DSDoXXXs8aCaax38ybN/aMTQYJFHAOQMk2MD8lw9kA+sCIEh4oa++Q0A5gLHLGFKk9br0GuAu5wmQHeJwgVKfWl8MqIDararbswhbGCUrtlkBt0W6+BbFG2wwYjM+wHBGvkwOkHKokqQlSKwCvgfSv1UZRaXePy5mFS+0a4Y1SlsPoSh8fAhDbBtOkG7XufcbllcCtQiBXLQJdFee91e5U5/PSrnAMCaZhPKOncbSuBfLorCcUCg8iRk+huSi6kEQpdt52lr1ux81TTMgBfG8z03h9kxXDFWkeVvDE/muzP+LDtqNSKOGL+wDv87QJb0F/swT4EBAYFwA3oCbAWt6BjQuavhf+k8MPi65y/Dl+f5kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F414365AB50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted U<EOS> for U<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrgUswNJbJ9",
        "outputId": "c481b633-7822-445a-cd71-1e5c5286df0e"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "LSTMtestLoss = []\n",
        "LSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      LSTMtestLoss.append(running_loss)\n",
        "      LSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.56157 acc: 48.400%\n",
            "[1,  2000] loss: 4.47824 acc: 50.000%\n",
            "[1,  3000] loss: 4.39765 acc: 50.000%\n",
            "[1,  4000] loss: 4.30606 acc: 50.000%\n",
            "[1,  5000] loss: 4.19796 acc: 50.000%\n",
            "[1,  6000] loss: 4.07397 acc: 50.000%\n",
            "[1,  7000] loss: 3.94622 acc: 50.030%\n",
            "[2,  1000] loss: 3.68766 acc: 50.540%\n",
            "[2,  2000] loss: 3.55134 acc: 51.310%\n",
            "[2,  3000] loss: 3.40052 acc: 53.245%\n",
            "[2,  4000] loss: 3.23160 acc: 56.195%\n",
            "[2,  5000] loss: 3.06545 acc: 59.390%\n",
            "[2,  6000] loss: 2.87635 acc: 62.235%\n",
            "[2,  7000] loss: 2.66348 acc: 65.705%\n",
            "[3,  1000] loss: 2.30184 acc: 71.535%\n",
            "[3,  2000] loss: 2.09118 acc: 74.880%\n",
            "[3,  3000] loss: 1.90122 acc: 78.030%\n",
            "[3,  4000] loss: 1.70102 acc: 80.930%\n",
            "[3,  5000] loss: 1.53972 acc: 83.295%\n",
            "[3,  6000] loss: 1.37654 acc: 85.670%\n",
            "[3,  7000] loss: 1.23783 acc: 87.500%\n",
            "[4,  1000] loss: 1.00445 acc: 90.135%\n",
            "[4,  2000] loss: 0.89318 acc: 91.390%\n",
            "[4,  3000] loss: 0.82866 acc: 91.710%\n",
            "[4,  4000] loss: 0.74795 acc: 92.360%\n",
            "[4,  5000] loss: 0.68584 acc: 92.930%\n",
            "[4,  6000] loss: 0.63772 acc: 93.425%\n",
            "[4,  7000] loss: 0.58845 acc: 93.590%\n",
            "[5,  1000] loss: 0.49379 acc: 94.715%\n",
            "[5,  2000] loss: 0.46836 acc: 94.775%\n",
            "[5,  3000] loss: 0.43274 acc: 95.055%\n",
            "[5,  4000] loss: 0.41549 acc: 95.290%\n",
            "[5,  5000] loss: 0.39713 acc: 95.315%\n",
            "[5,  6000] loss: 0.37941 acc: 95.330%\n",
            "[5,  7000] loss: 0.36549 acc: 95.495%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "w7yE-KSGJstq",
        "outputId": "670656bd-fca6-4200-a6a1-422925619d14"
      },
      "source": [
        "testAcc = testAcc[:35]\n",
        "print(len(testAcc))\n",
        "print(len(LSTMtestAcc))\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\", label=\"Raw Data\")\n",
        "ax.plot(LSTMtestAcc, color=\"tab:blue\", label=\"Corner Detection\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUZdr/8c+V3oCEkISQUCK9JnQVgiiWtSI20NVld11ZxfroquizruWnq+u6ojwqiqtiQUHEuoq7IKCgLhgk9BpqKEkgJCSkz9y/P84hBEyfJFNyvV+vec2ZM6dcOUm+ObnnPvcRYwxKKaV8i5+7C1BKKdX0NNyVUsoHabgrpZQP0nBXSikfpOGulFI+SMNdKaV8kIa78kgislBEJjf1skq1FqL93FVTEZHCKi/DgFLAYb/+ozFmTstX5ToRSQIygNeMMbe5ux6l6kPP3FWTMcZEnHgAe4HLq8yrDHYRCXBflY3yG+AoMFFEgltyxyLi35L7U75Dw101OxEZKyKZIvKgiBwC3hKRKBH5l4jkiMhRezqxyjrLROQP9vRvRWSFiDxnL7tLRC5u5LJJIvKdiBSIyGIReVlE3quldsEK9z8D5cDlp70/XkTSReSYiGSIyK/s+e1F5C0ROWDX8WnV+k7bhhGRHvb0bBGZKSJfichx4FwRuVRE1tj72Ccij522/mgR+UFE8uz3fysiw0Ukq+ofBxG5SkTW1uubpryehrtqKR2B9kBXYArWz95b9usuQDHwUi3rjwS2Ah2AZ4E37OBt6LLvA6uAaOAx4KY66h4NJAJzgQ+ByrZ9ERkBvAPcD0QCY4Dd9tvvYjVN9Qdigel17KeqG4CngDbACuA41h+YSOBS4DYRudKuoSuwEPg/IAZIAdKNMT8BR4ALq2z3Jrte1Qp427/Hyns5gUeNMaX262JgwYk3ReQpYGkt6+8xxrxuL/s28AoQBxyq77IiEgQMB8YZY8qAFSLyeR11TwYWGmOOisj7wHciEmuMyQZuBt40xiyyl91v7zMeuBiINsYctd/7to79VPWZMeZ7e7oEWFblvXUi8gFwDvAp1h+CxcaYD+z3j9gPgLeBG4GFItIeuAiY2oA6lBfTM3fVUnKMMSUnXohImIi8JiJ7ROQY8B0QWUsbc2WIG2OK7MmIBi7bCcitMg9gX00Fi0gocC0wx97Wj1ifJdxgL9IZ64PW03W293O0mvfq45SaRGSkiCy1m7DygVux/iuprQaA94DLRSQcuA5Ybow52MialJfRcFct5fRuWfcBvYGRxpi2WE0aADU1tTSFg0B7EQmrMq9zLctPANoCr4jIIfvzggRONs3sA7pXs94+ez+R1bx3HKu5BgAR6VjNMqcfq/eBz4HOxph2wKucPE411YAxZj/wI3AVVpPMu9Utp3yThrtylzZYTTN5dpPBo829Q2PMHiANeExEgkTkLE77gPQ0k4E3gYFYbdkpwCggWUQGAm8AvxORcSLiJyIJItLHPjteiPVHIUpEAkXkxB+vtUB/EUkRkRCsdv+6tMH6T6DEbue/ocp7c4DzReQ6EQkQkWgRSany/jvAA/bX8HE99qV8hIa7cpcXgFDgMPBf4OsW2u+vgbOw2qWfBOZh9cc/hYgkAOOAF4wxh6o8Vtu1TjbGrAJ+h/VhaT5Wu3pXexM3YfWu2QJkA/cAGGO2AU8Ai4HtWB+Y1mUq8ISIFAB/wfpgF3t7e4FLsP4TygXSgeQq635i1/TJac1RysfpRUyqVRORecAWY0yz/+fgLiKSgXUR2WJ316Jajp65q1bF7v/d3W5G+RUwHqvXiU8Skaux2vCXuLsW1bLqDHcReVNEskVkQ5V57UVkkYhst5+j7PkiIjNEZIeIrBORIc1ZvFKN0BGra2EhMAO4zRizxq0VNRMRWQbMBG43xjjdXI5qYXU2y9gfBBUC7xhjBtjznsX6gOcZEZkGRBljHhSRS4A7sdoARwIvGmNGNutXoJRS6hfqPHM3xnyH9UFNVeOxLpDAfr6yyvx3jOW/WP2W45uqWKWUUvXT2CtU46pcDHEI60pBsPoAV70AI9Oe94sLJ0RkCtZl6ISHhw/t06dPI0tRSqnWafXq1YeNMTHVvefy8APGGCMiDe5yY4yZBcwCGDZsmElLS3O1FKWUalVEZE9N7zW2t0zWieYW+znbnr+fU6/4S7TnKaWUakGNDffPOXkJ9mTgsyrzf2P3mjkTyNexLJRSquXV2Sxjj0A3FuggIplYl4k/A3woIjcDe7AGJQL4CqunzA6gCOvqPaWUUi2sznA3xlxfw1vjqlnWALe7WpRSSinX6HjuSinP5XTCke1wYA0U5YL4gZ8/iFjT4l9lnh/4BVjTfoH2dAD4B5yc9guE4AgIbQ+hUdZ7Da2nNB+K88A/EILbQlAE+NWzhfvE+kW51jaKcyGmN0R2afixqYOGu1LKMxgDeXvhwM+w/2cr0A+kQ1lB8+0zpJ0V9GHtISz65LSjDIqP2gF89OSjJJ9fjsgsENzGCvrgNhDS1p6OgLIie91cK9BL8uD0i4UveQ5G3NLkX5qGu1Kqdk4HlB2H8iLr+cSj/DhUlFph5XRYzyceVV87K8BRbgWms9yetl87yq15R3dbgV502NqnfxDEDYDkidBpCCQMgTbxNezDceprR7m1z6qPqvNKC6ygLTpyMnSLc6EwC7K3WNP+QdaZfWikFfbR3a3XIZH2czur7tICKDlmPZces8K/tMD6Oo7ugsAwa/12CSf/cIRGnTrdvtrh+F2m4a6Ut6gohSMZcDzHfhy2n7OrTOdYAWOc1pmwMacGonEC9vwTTRmnNG9UeW3sUK8oqbO0RvEPsh5+AVZw97oIOg2GhKEQ1x8CgptnvzUoq3BSWFpBYUkFZQ7rODkNOI3BVHk2Boz93onhWwzWfLCXsV+XVjgoLnNQXH7yuajEQUmB9bqo3MH4ZBh5RtN/PRruSnmqgizIXAX7VsK+n6xmCsdpQ8+LP4R3gPAY6zmqm3V26ecPnGiXliohbk/DaWfA5pdnwOIHQeEnH4FhVvtyUJj9OtwK4Mo/Ev7Vt4n7+dtBHmiHeeDJZZqAMVbQOpyG46UV5BeXk1dcbj0XlXGsuJy8ovLK+QUl5RSUVFBYWkFByYlHOaUVLTe2mgiEBvoTFuTPkC5RNMcAXBruSrmbMdbZdt4e2LfKemSuspoqwArEToOtdtlOg6FNRzvMY+wg982Ru0vKHazec5Tl2w+zYkcOh/JLcDgNFU5T+ey0n+sjLMifdqGBtA0JpE1IAO3Dg+gaHU5EcABtQwKICA6gTUgAESGBBPoLfiKIgJ8IfgJgPYsIgv1PDlJ5w0Ph5Hsi1nvBgX6EBvoTGuRfGeYhgf4EB/ghTfTHrSYa7ko1NafjZE+IoiMn23eLDp/afHI8B44fsZ6d5SfXj4iDziNg+B+g80iIT27xJgp3MMawPbuQ77blsHz7YVbuOkJJuZMAP2Fo1ygu6NexMnQD/AR/f/tZBH8/P/z9ICwogMiwQNqFBlY+twsNol1oIEEBvvlHsCYa7krVxRirl0Oh3b5dmHXq9PHDJ0O82O7i9oseFbbAsJPNKG0ToGNyldfxkDDM6hbXzGd1LaW4zMHRorJT2pyLyxwUlTkoKbfboMscbD54jOXbc8g6ZjU7dY8JZ9LwLqT27MDIM6KJCNaoaig9YkpVVXLM6oqXmWb13ji0HgoPWT07TucXcLKtO6yDFcqVvSDsrnVhVXpGhMdYbdU+whhDXlE5+/OKyTxazP68Yg7kFbO/yvSR49Uct2pEhgUyqkcHxvTswOieMSREhjZz9b5Pw121Xo5yyN5kB/lq65Gzlcqz7ugeVvNIu0SIiIXwWOv5xHRolM+2d5/gdBoOHithz5Hj7DlSxJ4jRezNPc7uw0XszS2isLTilOVDAv1IiAwlISqMAQntSIwKJTo8qLLNOTToZLuz1QYdQGigP21CAvDz843/VjyFhrtqPQpz7N4nqyDzJ+vMvKLYei8s2moSGXC11ae60xDrbLuVOFZSTkZ2ITuyC8nIOU5GTiE7cwrZd7SYsiq9SAL9hc5RYXSJDmNEUnsSo0JJjAolITKMhKhQosICm/2DQlU/Gu7KNzkqIHvjySDft8q6qASsrnjxg2DobyFxmPWI7Ooz7dynczgNeUVlHC0qI/d4ObnHyziUX0xGznE7zAvJLjjZxTLQX+gWHU6P2AjO7xtHl+gwukWH06V9GJ0iQ/HXM2yvoOGuvF/ZccjaBIfWQdYGq508a6N1RSVYvU8Sh8Ow31vNLPEpEBji3pqbmDGGrVkFLN6URfq+PHKPl3G0yAryYyXlVHer5DbBAXSPjWBMrxi6x0TQIzaC7jFWiAf4+3ZzU2ug4a68z+4VsPe/dohvsK7aPNFOHtIO4gbCkN9YgZ443Kd6n1RV7nCyalcuizZlsXhzFplHrSamnrERxLYNplNkKO3Dg4gMC6J9WCBR4UG0Dw8iKiyI2LbBxEQEaxOKD9NwV95l1evw1Z+s6ahu0HEgDLzOeu44ANp19skgPyG/uJxlW7NZvDmbZVuzKSipIDjAj9E9OnD7uT0Y1yeW2La+9V+JahwNd+U9ti6EhQ9A70tgwqvWWXorUVBSzrNfb+WDVXupcBo6RARxyYB4zu8Xx+geHQgN8nd3icrDaLgr77D/Z/jo99bVmlf/06f6i9dl0aYsHvl0A1kFJdwwogtXD00kJTFSuw6qWmm4K8+Xtxfen2hdKHT9vFYT7NkFJTz++Sa+XH+QPh3bMPPGIQzuEuXuspSX0HBXnq04D+Zcaw13O/kLaBPn7oqanTGG+WmZPPnlJkoqnNx/UW+mjDmDQO3BohpAw115rooymHej1Rvmpo8hto+7K2p2uw8f5+FP1vNDxhFGJLXn6asG0j0mwt1lKS+k4a48kzHwxV2wezlMeA2Sxri7omZVVuHkze93MX3RNoL8/fjrhIFMGt5Z29VVo2m4K8+07BlY+wGMfRiSJ7m7mmaTV1TGnJV7eefH3WQdK+Wi/nE8MX4AcdqdUblIw115nvT34dtnIOXXcM4D7q6mWezMKeSt73fz0epMissdjO7RgWevSeacXjHuLk35CA135Vl2LoPP74Skc+CyF3zqgiRjDP/dmcsbK3bxzZYsAv38GJ/Sid+PTqJvfFt3l6d8jIa78hwZS+DDyRDdEya+CwFB7q6oSVQ4nHyx7gD/XL6LjQeO0T48iDvP68mNZ3Yhto02v6jmoeGu3M8Ya1iBr6dBTG/49Xyfufp0xfbDPPGvjWzLKqRHbARPXzWQCYMTCAnUK0pV89JwV+7lKLeGFEh7E3pdDFe/DsFt3F2Vy3YfPs5TX21m0aYsurQP49Ubh3Bhv47a+0W1GA135T5FuTB/Muz6DkbdDeMeBT/vPqMtKCnnpaU7eGvFbgL9hQd/1Yffj+5GcIB3f13K+2i4K/fI2QYfTIT8TLhyJqTc4O6KXOJ0Gj5ancmz/97K4cJSrh2ayP0X9dYRGpXbaLirlrfjG5j/O+sD08n/gi4j3V2RS9J25/L4F5tYvz+fIV0ieWPyMJI7R7q7LNXKabirlmMMrHwN/v0QxPaD6z+wbqThpfbnFfPMwi18sfYA8e1CeHFSClckd9IbYCiPoOGuWobTad1kI+0N6HOZNaRAsHeOmVJUVsGr3+5k1ncZGAN3jevJreecQViQ/jopz6E/japlrHjeCvaz74LzHwc/7xvh0BjDZ+kHeGbhFg4dK+Hy5E5Mu7gPCZGh7i5NqV/QcFfNb/tiWPIkDLwWLnjCK686Td+Xx+NfbGTN3jwGJrTjpRsGM6xbe3eXpVSNNNxV88rdBQtuhrgBcPkMrwv2Q/klPPv1Fj5es5+YNsH8/ZpBXD0kUfurK4/nUriLyP8Af8C69fx64HdAPDAXiAZWAzcZY8pcrFN5o7Iiazx2sIYTCApzbz0NYIzhvZV7efqrzVQ4DVPHdmfquT2ICNbzIeUdGv2TKiIJwF1AP2NMsYh8CEwCLgGmG2PmisirwM3AzCapVnmPE+OxZ22EX38E7ZPcXVG95RSU8uCCdSzZkk1qzw48deVAukR7zx8mpcD1ZpkAIFREyoEw4CBwHnDiipS3gcfQcG99/jsT1s+H8x6Bnue7u5p6W7wpiwcXrKOgtILHLu/Hb87qpk0wyis1OtyNMftF5DlgL1AM/AerGSbPGFNhL5YJJFS3vohMAaYAdOnivX2dVTV2r4D//Nnq8jj6XndXUy9FZRU8+eVm3l+5l77xbflgUgq94rx/jBvVernSLBMFjAeSgDxgPvCr+q5vjJkFzAIYNmyYaWwdysPk74f5v4Xo7tawAl7Q5XFdZh73zE1n15Hj/HHMGdx7YS8dC0Z5PVeaZc4HdhljcgBE5GNgFBApIgH22XsisN/1MpVXqCiFD2+C8hL47RwI8ewbUDichpnLdvDC4u3EtAlmzh9Gcnb3Du4uS6km4Uq47wXOFJEwrGaZcUAasBS4BqvHzGTgM1eLVF7iq/th/2qY+B7E9HJ3NbU6kFfM3XPX8NPuo1w2KJ6nrhxIu7BAd5elVJNxpc19pYh8BPwMVABrsJpZvgTmisiT9rw3mqJQ5eFWz4af34bU+6Dv5e6uplbfbsvhnrlrKHcYpk9M5sqUBB0PRvkcl3rLGGMeBR49bfZOYIQr21VeJncnfPUAdB8H5/6vu6upkcNpePGb7fzfku30jmvDK78ewhkx3jm+jVJ10SsylOu+fhj8A2H8yx57s43DhaXcMzedFTsOc/WQRJ68cgChQZ5Zq1JNQcNduWbbf2DbQmvMmLbx7q6mWmm7c7nj/TXkFpXxt6sHct2wztoMo3yehrtqvIpS+PpBiO4JI29zdzW/YIzhjRW7eGbhFhKiQvlk6tn07+QbN95Wqi4a7qrxfnzJam+/8WPrrkoe5FhJOffPX8u/N2ZxUf84/n5tMm1DtDeMaj003FXj5O+H756zrkLtMc7d1Zxi66EC/vhuGvuOFvPnS/ty8+gkbYZRrY6Gu2qc//wZjBMu+qu7KznF1xsOce+H6YQHBzB3ypkM1zHXVSul4a4abtdy2PgxjH0Iorq6uxoAnE7DjCXbeWHxdpI7R/LajUPp2C7E3WUp5TYa7qphHBWw8AHrxtaj7nZ3NQAUllZw34fp/HtjFlcNSeCvEwYSEqjdHFXrpuGuGuanf0L2Jpg4BwLdf+/QvUeKuOWdNLZnF/DIZf34/ahu2r6uFBruqiEKc2DpX6H7edDnUndXw/c7DnP7+z9jDLz9+xGk9oxxd0lKeQwNd1V/3zwG5UVw8bNuvReqMYa3vt/NU19tpntMOK//Zhhdo8PdVo9SnkjDXdVPZhqseQ/Ovgs69HRbGQ6nYdqCdcxfncmF/eJ4fmKK3tdUqWrob4Wqm9MJX/0JIjrCOQ+4tZRXv81g/upM7jyvB/9zfi+9BZ5SNdBwV3Vb8y4cWANXvQ7B7rv13LrMPKYv2sZlg+K594Je+sGpUrXw/HugKfcqK4Il/w+6nAUDr3VbGUVlFdwzN53YNsE8deVADXal6qBn7qp26XPgeA5c945bP0R98svN7DpynDl/GKl3TFKqHvTMXdXMUQE/zIDEEdaZu5ss2pTF+yv3MiX1DL3HqVL1pOGuarbpU8jbC6PvcdtZe3ZBCQ8uWEe/+Lbce6Fn35dVKU+i4a6qZwyseAE69IZeF7upBMP989dxvLSCFyelEBygQwooVV8a7qp6Gd9A1noYdRf4uefH5J0f9/DtthwevqQvPePc10tHKW+k4a6q9/2L0CbebT1ktmcV8NevNjO2dwy/OcszRp5UyptouKtf2v8z7PoOzpwKAcEtvvvSCgd3z7XGZH/2mkHa7VGpRtCukOqXvn8BgtvB0N+6ZffP/2cbmw4e4/XfDCO2jY7JrlRj6Jm7OtWRDNj0OQy/GULatvjuf8g4zKzlO7l+RBcu6BfX4vtXyldouKtT/TAD/IPgzNtafNe7Dh/n3nlrSYoO55HL+rb4/pXyJdoso04qyIL0DyDlBoiIbdFd/5hxhFvfW42fwD8nDyMsSH80lXKF/gapk1bOBEcZnH1ni+527qq9/PnTDXTrEM4bk3VsdqWagoa7spQcg5/ehH5XQHT3Ftmlw2n429dbmPXdTlJ7duDlXw+hbYiOG6NUU9BwV5bVs6E0H0bd0yK7O15awd1z01m8OYubzuzKo5f3I8BfPwJSqqlouCuoKIX/vgJJYyBhSLPv7kBeMTe/ncbWQ8d4/Ir+TD67W7PvU6nWRsNdwboPoeAgjH+52XeVvi+PW95Jo7jMwZu/Hc7Y3i37wa1SrYWGe2vndFpDDXQcCN3Pa9Zd/WvdAe77cC0xbYKZ84eR9NLxYpRqNhrurd3Wr+DIdrj6jWYb1vd4aQVPfrmJD1btY2jXKF67aSgdIlp+WAOlWhMN99bMGFgxHSK7Qr8rm2UXabtzuffDtew7WsQfzzmDey/opUP3KtUCXAp3EYkE/gkMAAzwe2ArMA/oBuwGrjPGHHWpStU8Nn4M+9Pg8hng37R/58sqnExfvI3Xvs2gU2Qo86acxYik9k26D6VUzVzte/Yi8LUxpg+QDGwGpgHfGGN6At/Yr5WnKS+GRY9C3EAYfGOTbnrroQLGv/w9M5dlcO3Qznx9zxgNdqVaWKNP10SkHTAG+C2AMaYMKBOR8cBYe7G3gWXAg64UqZrBDy9B/j64cib4NU0zicNpeGPFTp779zbahgbw+m+G6eBfSrmJK/+LJwE5wFsikgysBu4G4owxB+1lDgHV/naLyBRgCkCXLl1cKEM12LEDsOJ56Hs5JKU2ySb35RZx3/y1rNqVy4X94nj6qoFE64emSrmNK80yAcAQYKYxZjBwnNOaYIwxBqst/heMMbOMMcOMMcNiYmJcKEM12DdPgLMCLvh/TbK5f288xCUvLmfTgWP8/ZpBvHbTUA12pdzMlTP3TCDTGLPSfv0RVrhniUi8MeagiMQD2a4WqZpQ5mpY+4E1zED7JJc2VeFw8vf/bOW1b3cyKLEdL98whM7tw5qoUKWUKxod7saYQyKyT0R6G2O2AuOATfZjMvCM/fxZk1SqXGcMfD0NwmNhzJ9c2lROQSl3fbCGH3ce4YaRXXj08n7axVEpD+Jq/7c7gTkiEgTsBH6H1dTzoYjcDOwBrnNxH6qpbFgAmavgipcguPFXh67ek8vUOT+TV1TOc9cmc83QxCYsUinVFFwKd2NMOjCsmrfGubJd1QzKimDRX6DjIOtmHI1gjGH2D7t56svNJESF8snUEfTr1PK34lNK1U2vUG0tfvg/OLYfrnq9UV0fj5dWMO3j9Xyx9gDn943lH9el0C5Ux15XylNpuLcG+fvh+xeg33joNqrBq+/ILuS291aTkVPI/Rf15rZzuuPn1zzj0CilmoaGe2vwzePgdMAFTzRoNWMM81dn8tjnGwkN9Ofdm0cyqkeHZipSKdWUNNx93b6fYN08GH0vRHWr92r5ReU8/Ol6vlx3kDPPaM/0iSnEtwttvjqVUk1Kw92Xnej6GBEHqffWe7VVu3K5Z+4asgtKuf+i3tx6Tnf8tRlGKa+i4e7L1s+3Rn0c/0q9uj6WO5zM+GY7Ly/dQZf2YSy47WySO0e2QKFKqaam4e6r9vwIX/0J4lMg+fo6F997pIi7561hzd48rhmayGNX9CciWH88lPJW+tvrizZ9BgtugcjOcN3b4Ff7EEKfrMnkkU83IgIv3TCYywZ1aqFClVLNRcPd16x8DRY+CInD4fq5EB5d46JbDxXwwuJtLNxwiOHdopg+MYXEKB0bRilfoOHuK5xOWPwo/DAD+lwGV/8TAqvv3bJ6Ty6vLM3gmy3ZhAX586cLe3Hb2B76oalSPkTD3RdUlMJnt1sfoA7/A1z87C+uQjXGsGxrDjOXZbBqdy5RYYH8z/m9mHx2VyLDgtxUuFKquWi4e7uSfJh3I+z6DsY9CqP/B+TkGXiFw8mX6w8yc1kGWw4V0KldCI9e3o+JwzsTFqTffqV8lf52e7NjB2DOtZCzBSa8BsmTADhcWMrmg8dYl5nP3J/2si+3mB6xETx3bTLjUzoR6O/qrXOVUp5Ow90bVZTC/p9xfnQLu4uC2HTmB2w62IVNP69i04FjZBeUVi46uEskj1zaj/P7xul4MEq1IhrunszppCBrJ/t3byUzcy+Z2UfYn1dCZpE/mc4OZJgnKCIYlpQT4LeTHrERjO7ZgX7xbekX35a+8W2JCtf2dKVaI98Nd0cFuTt+4pMfNrA6y1H9jVzdxBhwAk4EYwQnnHxGcBohr0zILIsgn3B7rS5AF4KlgsSwchLaBnFdYif6dY2jX3xbesZF6J2QlFKVfCvcc3fi3LGEFembmbevLYvKkymjE139jxAsDndXdwo/MfhhPUQMfoBg8LMin5hAJ0OiK0joYEiM70Ri1zNIiO1Ah4ggRLR5RSlVO+8O9+I8q5dIxhIObFvN/KPd+bBiLPs5j8iAMm7s48fEscn07qa3gVNKtS5eHe5lP85i8dJFzHOez3eOBzEIqV3DeOjs3lzQP06bKZRSrZZXh/tLhWOZUd6f+HbB3DmsC9cOTaRze718XimlvDrcrx09gCG9C0ntGaOXziulVBVeHe6d24fpmbpSSlVDL1VUSikfpOGulFI+SMNdKaV8kIa7Ukr5IA13pZTyQRruSinlgzTclVLKB2m4K6WUD9JwV0opH6ThrpRSPkjDXSmlfJCGu1JK+SANd6WU8kEuh7uI+IvIGhH5l/06SURWisgOEZknInqHZqWUamFNceZ+N7C5yuu/AdONMT2Ao8DNTbAPpZRSDeBSuItIInAp8E/7tQDnAR/Zi7wNXOnKPpRSSjWcq2fuLwAPAE77dTSQZ4ypsF9nAgnVrSgiU0QkTUTScnJyXCxDKaVUVY0OdxG5DMg2xqxuzPrGmFnGmGHGmGExMTGNLUMppVQ1XLnN3ijgChG5BAgB2gIvApEiEmCfvScC+10vUymlVEM0+szdGPOQMSbRGNMNmAQsMcb8GlgKXGMvNhn4zOUqlVJKNUhz9HN/ELhXRHZgtcG/0Qz7UEopVQtXmmUqGWOWAcvs6Z3AiKbYrlJKqcbRK1SVUi5nZkcAAA9qSURBVMoHabgrpZQP0nBXSikfpOGulFI+SMNdKaV8kIa7Ukr5IA13pZTyQRruSinlgzTclVLKB2m4K6WUD9JwV0opH6ThrpRSPkjDXSmlfJCGu1JK+SANd6WU8kEa7kop5YM03JVSygdpuCullA/ScFdKKR+k4a6UUj5Iw10ppXyQhrtSSvkgDXellPJBGu5KKeWDNNyVUsoHabgrpZQP0nBXSikfpOGulFI+SMNdKaV8kIa7Ukr5IA13pZTyQRruSinlgzTclVLKB2m4K6WUD2p0uItIZxFZKiKbRGSjiNxtz28vIotEZLv9HNV05SqllKoPV87cK4D7jDH9gDOB20WkHzAN+MYY0xP4xn6tlFKqBTU63I0xB40xP9vTBcBmIAEYD7xtL/Y2cKWrRSqllGqYJmlzF5FuwGBgJRBnjDlov3UIiKthnSkikiYiaTk5OU1RhlJKKZvL4S4iEcAC4B5jzLGq7xljDGCqW88YM8sYM8wYMywmJsbVMpRSSlXhUriLSCBWsM8xxnxsz84SkXj7/Xgg27USlVJKNZQrvWUEeAPYbIx5vspbnwOT7enJwGeNL08ppVRjBLiw7ijgJmC9iKTb8x4GngE+FJGbgT3Ada6VqJRSqqEaHe7GmBWA1PD2uMZuVymllOtcOXNvVuXl5WRmZlJSUuLuUlQzCAkJITExkcDAQHeXopRP8thwz8zMpE2bNnTr1g2reV/5CmMMR44cITMzk6SkJHeXo5RP8tixZUpKSoiOjtZg90EiQnR0tP5XplQz8thwBzTYfZh+b5VqXh4d7koppRpHw70W/v7+pKSkMGDAAC6//HLy8vKaZfv9+/cnOTmZf/zjHzidzlrX2b17N++//36T1qGU8j0a7rUIDQ0lPT2dDRs20L59e15++eVm2f7GjRtZtGgRCxcu5PHHH691HQ13pVR9eGxvmVMsnAaH1jftNjsOhIufqffiZ511FuvWrQNg1apV3H333ZSUlBAaGspbb71F7969ufTSS3n66acZNGgQgwcPZsKECfzlL3/hL3/5C507d+aWW26pcfuxsbHMmjWL4cOH89hjj7Fnzx5uuukmjh8/DsBLL73E2WefzbRp09i8eTMpKSlMnjyZCRMmVLucUqp1845wdzOHw8E333zDzTffDECfPn1Yvnw5AQEBLF68mIcffpgFCxaQmprK8uXL6dq1KwEBAXz//fcALF++nFdffbXO/Zxxxhk4HA6ys7OJjY1l0aJFhISEsH37dq6//nrS0tJ45plneO655/jXv/4FQFFRUbXLKaVaN+8I9wacYTel4uJiUlJS2L9/P3379uWCCy4AID8/n8mTJ7N9+3ZEhPLycgBSU1OZMWMGSUlJXHrppSxatIiioiJ27dpF7969G7Tv8vJy7rjjDtLT0/H392fbtm0uLaeUal20zb0WJ9rE9+zZgzGmss39kUce4dxzz2XDhg188cUXlf21hw8fTlpaGsuXL2fMmDEMHjyY119/naFDh9Zrfzt37sTf35/Y2FimT59OXFwca9euJS0tjbKysmrXqe9ySqnWRcO9HsLCwpgxYwb/+Mc/qKioID8/n4SEBABmz55duVxQUBCdO3dm/vz5nHXWWaSmpvLcc88xZsyYOveRk5PDrbfeyh133IGIkJ+fT3x8PH5+frz77rs4HA4A2rRpQ0FBQeV6NS2nlGrdNNzrafDgwQwaNIgPPviABx54gIceeojBgwdTUVFxynKpqanExsYSGhpKamoqmZmZpKamVrvNE80+/fv35/zzz+fCCy/k0UcfBWDq1Km8/fbbJCcns2XLFsLDwwEYNGgQ/v7+JCcnM3369BqXU0q1bmLdLMm9hg0bZk7/EHDz5s307dvXTRWplqDfY6VcIyKrjTHDqntPz9yVUsoHabgrpZQP0nBXSikfpOGulFI+SMNdKaV8kIa7Ukr5IA33Whw6dIhJkybRvXt3hg4dyiWXXOLWy/tnz55NTEwMgwcPpmfPnlx00UX88MMPda736aefsmnTpkbtMy8vj1deeaXy9YEDB7jmmmsatS2lVMvRcK+BMYYJEyYwduxYMjIyWL16NU8//TRZWVn1Wv/0i5sao7ptTJw4kTVr1rB9+3amTZvGVVddxebNm2vdTlOGe6dOnfjoo48atS2lVMvxioHDHv9iI5sOHGvSbfbr1JZHL+9f4/tLly4lMDCQW2+9tXJecnIyYAX/Aw88wMKFCxER/vznPzNx4kSWLVvGI488QlRUFFu2bGHWrFk89thjdOjQgQ0bNjB06FDee+89RITVq1dz7733UlhYSIcOHZg9ezbx8fGMHTuWlJQUVqxYwfXXX899991XY43nnnsuU6ZMYdasWUyfPp2MjAxuv/12cnJyCAsL4/XXXyc3N5fPP/+cb7/9lieffJIFCxYA/GK5Pn36kJWVxa233srOnTsBmDlzJjNmzCAjI4OUlBQuuOACbr/9di677DI2bNhASUkJt912G2lpaQQEBPD8889z7rnnMnv2bD7//HOKiorIyMhgwoQJPPvss03xbVNK1ZNXhLs7nAjj6nz88cekp6ezdu1aDh8+zPDhwyvHj/n555/ZsGEDSUlJLFu2jDVr1rBx40Y6derEqFGj+P777xk5ciR33nknn332GTExMcybN4///d//5c033wSgrKys3sP2DhkyhNdeew2AKVOm8Oqrr9KzZ09WrlzJ1KlTWbJkCVdccQWXXXZZZXPKuHHjql3urrvu4pxzzuGTTz7B4XBQWFjIM888w4YNG0hPTwesm4Wc8PLLLyMirF+/ni1btnDhhRdWNlulp6ezZs0agoOD6d27N3feeSedO3du+DdCKdUoXhHutZ1hu8OJs2p/f3/i4uI455xz+Omnn2jbti0jRowgKSmpctkRI0aQmJgIQEpKCrt37yYyMpINGzZUDiHscDiIj4+vXGfixIn1ruXE8BGFhYX88MMPXHvttZXvlZaW/mL52pZbsmQJ77zzDmDdArBdu3YcPXq01uNw5513AtYY9127dq0M93HjxtGuXTsA+vXrx549ezTclWpBXhHu7tC/f/9GtS2fPnBXcHBw5bS/vz8VFRUYY+jfvz8//vhjvbZRmzVr1tC3b1+cTieRkZGVZ9g1qe9yrqru61ZKtRz9QLUG5513HqWlpcyaNaty3rp161i+fDmpqanMmzcPh8NBTk4O3333HSNGjKj3tnv37k1OTk5luJeXl7Nx48YG1/jtt98ya9YsbrnlFtq2bUtSUhLz588HrDP6tWvXAqcOE1zbcuPGjWPmzJmA9d9Efn7+L4YYrio1NZU5c+YAsG3bNvbu3dvgm5IopZqHhnsNRIRPPvmExYsX0717d/r3789DDz1Ex44dmTBhAoMGDSI5OZnzzjuPZ599lo4dO9Z720FBQXz00Uc8+OCDJCcnk5KSUq8ujQDz5s0jJSWFXr168de//pUFCxZUjqw4Z84c3njjDZKTk+nfvz+fffYZAJMmTeLvf/87gwcPJiMjo8blXnzxRZYuXcrAgQMZOnQomzZtIjo6mlGjRjFgwADuv//+U2qZOnUqTqeTgQMHMnHiRGbPnn3KGbtSyn10yF/lNvo9Vso1OuSvUkq1MhruSinlgzw63D2hyUg1D/3eKtW8PDbcQ0JCOHLkiIaADzLGcOTIEUJCQtxdilI+y2P7uScmJpKZmUlOTo67S1HNICQkpPLiLqVU0/PYcA8MDDzlSk+llFL11yzNMiLyKxHZKiI7RGRac+xDKaVUzZo83EXEH3gZuBjoB1wvIv2aej9KKaVq1hxn7iOAHcaYncaYMmAuML4Z9qOUUqoGzdHmngDsq/I6Exh5+kIiMgWYYr8sFJGtjdxfB+BwI9d1F625ZXhbzd5WL2jNLaWmmrvWtILbPlA1xswCZtW5YB1EJK2my289ldbcMrytZm+rF7TmltKYmpujWWY/UHXg7kR7nlJKqRbSHOH+E9BTRJJEJAiYBHzeDPtRSilVgyZvljHGVIjIHcC/AX/gTWNMwwcrrz+Xm3bcQGtuGd5Ws7fVC1pzS2lwzR4x5K9SSqmm5bFjyyillGo8DXellPJBXh3u3jjMgYjsFpH1IpIuIml1r9HyRORNEckWkQ1V5rUXkUUist1+jnJnjVXVUO9jIrLfPs7pInKJO2s8nYh0FpGlIrJJRDaKyN32fI88zrXU67HHWURCRGSViKy1a37cnp8kIivt3Jhnd/zwCLXUPFtEdlU5zil1bswY45UPrA9rM4AzgCBgLdDP3XXVo+7dQAd311FHjWOAIcCGKvOeBabZ09OAv7m7zjrqfQz4k7trq6XmeGCIPd0G2IY1XIdHHuda6vXY4wwIEGFPBwIrgTOBD4FJ9vxXgdvcXWs9ap4NXNOQbXnzmbsOc9BMjDHfAbmnzR4PvG1Pvw1c2aJF1aKGej2aMeagMeZne7oA2Ix1dbdHHuda6vVYxlJovwy0HwY4D/jInu8xxxhqrbnBvDncqxvmwKN/2GwG+I+IrLaHYPAWccaYg/b0ISDOncXU0x0iss5utvGI5o3qiEg3YDDWWZrHH+fT6gUPPs4i4i8i6UA2sAjrv/08Y0yFvYjH5cbpNRtjThznp+zjPF1EguvajjeHu7cabYwZgjVq5u0iMsbdBTWUsf5n9PQ+tDOB7kAKcBD4h3vLqZ6IRAALgHuMMceqvueJx7maej36OBtjHMaYFKwr5UcAfdxcUp1Or1lEBgAPYdU+HGgPPFjXdrw53L1ymANjzH77ORv4BOsHzhtkiUg8gP2c7eZ6amWMybJ/SZzA63jgcRaRQKygnGOM+die7bHHubp6veE4Axhj8oClwFlApIicuIDTY3OjSs2/spvFjDGmFHiLehxnbw53rxvmQETCRaTNiWngQmBD7Wt5jM+Byfb0ZOAzN9ZSpxMBaZuAhx1nERHgDWCzMeb5Km955HGuqV5PPs4iEiMikfZ0KHAB1mcFS4Fr7MU85hhDjTVvqfIHX7A+I6jzOHv1Fap2t6sXODnMwVNuLqlWInIG1tk6WEM/vO+JNYvIB8BYrGFGs4BHgU+xehl0AfYA1xljPOJDzBrqHYvVVGCweij9sUpbttuJyGhgObAecNqzH8Zqx/a441xLvdfjocdZRAZhfWDqj3Ui+6Ex5gn793AuVvPGGuBG+4zY7WqpeQkQg9WbJh24tcoHr9Vvy5vDXSmlVPW8uVlGKaVUDTTclVLKB2m4K6WUD9JwV0opH6ThrpRSPkjDXSmlfJCGu1JK+aD/D0r2eSsgpF26AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR2cb85uGhEm"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "cornerVal_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhV9wHjEGGNx",
        "outputId": "5349f2b9-eba7-4606-d2be-68a3f770127e"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerVal_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.25047 acc: 86.995%\n",
            "[2,  1000] loss: 1.24245 acc: 86.915%\n",
            "[3,  1000] loss: 1.24736 acc: 86.915%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "wGU8SBiRGT8m",
        "outputId": "161a322c-e867-4139-f067-45d2e7aa5305"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "k56MY3j5mDtY",
        "outputId": "5a7e0243-68a3-4e54-daf7-907c0b0b3b63"
      },
      "source": [
        "# experiment with effects of FAST  detection\n",
        "print(\"original image\")\n",
        "input = np.float32(conv_train_data[61345][0])\n",
        "#input = cv2.cvtColor(input, cv2.COLOR_GRAY2RGB)\n",
        "cv2_imshow(input)\n",
        "#print(input.shape)\n",
        "corners = cv2.goodFeaturesToTrack(input,25,0.01,10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "out = np.zeros((32,32), dtype=float)\n",
        "for i in corners:\n",
        "  x,y = i.ravel()\n",
        "  cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "print(\"corner detection\")\n",
        "cv2_imshow(out)\n",
        "\n",
        "combo = np.concatenate([input,out],1)\n",
        "print(\"concatenated\")\n",
        "cv2_imshow(np.float32(combo))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACWklEQVR4nG1TTUiUURQ95773fTPmP5IUQSHRJjNqkUJEhFi5yiAYatcmomgRZCAkBG2CfqgWgVCLQGvZKohIESrCXdBgkS0UQVCxwvydme+92+J9I4nztvdw3jnnngtUeAYEYcFKQ0khAgCkbAGQBAUABVKZwoBChpmtMCcMYetoRbbypxq3nbz7PlcFsLLOzNGXqwVdO08BTIUv6p7Mq3fq38WpDpAGAAEBTNw9uqbqE6/56gAgARghIWR8oH+xpMnCs1ze52tDLITYMlPdxbl11T+vO+PsA83XUEw5OQoQof7Ksvdu8kYj0DzuxqsAgQEkjSPTdm9Fk7Xn7TENuxb1S0MwKQAIi57BuRWXzPQ2AxD0azIgJAjrIaqSRN05sf7bcn0GpD99KeEHT0AZdGLH7RnV1cc3EzfYujtz4kdJv+4CIBJ8snWopE5HsmeLyfrk2NPZRH/3GZCQkEPtI3VO13vihhfOe02c/3W1KfUHANH1laJzOtQo2PlGk8Ls/NjlCALChB+2T2iiWrwACHqd/364rUUgwaAAtNe806JOHycs2qb15z4ClpJmTBxacKo6dYyIwMytVT/ctJEeBJb3vXq/NFgrgAFbFkq+rz0SA5OWU15550p34nT51Q+dc3Nvc3v2dzalFeka/Th8rqrcDuydLnmnycSUP8Wwrig2WdgNAM98+qve+aXPHVYAguIEHtS0cp7VRxql4+DASKGAqAQDRGmvAkEEgUW2RiAhSJbPbQNCIWjDFg1gyP8xUl4BAWPSKkM2kQhBMJwEARhsup9wAwQAC+IffyXYbeG0Qp0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4143543D50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corner detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAANklEQVR4nGNgGAWUgP9wFhMO+f9YxbGZQG+AzWYmFPnB63bCAK/bmHD77j8WFor8/6EcKqQCADKlEPb+mSiDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4143543BD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "concatenated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACoUlEQVR4nJ2VT0gVURTGv+/cmXnP/I8kRVBItMmMWqQQESFWrjIIpHZtIooWQQZCQtAm6A/VIhBqEWgtWwURKUJFuAt6WGQLRRBUrDD/Pt/ce1rMe/q0p46e5cyc33z3nO+cCxQIA4LwwEIvNwrJIgQASNk0gCQoACiQrUkwoJBRrreFfMIQXhk9kc3rj8JsO3n3fWsRwK3VMXH05Vxa589TALP5dCl7MqHOqnsXMObvSQOAgAAmaO6bV3Wh01RxPAAJwAgJIYMDHVMZDSeftaZcqhTI2eK/0DwAxMuhyi6OL6j+ed0YJB9oqoSyVglUlwmR4wTwUX5lxjk7dKMSqB6wA0WArFXFPAWQrF0SdfdmNZx/Xh/QsGlKv1TEa6IAIDy0dI3P2nC0rRqAoEPDTiFjEDwHUZXQb24Vz32bKU+AdKcvhfzgCCh1Q4SAwI7bo6pzj2+Gtqt2d+LEj4x+3QVglZkLw0iwtjujVnuTZxfDhaH+p2Oh/m43IFd2Mb/2K/JR+kit1YWWoOKFdU5D635drcr2Z2MFgH99dtFa7a4U7HyjYXpsov+yDwFjjQKJ7YMaqi5eAARt1n0/XFcjkKhBMSaa3jVndVFHjhMe6kb05z4CHmXZo+vn49CkVdXhY4QPJm7NuZ6qJXfFUODxvlPnprtKBTBgzWTGtdf7YmCyy3HN2mVDXjlrM3eCbL2LH1prx9+27tnfWBU9Kdy9PG5T38eec0VLw793JOOshoPD7lS0I9ZQkOMa+IFJwlsC8Mynv+qsm/7c4K1bgSifoFiBQ8704lh8pFIaDnb2ptPwM+ufH4AB/Oxeiog+BB6SJQJBrHnm6l6RQtCLpiyGFw2Zz5DcCBAwJoYCiW7SfBFCEDQ5fRtLWPlZtOMJINYF/w8Z8+1Yf7dcRQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7F4143543D50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O47MxWFyVMSZ"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsGFTTDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      corners = cv2.goodFeaturesToTrack(image,25,0.01,10)\n",
        "      if (corners is not None):\n",
        "        corners = np.int0(corners)\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "        for i in corners:\n",
        "          x,y = i.ravel()\n",
        "          cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "      else:\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "\n",
        "      combo = np.concatenate([image,out],1)\n",
        "      #cv2_imshow(combo)\n",
        "      image = torch.tensor(combo)\n",
        "      #print(image.shape)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      #image = self.transform(image)\n",
        "      #print(image.shape)\n",
        "      #cv2_imshow(np.float32(image))\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      #T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g07HWPEHXJ53"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsGFTTDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "GFTTTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)\n",
        "GFTTLSTMModel = convLSTM(40, 10, 2).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lKe4Rj7RXW6E",
        "outputId": "856b4f59-11ad-4d5b-b9e5-5a95145dba27"
      },
      "source": [
        "testItem, testLabel = next(iter(GFTTTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "#print(testItem[0].cpu().numpy()[0].shape)\n",
        "image = testItem[0].cpu().numpy()\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACVUlEQVR4nJ2VO2gVURCGv5mzuV4hEkWNikoCISLEyiog2AhqqY1YaKc2VhYWdkIawUdlChEkYgqxUvAJQXxjo6DYBCNRRPD6AF/xavbsjMXeoEk2680dWDgs/P/5Z+Y/M/A3RAHJP2G+oQhokh9J5o0HggoQWoECIKCiCgEJzfNM0yrmhFULs9dOVoLxwgJprkF7bnwa6y+9390L/+ctWHbNs+xQKK1iMR5BhJ4z3838QXuZgiKcgKBK57GYeRZHO0VQQZqyg4rm+tFFl+ruqbntaN5QKuZWwYQ2FgxsrdQHb0qdCjgBT+bKdxqFai53+b4v0YcXnTB/1SUqTXoqwQAjmO4+viA9d2PFSonWtcYz/fjGHPm/BBVA+65er3k28eT929Sjx5+TMRvtIcmLUxz/UoeOi5a6e/SYunk0z1KrbyxNouGnhJAh1dO7JHHX4CoIE8+jXBiPL7ESAplytBK2PJ50i7XbIyNXDtQyH9uUCFLYwIKKKGweN3d7uF5U6a/Z5JFqIBQSFD+D6llzzx5tI8CKRxbfrEN0DgfNxifKL5Wonw+8EBO6u8WGxtuiaXH6RbTSO2b2YXsgKCsfZ/HrTqSsezNDOfrbbX8+Cfd+8x8H25RK80NVTZZUoiwNAqHvcLvdHU4hnWHAMjsqJ6PHWwGl407q93tl9hucawY1GDYMPjvfK4GuU/W0tqe4fSV4IWHtYgUdyH7d66/Od6MkYLwFpbL46dDldy2thYSgQHV1mE/7pkIU8uHV0joDlKAN57a21jQ0JARaWcooSD6UWoLzB0Z95A3m70YaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7F414365A750>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-324-34643acf8563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# predict twice with same image but different hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-dbd35ecf87f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# resnet layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 1, 2, 2], but got 3-dimensional input of size [10, 32, 64] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "CB_1EGIYXvw_",
        "outputId": "e119946b-2baf-45c2-830d-1d4e922b074b"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "GFTTLSTMtestLoss = []\n",
        "GFTTLSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(GFTTTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      GFTTLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = GFTTLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      GFTTLSTMtestLoss.append(running_loss)\n",
        "      GFTTLSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 7.33560 acc: 1.300%\n",
            "[1,  2000] loss: 7.33449 acc: 1.350%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-239-367cfeb3a891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo3EtyKa3Ukl",
        "outputId": "3b6daff2-ab38-4524-acb7-53103dd7553f"
      },
      "source": [
        "!unzip captcha.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  captcha.zip\n",
            "   creating: captcha/\n",
            "  inflating: captcha/captchaTrainSmall.npy  \n",
            "  inflating: captcha/captchaValSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-6hB3gsc3gNF",
        "outputId": "9774b740-1135-4e7d-8567-5ca4d4a00720"
      },
      "source": [
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "with open(\"captcha/captchaTrainSmall.npy\", \"rb\") as f:\n",
        "    capt_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"captcha/captchaValSmall.npy\", \"rb\") as f:\n",
        "    capt_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "#capt_val_data = capt_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(capt_train_data))\n",
        "print(\"validation data size:\",len(capt_val_data))\n",
        "\n",
        "item = capt_train_data[123]\n",
        "\n",
        "print(\"training data shape:\", item[0].shape)\n",
        "cv2_imshow(item[0])\n",
        "print(\"data type of image =\", type(item[0]))\n",
        "print(\"training data label:\", item[1])\n",
        "print(\"each index in dataset has image (50x200) and char label\")\n",
        "print(item[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 308\n",
            "validation data size: 14\n",
            "training data shape: (50, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=200x50 at 0x7F0C9DA64610>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: 8p7n\n",
            "each index in dataset has image (50x200) and char label\n",
            "[[192 192 192 ... 230 230 230]\n",
            " [192 192 192 ... 230 230 230]\n",
            " [192 192 192 ... 230 230 230]\n",
            " ...\n",
            " [230 230 230 ... 230 230 230]\n",
            " [230 230 230 ... 230 230 230]\n",
            " [230 230 230 ... 230 230 230]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W03RFn0LzeR",
        "outputId": "a866bd7b-ca49-4ec5-c326-faccb83bf38f"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in capt_train_data:\n",
        "  label = i[1]\n",
        "  for char in label:\n",
        "    if char not in labels:\n",
        "      labels.append(char)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in capt_val_data:\n",
        "  label = i[1]\n",
        "  for char in label:\n",
        "    if char not in labels:\n",
        "      labels.append(char)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "captLabelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  captLabelDict[i] = labels[i]\n",
        "\n",
        "print(captLabelDict)\n",
        "invertedCaptDict = {y:x for x,y in captLabelDict.items()}\n",
        "print(invertedCaptDict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 37 labels in the training dataset\n",
            "there are 37 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '0', 2: '1', 3: 's', 4: 'e', 5: '3', 6: 'a', 7: '4', 8: 'r', 9: '5', 10: 'm', 11: 'b', 12: 'g', 13: 'v', 14: 'f', 15: 'i', 16: 'u', 17: 'j', 18: 'k', 19: '9', 20: 'l', 21: 't', 22: 'x', 23: '7', 24: 'w', 25: 'z', 26: '2', 27: 'y', 28: '8', 29: 'd', 30: 'h', 31: 'n', 32: 'q', 33: 'o', 34: 'p', 35: '6', 36: 'c'}\n",
            "{'<EOS>': 0, '0': 1, '1': 2, 's': 3, 'e': 4, '3': 5, 'a': 6, '4': 7, 'r': 8, '5': 9, 'm': 10, 'b': 11, 'g': 12, 'v': 13, 'f': 14, 'i': 15, 'u': 16, 'j': 17, 'k': 18, '9': 19, 'l': 20, 't': 21, 'x': 22, '7': 23, 'w': 24, 'z': 25, '2': 26, 'y': 27, '8': 28, 'd': 29, 'h': 30, 'n': 31, 'q': 32, 'o': 33, 'p': 34, '6': 35, 'c': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtmB4UNuKzJi"
      },
      "source": [
        "# dataset class\n",
        "class captchaDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # turn all chars of label into numbers\n",
        "      chars = []\n",
        "      for i in range(len(self.labels[index])):\n",
        "        char = self.classToNum[self.labels[index][i]]\n",
        "        chars.append(char)\n",
        "      # don't forget end of sentence \n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      chars.append(end)\n",
        "      # turn it all into a tensor\n",
        "      label = tensor(chars)\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9KspcibMjRy"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = captchaDataset(X=capt_train_data, classToNum=invertedCaptDict)\n",
        "capt_train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoLfH33-N0CY"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "# Captcha version!\n",
        "\n",
        "class captchaLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(captchaLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    # 1536 is resulting size from captcha images\n",
        "    self.l1 = nn.Linear(1536, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGHtkA1LNIiy"
      },
      "source": [
        "# create model instance\n",
        "# 37 possible classes to predict\n",
        "# 5 as max length pred because all images have 4 chars + eos\n",
        "LSTMModel = captchaLSTM(37, 10, 5).to(device)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "o9RxIIuvNRgS",
        "outputId": "ef584013-ec69-454f-9651-6ac102237524"
      },
      "source": [
        "testItem, testLabel = next(iter(capt_train_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output3, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output4, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output5, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = captLabelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = captLabelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred3 = captLabelDict[output3[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred4 = captLabelDict[output4[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred5 = captLabelDict[output5[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2 + pred3 + pred4 + pred5, \"for\", \"\".join(toChars(testLabel, captLabelDict)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 50, 200])\n",
            "Labels batch shape: torch.Size([10, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=200x50 at 0x7F0C9CE22DD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted hhshh for 0uai<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}