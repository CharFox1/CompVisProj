{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstmCharModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyV2F1aJlKG9sUFYXfyFMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/lstmCharModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzZmHEit6L7",
        "outputId": "90db3bf7-f715-4440-9a90-c9584a49e1d2"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxq_5vKjt9PR",
        "outputId": "79e1a3d2-d180-4246-d654-e79f34bba155"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8700YD7t_ua"
      },
      "source": [
        "# Manually pick cpu device if desired\n",
        "device = \"cpu\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfK0gAeuB-0"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUn_O6vuFZV"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5P5eSNzyuJCY",
        "outputId": "f96bfed6-3cf5-456e-aeda-02470ec6ddbf"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "conv_val_data = conv_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13000\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4231882810>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW8fDU_nuyqO",
        "outputId": "34398a4a-5334-45ca-daa6-4dad0af80559"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 40 labels in the training dataset\n",
            "there are 40 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '#', 2: '$', 3: '&', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '@', 15: 'A', 16: 'B', 17: 'C', 18: 'D', 19: 'E', 20: 'F', 21: 'G', 22: 'H', 23: 'I', 24: 'J', 25: 'K', 26: 'L', 27: 'M', 28: 'N', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z'}\n",
            "{'<EOS>': 0, '#': 1, '$': 2, '&': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6VrPTAwMBp"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # since lstm is being used but there is always only 1 char, no need to worry about parsing label\n",
        "      # just make one hot vector for char and end token and put them together in tensor\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86QKpTZozE8N"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDXQCzpLO8L"
      },
      "source": [
        "def toChars(nums, dict):\n",
        "  out = []\n",
        "  for num in nums:\n",
        "    out.append(dict[num])\n",
        "  return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASlls_LuDEM"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "class convLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(convLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOt1YmJHfAE"
      },
      "source": [
        "def parsePred(pred):\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  #print(pred)\n",
        "  out = []\n",
        "  for i in pred:\n",
        "    #print(i[0])\n",
        "    out.append(labelDict[i.argmax(0).item()])\n",
        "\n",
        "  return \"\".join(out)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_69r9ZIEeO"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "7uDGucfozFod",
        "outputId": "d6298265-2b9d-4528-b88f-b61ae15e6152"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABdElEQVR4nMVQMUvDYBS8GIsECcmSqQ6GDE6BDk5CBxcXKa2Dg/oLnHSwoLhbqIKDo7hVcFMw7hqX2iWQTkJTMxQ6OBRbm2JMeQ5p0jSWjHrL99337t277wH/D2aSSjvj+70FALPRsqIungMvbQCptZRtxczkTJnIM41VABA7lI/VeY3I7diCzwTbiwt0l0gThVEoRjAnBbzuEFXkKPcFQUhW5Uo16y0UsCrHRAXpY+70ygLSBwCg38bGQykTZUZnpUGPOenQqagRQYFcTQaUE3I1+ahB1X2/IUSeOiKAPXJ0Hii2iDxzCRMhAQBuv5n7xPwlWxq2sh9THOa2PVtApeOQGewjcHjd7AH46rMLz0NZuD4bhP2+gBnUAQC1DQCAVR+bT2RA+y6+AMz4h7Q796sURcEPOQ2+A4FVuSSB1+P15VTCiFSWqLueFILLdqlpFH1SNIybsDL65qC6dSGDXwEAqEqkNdjD94MoBW9PwHvSuD/HD7vvhug5aGgaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F41435D0B10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted @<EOS> for @<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBJlsmLndDk"
      },
      "source": [
        "# function to find accuracy for variable size output\n",
        "def findAccuracy(pred, labels):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  #print(pred.shape)\n",
        "  #print(labels.shape)\n",
        "\n",
        "  for p, l in zip(pred, labels):\n",
        "    for b in range(len(p)):\n",
        "      #print(p[b])\n",
        "      if p[b].argmax(0).item() == l[b]:\n",
        "        accuracy += 1\n",
        "\n",
        "  return accuracy/(pred.shape[0] * pred.shape[1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knuiNPbtZq7"
      },
      "source": [
        "# NOT USED\n",
        "\n",
        "# function to define loss for variable size output\n",
        "# works like categorical crossentropy for each of the char predictions\n",
        "def lstmLoss(pred, labels, lossFunc):\n",
        "  pred = pred.permute(1,0,2)\n",
        "  #print(pred.shape)\n",
        "  labels = labels.permute(1,0)\n",
        "  #print(labels.shape)\n",
        "  loss = []\n",
        "  for char, lab in zip(pred, labels):\n",
        "    #print(char.shape)\n",
        "    #print(lab.shape)\n",
        "    loss.append(lossFunc(char, lab))\n",
        "  return loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJIj4ZUcngqN",
        "outputId": "e8190cf1-6dcf-46d6-bd9d-126afbee4b62"
      },
      "source": [
        "#LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(LSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.36860 acc: 48.880%\n",
            "[1,  2000] loss: 4.44057 acc: 50.000%\n",
            "[1,  3000] loss: 4.34475 acc: 50.000%\n",
            "[1,  4000] loss: 4.24242 acc: 50.000%\n",
            "[1,  5000] loss: 4.14207 acc: 50.000%\n",
            "[1,  6000] loss: 4.01836 acc: 50.000%\n",
            "[1,  7000] loss: 3.84439 acc: 50.005%\n",
            "[2,  1000] loss: 3.38432 acc: 50.725%\n",
            "[2,  2000] loss: 3.07826 acc: 54.380%\n",
            "[2,  3000] loss: 2.77321 acc: 61.570%\n",
            "[2,  4000] loss: 2.46536 acc: 70.440%\n",
            "[2,  5000] loss: 2.19064 acc: 77.395%\n",
            "[2,  6000] loss: 1.90105 acc: 82.595%\n",
            "[2,  7000] loss: 1.65787 acc: 86.020%\n",
            "[3,  1000] loss: 1.24919 acc: 90.760%\n",
            "[3,  2000] loss: 1.09639 acc: 91.525%\n",
            "[3,  3000] loss: 0.94469 acc: 92.445%\n",
            "[3,  4000] loss: 0.82848 acc: 93.325%\n",
            "[3,  5000] loss: 0.74491 acc: 93.700%\n",
            "[3,  6000] loss: 0.67060 acc: 94.010%\n",
            "[3,  7000] loss: 0.60970 acc: 94.360%\n",
            "[4,  1000] loss: 0.49863 acc: 95.290%\n",
            "[4,  2000] loss: 0.45521 acc: 95.645%\n",
            "[4,  3000] loss: 0.42859 acc: 95.675%\n",
            "[4,  4000] loss: 0.40750 acc: 95.635%\n",
            "[4,  5000] loss: 0.39040 acc: 95.820%\n",
            "[4,  6000] loss: 0.37205 acc: 95.860%\n",
            "[4,  7000] loss: 0.35471 acc: 95.950%\n",
            "[5,  1000] loss: 0.29725 acc: 96.805%\n",
            "[5,  2000] loss: 0.28802 acc: 96.765%\n",
            "[5,  3000] loss: 0.28203 acc: 96.695%\n",
            "[5,  4000] loss: 0.28547 acc: 96.385%\n",
            "[5,  5000] loss: 0.26906 acc: 96.670%\n",
            "[5,  6000] loss: 0.27917 acc: 96.435%\n",
            "[5,  7000] loss: 0.24883 acc: 96.735%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "odSxfPhhDWBu",
        "outputId": "80290d52-4f36-4e8a-a044-1ebe67118499"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOuTSe6DnvB"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKIgoqbDw43",
        "outputId": "07d81870-038a-4563-fa71-a68c57947e2b"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.14879 acc: 88.275%\n",
            "[2,  1000] loss: 1.15740 acc: 88.100%\n",
            "[3,  1000] loss: 1.14069 acc: 88.100%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "YNzAl0F9EBqz",
        "outputId": "70487e40-fbf0-4462-d677-ec6f15e5bdd7"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVHwMT7H0wf"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsCornerDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      image = image * cv2.cornerHarris(image, 2, 3, 0.04)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVR__AEdH2VL"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "cornerTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdEzjgrH5oS"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "CornerLSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "ifHMtLREIDt1",
        "outputId": "d8e8edec-fb1b-41e7-d1b5-1ea7b0bd7422"
      },
      "source": [
        "testItem, testLabel = next(iter(cornerTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABRklEQVR4nI2TPU4DMRCFP292owWiDVEkOv5SgkKHuMM2SFtF4q9JhRDiCFwAkChyAFoaJERFAaKAloKWI9DSDoXXXs8aCaax38ybN/aMTQYJFHAOQMk2MD8lw9kA+sCIEh4oa++Q0A5gLHLGFKk9br0GuAu5wmQHeJwgVKfWl8MqIDararbswhbGCUrtlkBt0W6+BbFG2wwYjM+wHBGvkwOkHKokqQlSKwCvgfSv1UZRaXePy5mFS+0a4Y1SlsPoSh8fAhDbBtOkG7XufcbllcCtQiBXLQJdFee91e5U5/PSrnAMCaZhPKOncbSuBfLorCcUCg8iRk+huSi6kEQpdt52lr1ux81TTMgBfG8z03h9kxXDFWkeVvDE/muzP+LDtqNSKOGL+wDv87QJb0F/swT4EBAYFwA3oCbAWt6BjQuavhf+k8MPi65y/Dl+f5kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F414365AB50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted U<EOS> for U<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrgUswNJbJ9",
        "outputId": "c481b633-7822-445a-cd71-1e5c5286df0e"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "LSTMtestLoss = []\n",
        "LSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      LSTMtestLoss.append(running_loss)\n",
        "      LSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.56157 acc: 48.400%\n",
            "[1,  2000] loss: 4.47824 acc: 50.000%\n",
            "[1,  3000] loss: 4.39765 acc: 50.000%\n",
            "[1,  4000] loss: 4.30606 acc: 50.000%\n",
            "[1,  5000] loss: 4.19796 acc: 50.000%\n",
            "[1,  6000] loss: 4.07397 acc: 50.000%\n",
            "[1,  7000] loss: 3.94622 acc: 50.030%\n",
            "[2,  1000] loss: 3.68766 acc: 50.540%\n",
            "[2,  2000] loss: 3.55134 acc: 51.310%\n",
            "[2,  3000] loss: 3.40052 acc: 53.245%\n",
            "[2,  4000] loss: 3.23160 acc: 56.195%\n",
            "[2,  5000] loss: 3.06545 acc: 59.390%\n",
            "[2,  6000] loss: 2.87635 acc: 62.235%\n",
            "[2,  7000] loss: 2.66348 acc: 65.705%\n",
            "[3,  1000] loss: 2.30184 acc: 71.535%\n",
            "[3,  2000] loss: 2.09118 acc: 74.880%\n",
            "[3,  3000] loss: 1.90122 acc: 78.030%\n",
            "[3,  4000] loss: 1.70102 acc: 80.930%\n",
            "[3,  5000] loss: 1.53972 acc: 83.295%\n",
            "[3,  6000] loss: 1.37654 acc: 85.670%\n",
            "[3,  7000] loss: 1.23783 acc: 87.500%\n",
            "[4,  1000] loss: 1.00445 acc: 90.135%\n",
            "[4,  2000] loss: 0.89318 acc: 91.390%\n",
            "[4,  3000] loss: 0.82866 acc: 91.710%\n",
            "[4,  4000] loss: 0.74795 acc: 92.360%\n",
            "[4,  5000] loss: 0.68584 acc: 92.930%\n",
            "[4,  6000] loss: 0.63772 acc: 93.425%\n",
            "[4,  7000] loss: 0.58845 acc: 93.590%\n",
            "[5,  1000] loss: 0.49379 acc: 94.715%\n",
            "[5,  2000] loss: 0.46836 acc: 94.775%\n",
            "[5,  3000] loss: 0.43274 acc: 95.055%\n",
            "[5,  4000] loss: 0.41549 acc: 95.290%\n",
            "[5,  5000] loss: 0.39713 acc: 95.315%\n",
            "[5,  6000] loss: 0.37941 acc: 95.330%\n",
            "[5,  7000] loss: 0.36549 acc: 95.495%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "w7yE-KSGJstq",
        "outputId": "670656bd-fca6-4200-a6a1-422925619d14"
      },
      "source": [
        "testAcc = testAcc[:35]\n",
        "print(len(testAcc))\n",
        "print(len(LSTMtestAcc))\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\", label=\"Raw Data\")\n",
        "ax.plot(LSTMtestAcc, color=\"tab:blue\", label=\"Corner Detection\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR2cb85uGhEm"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "cornerVal_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhV9wHjEGGNx",
        "outputId": "5349f2b9-eba7-4606-d2be-68a3f770127e"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerVal_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.25047 acc: 86.995%\n",
            "[2,  1000] loss: 1.24245 acc: 86.915%\n",
            "[3,  1000] loss: 1.24736 acc: 86.915%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "wGU8SBiRGT8m",
        "outputId": "161a322c-e867-4139-f067-45d2e7aa5305"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "k56MY3j5mDtY",
        "outputId": "5a7e0243-68a3-4e54-daf7-907c0b0b3b63"
      },
      "source": [
        "# experiment with effects of FAST  detection\n",
        "print(\"original image\")\n",
        "input = np.float32(conv_train_data[61345][0])\n",
        "#input = cv2.cvtColor(input, cv2.COLOR_GRAY2RGB)\n",
        "cv2_imshow(input)\n",
        "#print(input.shape)\n",
        "corners = cv2.goodFeaturesToTrack(input,25,0.01,10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "out = np.zeros((32,32), dtype=float)\n",
        "for i in corners:\n",
        "  x,y = i.ravel()\n",
        "  cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "print(\"corner detection\")\n",
        "cv2_imshow(out)\n",
        "\n",
        "combo = np.concatenate([input,out],1)\n",
        "print(\"concatenated\")\n",
        "cv2_imshow(np.float32(combo))"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACWklEQVR4nG1TTUiUURQ95773fTPmP5IUQSHRJjNqkUJEhFi5yiAYatcmomgRZCAkBG2CfqgWgVCLQGvZKohIESrCXdBgkS0UQVCxwvydme+92+J9I4nztvdw3jnnngtUeAYEYcFKQ0khAgCkbAGQBAUABVKZwoBChpmtMCcMYetoRbbypxq3nbz7PlcFsLLOzNGXqwVdO08BTIUv6p7Mq3fq38WpDpAGAAEBTNw9uqbqE6/56gAgARghIWR8oH+xpMnCs1ze52tDLITYMlPdxbl11T+vO+PsA83XUEw5OQoQof7Ksvdu8kYj0DzuxqsAgQEkjSPTdm9Fk7Xn7TENuxb1S0MwKQAIi57BuRWXzPQ2AxD0azIgJAjrIaqSRN05sf7bcn0GpD99KeEHT0AZdGLH7RnV1cc3EzfYujtz4kdJv+4CIBJ8snWopE5HsmeLyfrk2NPZRH/3GZCQkEPtI3VO13vihhfOe02c/3W1KfUHANH1laJzOtQo2PlGk8Ls/NjlCALChB+2T2iiWrwACHqd/364rUUgwaAAtNe806JOHycs2qb15z4ClpJmTBxacKo6dYyIwMytVT/ctJEeBJb3vXq/NFgrgAFbFkq+rz0SA5OWU15550p34nT51Q+dc3Nvc3v2dzalFeka/Th8rqrcDuydLnmnycSUP8Wwrig2WdgNAM98+qve+aXPHVYAguIEHtS0cp7VRxql4+DASKGAqAQDRGmvAkEEgUW2RiAhSJbPbQNCIWjDFg1gyP8xUl4BAWPSKkM2kQhBMJwEARhsup9wAwQAC+IffyXYbeG0Qp0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4143543D50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corner detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAANklEQVR4nGNgGAWUgP9wFhMO+f9YxbGZQG+AzWYmFPnB63bCAK/bmHD77j8WFor8/6EcKqQCADKlEPb+mSiDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4143543BD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "concatenated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACoUlEQVR4nJ2VT0gVURTGv+/cmXnP/I8kRVBItMmMWqQQESFWrjIIpHZtIooWQQZCQtAm6A/VIhBqEWgtWwURKUJFuAt6WGQLRRBUrDD/Pt/ce1rMe/q0p46e5cyc33z3nO+cCxQIA4LwwEIvNwrJIgQASNk0gCQoACiQrUkwoJBRrreFfMIQXhk9kc3rj8JsO3n3fWsRwK3VMXH05Vxa589TALP5dCl7MqHOqnsXMObvSQOAgAAmaO6bV3Wh01RxPAAJwAgJIYMDHVMZDSeftaZcqhTI2eK/0DwAxMuhyi6OL6j+ed0YJB9oqoSyVglUlwmR4wTwUX5lxjk7dKMSqB6wA0WArFXFPAWQrF0SdfdmNZx/Xh/QsGlKv1TEa6IAIDy0dI3P2nC0rRqAoEPDTiFjEDwHUZXQb24Vz32bKU+AdKcvhfzgCCh1Q4SAwI7bo6pzj2+Gtqt2d+LEj4x+3QVglZkLw0iwtjujVnuTZxfDhaH+p2Oh/m43IFd2Mb/2K/JR+kit1YWWoOKFdU5D635drcr2Z2MFgH99dtFa7a4U7HyjYXpsov+yDwFjjQKJ7YMaqi5eAARt1n0/XFcjkKhBMSaa3jVndVFHjhMe6kb05z4CHmXZo+vn49CkVdXhY4QPJm7NuZ6qJXfFUODxvlPnprtKBTBgzWTGtdf7YmCyy3HN2mVDXjlrM3eCbL2LH1prx9+27tnfWBU9Kdy9PG5T38eec0VLw793JOOshoPD7lS0I9ZQkOMa+IFJwlsC8Mynv+qsm/7c4K1bgSifoFiBQ8704lh8pFIaDnb2ptPwM+ufH4AB/Oxeiog+BB6SJQJBrHnm6l6RQtCLpiyGFw2Zz5DcCBAwJoYCiW7SfBFCEDQ5fRtLWPlZtOMJINYF/w8Z8+1Yf7dcRQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7F4143543D50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O47MxWFyVMSZ"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsGFTTDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      corners = cv2.goodFeaturesToTrack(image,25,0.01,10)\n",
        "      if (corners is not None):\n",
        "        corners = np.int0(corners)\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "        for i in corners:\n",
        "          x,y = i.ravel()\n",
        "          cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "      else:\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "\n",
        "      combo = np.concatenate([image,out],1)\n",
        "      #cv2_imshow(combo)\n",
        "      image = torch.tensor(combo)\n",
        "      #print(image.shape)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      #image = self.transform(image)\n",
        "      #print(image.shape)\n",
        "      #cv2_imshow(np.float32(image))\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      #T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g07HWPEHXJ53"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsGFTTDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "GFTTTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)\n",
        "GFTTLSTMModel = convLSTM(40, 10, 2).to(device)"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lKe4Rj7RXW6E",
        "outputId": "856b4f59-11ad-4d5b-b9e5-5a95145dba27"
      },
      "source": [
        "testItem, testLabel = next(iter(GFTTTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "#print(testItem[0].cpu().numpy()[0].shape)\n",
        "image = testItem[0].cpu().numpy()\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACVUlEQVR4nJ2VO2gVURCGv5mzuV4hEkWNikoCISLEyiog2AhqqY1YaKc2VhYWdkIawUdlChEkYgqxUvAJQXxjo6DYBCNRRPD6AF/xavbsjMXeoEk2680dWDgs/P/5Z+Y/M/A3RAHJP2G+oQhokh9J5o0HggoQWoECIKCiCgEJzfNM0yrmhFULs9dOVoLxwgJprkF7bnwa6y+9390L/+ctWHbNs+xQKK1iMR5BhJ4z3838QXuZgiKcgKBK57GYeRZHO0VQQZqyg4rm+tFFl+ruqbntaN5QKuZWwYQ2FgxsrdQHb0qdCjgBT+bKdxqFai53+b4v0YcXnTB/1SUqTXoqwQAjmO4+viA9d2PFSonWtcYz/fjGHPm/BBVA+65er3k28eT929Sjx5+TMRvtIcmLUxz/UoeOi5a6e/SYunk0z1KrbyxNouGnhJAh1dO7JHHX4CoIE8+jXBiPL7ESAplytBK2PJ50i7XbIyNXDtQyH9uUCFLYwIKKKGweN3d7uF5U6a/Z5JFqIBQSFD+D6llzzx5tI8CKRxbfrEN0DgfNxifKL5Wonw+8EBO6u8WGxtuiaXH6RbTSO2b2YXsgKCsfZ/HrTqSsezNDOfrbbX8+Cfd+8x8H25RK80NVTZZUoiwNAqHvcLvdHU4hnWHAMjsqJ6PHWwGl407q93tl9hucawY1GDYMPjvfK4GuU/W0tqe4fSV4IWHtYgUdyH7d66/Od6MkYLwFpbL46dDldy2thYSgQHV1mE/7pkIU8uHV0joDlKAN57a21jQ0JARaWcooSD6UWoLzB0Z95A3m70YaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7F414365A750>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-324-34643acf8563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# predict twice with same image but different hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-dbd35ecf87f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# resnet layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 1, 2, 2], but got 3-dimensional input of size [10, 32, 64] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "CB_1EGIYXvw_",
        "outputId": "e119946b-2baf-45c2-830d-1d4e922b074b"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "GFTTLSTMtestLoss = []\n",
        "GFTTLSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(GFTTTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      GFTTLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = GFTTLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      GFTTLSTMtestLoss.append(running_loss)\n",
        "      GFTTLSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 7.33560 acc: 1.300%\n",
            "[1,  2000] loss: 7.33449 acc: 1.350%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-239-367cfeb3a891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}