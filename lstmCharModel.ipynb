{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstmCharModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGUXn1NJribmseuSKzYrPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/lstmCharModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzZmHEit6L7",
        "outputId": "1900cf52-b54c-4397-b271-161b9d458702"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxq_5vKjt9PR",
        "outputId": "0ef4ff4a-cc2f-4a1e-98a2-3f08514cf979"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8700YD7t_ua"
      },
      "source": [
        "# Manually pick cpu device if desired\n",
        "device = \"cpu\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfK0gAeuB-0"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUn_O6vuFZV"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "5P5eSNzyuJCY",
        "outputId": "69f660d2-5362-439c-ed5b-e38e862684eb"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "conv_val_data = conv_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13000\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FC0E0BC7A50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW8fDU_nuyqO",
        "outputId": "eab6801b-3939-4dbc-e106-ecb026ebfb11"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 40 labels in the training dataset\n",
            "there are 40 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '#', 2: '$', 3: '&', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '@', 15: 'A', 16: 'B', 17: 'C', 18: 'D', 19: 'E', 20: 'F', 21: 'G', 22: 'H', 23: 'I', 24: 'J', 25: 'K', 26: 'L', 27: 'M', 28: 'N', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z'}\n",
            "{'<EOS>': 0, '#': 1, '$': 2, '&': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6VrPTAwMBp"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # since lstm is being used but there is always only 1 char, no need to worry about parsing label\n",
        "      # just make one hot vector for char and end token and put them together in tensor\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86QKpTZozE8N"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDXQCzpLO8L"
      },
      "source": [
        "def toChars(nums, dict):\n",
        "  out = []\n",
        "  for num in nums:\n",
        "    out.append(dict[num])\n",
        "  return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASlls_LuDEM"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "class convLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(convLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOt1YmJHfAE"
      },
      "source": [
        "def parsePred(pred):\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  #print(pred)\n",
        "  out = []\n",
        "  for i in pred:\n",
        "    #print(i[0])\n",
        "    out.append(labelDict[i.argmax(0).item()])\n",
        "\n",
        "  return \"\".join(out)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_69r9ZIEeO"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "7uDGucfozFod",
        "outputId": "24ab176a-863b-43c6-ca28-fcaf4da7d422"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 32, 32])\n",
            "Labels batch shape: torch.Size([10, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACgklEQVR4nF2T34uWRRTHP99znufFXWM3U2hFLdGWJIWQIChSjA0KKipI1EAIQo2uUiG8CxbqT9A2SSQhFVroKre2m0TKJVgoQTIi0ZCiKDDb3PfdZ+Z4Mc+bq3Mzw5yZc74/zoH+EkjlYIu2O+KAOSDHwOl/KcuEDHD1n5vki+K0KYQcF3V59n+dQArVZKNa+dbZmZnnncDjdgoHM6B+auJqpIiPlt4F0iTDln3wbW7i4sGpmF0Jd2KoGNg5dTl1L44/OrzmXJ7fWRVYDhiG+ejEQm9ufPsK5Edzc/VJ9flXQlTrJv9J1w5vLOBPNnFmsCrcMGT109/82fvpzVWVIdw4FekNAU6FZU88dmxtpK+OJkyhxNDSrE5YtmQgMB86mbrvD9MpWtlrN+PSulZoKxIuOxx/bxblYu33EUcG1bKgcF39wscP444Dj8/1ft6AwK2kkSEha3lp33ycG5ZJIERxT5gwhBiYjrzXioGqAvDkjcLv3/j6Awn/ZMtYmv4ye7IMQeEBPjR+LfLNK/MLEU3sAqq2YYRQba+c786ePjh2z9gXvRR5f12i1pf6vsm5Cy+NypG29nLEjQNLbGT7YAcvBZ749L/PR0upgfci/9WNP158eWKkVaHmoa+7H96LGTJ2RXd604mmuf7b1HLHgA6rZ9Kvm9pyG35sftlsD/4Q0dtTUlq1ZjLl43Vx4ZFLcf1ZN9/xez47otbuQ7npHhIGtvtCLByrEWz77DmDCmHV2ykfcRM+9O5C/vfVJS6sAzKq0o27Z8+sr3BV73x3fvIZu93LkgSGScLxMjJIcjBXHwJCwmVIhqhpZUbFfKG2s9sJdLXulLkoZpsWad9Of9lvAdtbx7+nu3H1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F16811752D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted 2<EOS> for 2<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBJlsmLndDk"
      },
      "source": [
        "# function to find accuracy for variable size output\n",
        "def findAccuracy(pred, labels):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  #print(pred.shape)\n",
        "  #print(labels.shape)\n",
        "\n",
        "  for p, l in zip(pred, labels):\n",
        "    for b in range(len(p)):\n",
        "      #print(p[b])\n",
        "      if p[b].argmax(0).item() == l[b]:\n",
        "        accuracy += 1\n",
        "\n",
        "  return accuracy/(pred.shape[0] * pred.shape[1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knuiNPbtZq7"
      },
      "source": [
        "# function to define loss for variable size output\n",
        "# works like categorical crossentropy for each of the char predictions\n",
        "def lstmLoss(pred, labels, lossFunc):\n",
        "  pred = pred.permute(1,0,2)\n",
        "  #print(pred.shape)\n",
        "  labels = labels.permute(1,0)\n",
        "  #print(labels.shape)\n",
        "  loss = []\n",
        "  for char, lab in zip(pred, labels):\n",
        "    #print(char.shape)\n",
        "    #print(lab.shape)\n",
        "    loss.append(lossFunc(char, lab))\n",
        "  return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJIj4ZUcngqN",
        "outputId": "b9bd7f01-41f5-4888-e4b9-99162768e9cf"
      },
      "source": [
        "import torch.optim as optim\n",
        "#LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(LSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.37488 acc: 49.430%\n",
            "[1,  2000] loss: 4.43510 acc: 50.000%\n",
            "[1,  3000] loss: 4.31214 acc: 50.000%\n",
            "[1,  4000] loss: 4.13866 acc: 50.000%\n",
            "[1,  5000] loss: 3.95189 acc: 50.000%\n",
            "[1,  6000] loss: 3.77060 acc: 50.000%\n",
            "[1,  7000] loss: 3.51738 acc: 50.035%\n",
            "[2,  1000] loss: 2.84604 acc: 66.875%\n",
            "[2,  2000] loss: 2.43004 acc: 78.225%\n",
            "[2,  3000] loss: 2.05487 acc: 82.520%\n",
            "[2,  4000] loss: 1.69731 acc: 86.990%\n",
            "[2,  5000] loss: 1.39592 acc: 89.650%\n",
            "[2,  6000] loss: 1.13518 acc: 91.645%\n",
            "[2,  7000] loss: 0.94093 acc: 92.625%\n",
            "[3,  1000] loss: 0.68564 acc: 94.350%\n",
            "[3,  2000] loss: 0.59292 acc: 94.875%\n",
            "[3,  3000] loss: 0.52551 acc: 95.225%\n",
            "[3,  4000] loss: 0.48486 acc: 95.045%\n",
            "[3,  5000] loss: 0.44654 acc: 95.335%\n",
            "[3,  6000] loss: 0.40361 acc: 95.745%\n",
            "[3,  7000] loss: 0.39045 acc: 95.475%\n",
            "[4,  1000] loss: 0.32422 acc: 96.370%\n",
            "[4,  2000] loss: 0.30652 acc: 96.305%\n",
            "[4,  3000] loss: 0.29295 acc: 96.540%\n",
            "[4,  4000] loss: 0.29371 acc: 96.400%\n",
            "[4,  5000] loss: 0.27254 acc: 96.685%\n",
            "[4,  6000] loss: 0.26579 acc: 96.635%\n",
            "[4,  7000] loss: 0.25165 acc: 96.685%\n",
            "[5,  1000] loss: 0.22601 acc: 96.950%\n",
            "[5,  2000] loss: 0.21511 acc: 97.235%\n",
            "[5,  3000] loss: 0.21952 acc: 96.925%\n",
            "[5,  4000] loss: 0.21898 acc: 96.985%\n",
            "[5,  5000] loss: 0.20366 acc: 97.205%\n",
            "[5,  6000] loss: 0.20921 acc: 97.100%\n",
            "[5,  7000] loss: 0.20860 acc: 97.020%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "odSxfPhhDWBu",
        "outputId": "f9643a44-7239-4fb6-b609-95bda2213ff7"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOuTSe6DnvB"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKIgoqbDw43",
        "outputId": "44bd35c4-87e6-474b-c4ef-07846f7b64ad"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.06518 acc: 88.845%\n",
            "[2,  1000] loss: 1.05418 acc: 88.810%\n",
            "[3,  1000] loss: 1.08221 acc: 88.585%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "YNzAl0F9EBqz",
        "outputId": "ea5ec277-9613-4d2a-c673-9b245f99faa0"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVHwMT7H0wf"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsCornerDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      image = image * cv2.cornerHarris(image, 2, 3, 0.04)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVR__AEdH2VL"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "cornerTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdEzjgrH5oS"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "CornerLSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "ifHMtLREIDt1",
        "outputId": "c2d79080-bfc5-4c82-df8a-0edd54df0ed7"
      },
      "source": [
        "testItem, testLabel = next(iter(cornerTrain_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 32, 32])\n",
            "Labels batch shape: torch.Size([10, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABGUlEQVR4nH2TPU7DQBCFP68tgRMniEgIicIQpDQRSkdDS6qIgkNwApDoaLgAF+AacAUOkROko06RSbHrzf6MM9XI7+17b2bXoFQBBqCEKkcFYAUw1g5bwteMJXzf0QwTtDqXrjUw48Z6HWo6LJwJuy1reb7VPDqR9zM1gngCLP6mPSn1egFOVEQE+ID5awI8Rq73ACYe6zOIbRjwBLRHQoDAQw9BnNBl3xxCuIBDGeXbadTPowAZIUIt4T+M0ELhcE9AjWIJGTKigZ8jBMZwnSsEPPGZJTD3SiL8ppMkKbUu7DcADPoJgH1D7klMsiko3arViyprqOHKSwraGw4s3wDaVKqxjAUFtDQXbiUWrGrAgP+DjL88um4PMyFjXTIBQhwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F163FE18B50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted XZ for W<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrgUswNJbJ9",
        "outputId": "af137d11-5316-442a-948f-21bb251dde53"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.37755 acc: 48.790%\n",
            "[1,  2000] loss: 4.43367 acc: 50.000%\n",
            "[1,  3000] loss: 4.32828 acc: 50.000%\n",
            "[1,  4000] loss: 4.21009 acc: 50.000%\n",
            "[1,  5000] loss: 4.10253 acc: 50.000%\n",
            "[1,  6000] loss: 3.98895 acc: 50.010%\n",
            "[1,  7000] loss: 3.86689 acc: 50.020%\n",
            "[2,  1000] loss: 3.61003 acc: 50.610%\n",
            "[2,  2000] loss: 3.47025 acc: 51.370%\n",
            "[2,  3000] loss: 3.30581 acc: 54.045%\n",
            "[2,  4000] loss: 3.12624 acc: 57.480%\n",
            "[2,  5000] loss: 2.91168 acc: 61.590%\n",
            "[2,  6000] loss: 2.69022 acc: 64.905%\n",
            "[2,  7000] loss: 2.48426 acc: 67.985%\n",
            "[3,  1000] loss: 2.12148 acc: 74.185%\n",
            "[3,  2000] loss: 1.95044 acc: 76.965%\n",
            "[3,  3000] loss: 1.77447 acc: 80.145%\n",
            "[3,  4000] loss: 1.61251 acc: 83.015%\n",
            "[3,  5000] loss: 1.45986 acc: 85.080%\n",
            "[3,  6000] loss: 1.29783 acc: 87.305%\n",
            "[3,  7000] loss: 1.16602 acc: 89.030%\n",
            "[4,  1000] loss: 0.92116 acc: 91.370%\n",
            "[4,  2000] loss: 0.84466 acc: 91.955%\n",
            "[4,  3000] loss: 0.75481 acc: 92.920%\n",
            "[4,  4000] loss: 0.70994 acc: 92.885%\n",
            "[4,  5000] loss: 0.63825 acc: 93.485%\n",
            "[4,  6000] loss: 0.58862 acc: 94.210%\n",
            "[4,  7000] loss: 0.54948 acc: 94.220%\n",
            "[5,  1000] loss: 0.45388 acc: 95.130%\n",
            "[5,  2000] loss: 0.43750 acc: 95.285%\n",
            "[5,  3000] loss: 0.42857 acc: 95.175%\n",
            "[5,  4000] loss: 0.39716 acc: 95.340%\n",
            "[5,  5000] loss: 0.39156 acc: 95.310%\n",
            "[5,  6000] loss: 0.37093 acc: 95.525%\n",
            "[5,  7000] loss: 0.35035 acc: 95.730%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "w7yE-KSGJstq",
        "outputId": "0d64303b-b113-4326-8d1b-660971b3f53e"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "k56MY3j5mDtY",
        "outputId": "f8d3bed9-2272-4b85-9b35-8c5f616843b8"
      },
      "source": [
        "# experiment with effects of FAST  detection\n",
        "print(\"original image\")\n",
        "input = np.float32(conv_train_data[12345][0])\n",
        "#input = cv2.cvtColor(input, cv2.COLOR_GRAY2RGB)\n",
        "cv2_imshow(input)\n",
        "print(input.shape)\n",
        "corners = cv2.goodFeaturesToTrack(input,25,0.01,10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "for i in corners:\n",
        "  x,y = i.ravel()\n",
        "  cv2.circle(input, (x,y), 1, 255, -1)\n",
        "\n",
        "print(\"corner detection\")\n",
        "cv2_imshow(input)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACLUlEQVR4nF1Tz0uUURQ95973TQyjRY6DEREigVFSgQhFkFi4CDf+ARUhtq1VtWxRggRG7o1a92PjooWCQUUkBEULU7ChX7NIFFJCp5nvvdviG53v824ej3veuefcwwMaJRCFEAIAySEE0SxF0kQEKJMLXRoBB25TQaDYVdpgIYmEmoLUDAJKOOBwT6RRY0JWBIkc9PznlZt5RASy8wESYPHqL29bE2f2wyVS0iPIzqFXsYUQ/L8PT7pyDVNND633v8fma19mX4zNrfqlEZBIgQR9qxaXJ/uLIPXoVO3TAMGM1+5Hz253AqpQypU1m1AgqzMCIyUAyY3+ttoD7AJQ4ACo5nvH1y287MmGIYQCjFgceFyx8G2sSwlkV8kORv2l/nMtsjIz9Y51F9NSDDh07+NyectC7ev46QIgoKQeA9K35s0s+J+DymbP0QCaiYXK7Em3+KY42n6jsiAhpV6THIC2Usc+RNc26pOKXKotgEAFEQBE0jbtn+abz8W7AAZYQF0dpB7+LPPsCd1RL4zb+kqEQRFip9BCK/L0luIY2pjuhpKOILTlYdUWj0fpFQ/X7fXwERFA9vSOzNfi6h1mMmi//CNYZWZ08MLd93998G8v7k2HJABuza3H5s2bVf3K9QNIZ0SaC1Y4delYb6jOb9afL5Q9AtQ3twxIkIDSwWhzCRZcDIgZd2woSCBKoiVAiGRShgLIAXAOTAXYLG7/qgQLSdJJ22DSYgPNjIv/m3iyo/GsfYcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FC077488290>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(32, 32)\n",
            "corner detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACI0lEQVR4nF1Tz0uVQRQ9596ZF6IW+XwYFiESCCERSFAEiYWLaOMfUBBi21pVf0AIEhi5N2rdj02LFgYGFZEQFC1MwaRfb5EYZIS+3vtmbovv6ZvPuxmGe+bcc+5hgGYJRCGEAEB+CEG0SpE34QFlfqFLEXDgNhUEil2lTRaSyKkpSGYQUMIBhwe9+uaEoggSJejZj2vX2+AJFOcDJMDy5R/BtqZP7YfLpaQjyL4LLzKLMYZ/7x70l5qmWh46b3/NLNQ/PX8yOb8elsdBIgEJTqxbtjozXAapNlv/MEKw4HXg3qObfYAq1Mx+2bQCRZ0e9EoAUrKfVr+DXQAKHADVtqGpDYvPBothCKEAPcsj96sWv0z2K4HiKtlDP1wZPtMha3Ozb9hwGS1hwCF7v7K6ZbH+eepkOyCgJI8BMTMzi+H7qLLVczSAZmLxYHXJLb0qT3Rfqy5KTNRrngPQVenZB3/lT2NGUUraAghU4AHAS9fT8LCt9VyCi2CERTTUQRrx9wpPH9Md9cKsyyqEQREzp9D2TvQyWMJhZgNQ0hGEdtytmZlPVzxm9nLsiAgge4bGF+pZzVjIoPvSt2jVuYnRc7fe/g0xvD6/Nw1JANyY38gsWDCrhbWrB5BmRJqL1n784tGhWFvYbDxeXA2I0NDaMiBRIiq9fnMZFl0GiBl3bChIwOfREiBECilDAZQAOAcmAbaK278qx0LydFIbzFtsollw8R/ljrt7czaqnwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FC07747D8D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}