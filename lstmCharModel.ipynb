{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstmCharModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNg6qyz3I/7mDBvVdj4t2Tz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/lstmCharModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzZmHEit6L7",
        "outputId": "7d043bad-9c1b-46e0-87f6-ceccc14ba1b9"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxq_5vKjt9PR",
        "outputId": "a692f6c0-130a-49e0-8072-b121e8086e18"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8700YD7t_ua"
      },
      "source": [
        "# Manually pick cpu device if desired\n",
        "device = \"cpu\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfK0gAeuB-0"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUn_O6vuFZV"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "5P5eSNzyuJCY",
        "outputId": "1324e951-daca-4dce-acde-941a9fd183ae"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13209\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FB7DDA3D590>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW8fDU_nuyqO",
        "outputId": "fb360cca-0f10-4aa0-d1ce-99351954c97e"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 40 labels in the training dataset\n",
            "there are 40 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '#', 2: '$', 3: '&', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '@', 15: 'A', 16: 'B', 17: 'C', 18: 'D', 19: 'E', 20: 'F', 21: 'G', 22: 'H', 23: 'I', 24: 'J', 25: 'K', 26: 'L', 27: 'M', 28: 'N', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z'}\n",
            "{'<EOS>': 0, '#': 1, '$': 2, '&': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6VrPTAwMBp"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # since lstm is being used but there is always only 1 char, no need to worry about parsing label\n",
        "      # just make one hot vector for char and end token and put them together in tensor\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86QKpTZozE8N"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDXQCzpLO8L"
      },
      "source": [
        "def toChars(nums, dict):\n",
        "  out = []\n",
        "  for num in nums:\n",
        "    out.append(dict[num])\n",
        "  return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASlls_LuDEM"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "class convLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(convLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # recurrent an supporting layers\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOt1YmJHfAE"
      },
      "source": [
        "def parsePred(pred):\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  #print(pred)\n",
        "  out = []\n",
        "  for i in pred:\n",
        "    #print(i[0])\n",
        "    out.append(labelDict[i.argmax(0).item()])\n",
        "\n",
        "  return \"\".join(out)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_69r9ZIEeO"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "7uDGucfozFod",
        "outputId": "60bc5b60-8fff-4610-a1eb-bbf1add36238"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 32, 32])\n",
            "Labels batch shape: torch.Size([10, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACQUlEQVR4nF2TT4jNURTHP99z7+9NM96YRs0kUzZD+TNTg2SBlJXsRNkRGxa2ZG9jY6GwsFdKUoaFjcRWEQobFKMsDL3hzcx7v3uPxe/9G3dx76/fOed77j3nc6C3zACQUPWBwNQz5+qnVz7uAKbcixdIXYnqNOjFA5LU0Q2R0IkZcKiyWSUQUWDtHbCMuQO2cet44/UilpEPpoAAURtOzS+svj02goRhA1cAKCbP3Pleemo9niswAlBtMiTC+rlr35N76f7r8qgRQZGEcHcpM3bm9JbhnEJwjUzGTGnkKAdHUhqePnR2u+cq6++vLSnnzstQMIb3zDeW3T2n7N68O2MmA0UkpMTUkaMHR3G1l959na0/v/nBHeu+02Di7KecPfvSk5NT47t3TUIAVRuI6Ytf8qqXqXF7phCCGqLXC6Hjnz15Kt9f2hQIVQEEMgwChOkbi5595dmJeqdkA00KhLj/XtNTbtzbW0cCM2nAw6hf+dt2X36xH9Tpad8eyTY2HkOpn/dfBvfRjRret+7RR8s9B2nuQI3oi5suJLeds2M2kUdu/ewpyNL6iWRlMbsjkC2FbCz8aQ2kAJJU4AG3dpFC88fjB0ux7CkkrfyeIpsAL8rYeHr9zV/6NCNtOP961ZN78tTO365ui4b6JAFx6PDDXyvNZmtpeeXVuTo11K+UKNqIzTNDyWtt4/OHFlmx3cdV1EIXTglZ6A9fF+YuNMIUMKFCa5ohEQzDhCGi8d/MdaFQ6Iy4KtTXmPt8/Dey/wAVD89Eegx5NwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FB779EE47D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted C<EOS> for C<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBJlsmLndDk"
      },
      "source": [
        "# function to find accuracy for variable size output\n",
        "def findAccuracy(pred, labels):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  #print(pred.shape)\n",
        "  #print(labels.shape)\n",
        "\n",
        "  for p, l in zip(pred, labels):\n",
        "    for b in range(len(p)):\n",
        "      #print(p[b])\n",
        "      if p[b].argmax(0).item() == l[b]:\n",
        "        accuracy += 1\n",
        "\n",
        "  return accuracy/(pred.shape[0] * pred.shape[1])"
      ],
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knuiNPbtZq7"
      },
      "source": [
        "# function to define loss for variable size output\n",
        "# works like categorical crossentropy for each of the char predictions\n",
        "def lstmLoss(pred, labels, lossFunc):\n",
        "  pred = pred.permute(1,0,2)\n",
        "  #print(pred.shape)\n",
        "  labels = labels.permute(1,0)\n",
        "  #print(labels.shape)\n",
        "  loss = []\n",
        "  for char, lab in zip(pred, labels):\n",
        "    #print(char.shape)\n",
        "    #print(lab.shape)\n",
        "    loss.append(lossFunc(char, lab))\n",
        "  return loss"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJIj4ZUcngqN",
        "outputId": "f17256f1-3613-4ba5-a492-5afaf501e6a0"
      },
      "source": [
        "import torch.optim as optim\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(LSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.51888 acc: 48.875%\n",
            "[1,  2000] loss: 4.44372 acc: 50.000%\n",
            "[1,  3000] loss: 4.32914 acc: 50.000%\n",
            "[1,  4000] loss: 4.18248 acc: 50.000%\n",
            "[1,  5000] loss: 4.01081 acc: 50.000%\n",
            "[1,  6000] loss: 3.76126 acc: 50.000%\n",
            "[1,  7000] loss: 3.40329 acc: 50.280%\n",
            "[2,  1000] loss: 2.67183 acc: 71.260%\n",
            "[2,  2000] loss: 2.28655 acc: 79.380%\n",
            "[2,  3000] loss: 1.91427 acc: 84.015%\n",
            "[2,  4000] loss: 1.61632 acc: 87.120%\n",
            "[2,  5000] loss: 1.35750 acc: 89.375%\n",
            "[2,  6000] loss: 1.11552 acc: 91.695%\n",
            "[2,  7000] loss: 0.93612 acc: 92.735%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}