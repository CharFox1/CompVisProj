{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstmCharModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6ge6+2hu/REhiZlm7OcXx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/lstmCharModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzZmHEit6L7",
        "outputId": "71bc46b9-4c06-4844-f00e-9b4397b8e5ba"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxq_5vKjt9PR",
        "outputId": "e37ccffe-bd07-4b38-ff3a-8a4eb936db58"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8700YD7t_ua"
      },
      "source": [
        "# Manually pick cpu device if desired\n",
        "device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfK0gAeuB-0"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUn_O6vuFZV"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5P5eSNzyuJCY",
        "outputId": "117f4913-123b-4867-d0a3-aa363a3832fe"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "conv_val_data = conv_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13000\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FD7BFABA110>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW8fDU_nuyqO",
        "outputId": "80679e53-0eaf-40a2-adea-884384642d4b"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 40 labels in the training dataset\n",
            "there are 40 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '#', 2: '$', 3: '&', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '@', 15: 'A', 16: 'B', 17: 'C', 18: 'D', 19: 'E', 20: 'F', 21: 'G', 22: 'H', 23: 'I', 24: 'J', 25: 'K', 26: 'L', 27: 'M', 28: 'N', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z'}\n",
            "{'<EOS>': 0, '#': 1, '$': 2, '&': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6VrPTAwMBp"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # since lstm is being used but there is always only 1 char, no need to worry about parsing label\n",
        "      # just make one hot vector for char and end token and put them together in tensor\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86QKpTZozE8N"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDXQCzpLO8L"
      },
      "source": [
        "def toChars(nums, dict):\n",
        "  out = []\n",
        "  for num in nums:\n",
        "    out.append(dict[num])\n",
        "  return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASlls_LuDEM"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "class convLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen):\n",
        "    super(convLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm = nn.LSTM(256, 256)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    #h0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "    #c0 = torch.zeros(1, self.batchSize, 256).to(device)\n",
        "\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, self.batchSize, 256).to(device),\n",
        "            torch.zeros(1, self.batchSize, 256).to(device))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOt1YmJHfAE"
      },
      "source": [
        "def parsePred(pred):\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  #print(pred)\n",
        "  out = []\n",
        "  for i in pred:\n",
        "    #print(i[0])\n",
        "    out.append(labelDict[i.argmax(0).item()])\n",
        "\n",
        "  return \"\".join(out)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_69r9ZIEeO"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "7uDGucfozFod",
        "outputId": "e3870191-907d-4a46-84cf-cb1cf5b43630"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = LSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = LSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACYklEQVR4nFWTvY9WVRDGf8/MuVdZVnGJMYGAuHwFRQoCJBQU/AFQSKHxIzGx2E4NrZWUVPwBUCmtlYWViYUFGJKNSggEEjYbviTBrIHs+u69Z8bivi/va3Uy5/xmnpknZ5CEQGI4BDaEAOOjCEAa3w8ZYAbCyjgSAhy5kDEpC2CMExEUxqAZgFkBSEkD4q8sfvqmQRIhgwA5LhlycOyL1effb3fDzABRLGt7sNuca6LfcvTEq+s7X5//4MXV+48zKTWVgsXPzmttR5q6tvQlpKp67+61n5e78UTf5mb2NbvRaHM0Go1GGdllxMaXOKgkd+5vv3ntrwO/LW+0Ec6Oy7vWLr3x2nv7z/3wyDIxb/bu3SZsPDP7VrqVPdjC/j0uwyZWDoSZWLybax+5EIYwM5qiPqVIKSK9b2P+UKWkIxGGuj7lkaZMudWnP+G7t1jvNTPcJvYOtgLY0Qcx+goNHmNKowchRchEri6rObWQ2RsSlkqTSDLlGYk9WyaPbyMJMjHIsGwweVYEyb+YT4bGLO3zq291HlkplhDeQJcvgYB3Tr/fVLnoA8NCEe0UoInbc0sLoqYLDyIfvrA6laDj19UPl0oWKlSjaG5rDaYA7eMf4+ulthfphPXa524vAQR+/M/858IuGkyo/eZ5jStzU0CCxZOXc+XimXfb9vDZSxu1u35EzIiY4R8/zcgnf/z+qNaoN85oCjgGxcsnt/7uczMzuwffvS1spgV5BCrzxw53Hmj9lyfrTZ/MlMAMlwkJGbLCTAsaVtCEBjkXNiMx3jiBDBNy/vc6WURn+DqT2tMp/gOK8fO3/BfcrQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FD7B7B3B890>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted K4 for J<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBJlsmLndDk"
      },
      "source": [
        "# function to find accuracy for variable size output\n",
        "def findAccuracy(pred, labels):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  #print(pred.shape)\n",
        "  #print(labels.shape)\n",
        "\n",
        "  for p, l in zip(pred, labels):\n",
        "    for b in range(len(p)):\n",
        "      #print(p[b])\n",
        "      if p[b].argmax(0).item() == l[b]:\n",
        "        accuracy += 1\n",
        "\n",
        "  return accuracy/(pred.shape[0] * pred.shape[1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knuiNPbtZq7"
      },
      "source": [
        "# NOT USED\n",
        "\n",
        "# function to define loss for variable size output\n",
        "# works like categorical crossentropy for each of the char predictions\n",
        "def lstmLoss(pred, labels, lossFunc):\n",
        "  pred = pred.permute(1,0,2)\n",
        "  #print(pred.shape)\n",
        "  labels = labels.permute(1,0)\n",
        "  #print(labels.shape)\n",
        "  loss = []\n",
        "  for char, lab in zip(pred, labels):\n",
        "    #print(char.shape)\n",
        "    #print(lab.shape)\n",
        "    loss.append(lossFunc(char, lab))\n",
        "  return loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJIj4ZUcngqN",
        "outputId": "3cd40247-bada-46e5-a91b-77664bb2ccf6"
      },
      "source": [
        "#LSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(LSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "testLoss = []\n",
        "testAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      testLoss.append(running_loss)\n",
        "      testAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.47372 acc: 46.995%\n",
            "[1,  2000] loss: 4.44516 acc: 50.000%\n",
            "[1,  3000] loss: 4.34066 acc: 50.000%\n",
            "[1,  4000] loss: 4.23728 acc: 50.000%\n",
            "[1,  5000] loss: 4.13846 acc: 50.000%\n",
            "[1,  6000] loss: 4.01338 acc: 50.000%\n",
            "[1,  7000] loss: 3.84248 acc: 50.000%\n",
            "[2,  1000] loss: 3.41915 acc: 50.730%\n",
            "[2,  2000] loss: 3.12957 acc: 53.620%\n",
            "[2,  3000] loss: 2.80466 acc: 60.880%\n",
            "[2,  4000] loss: 2.49115 acc: 69.905%\n",
            "[2,  5000] loss: 2.17497 acc: 77.610%\n",
            "[2,  6000] loss: 1.89323 acc: 82.875%\n",
            "[2,  7000] loss: 1.63725 acc: 86.190%\n",
            "[3,  1000] loss: 1.25183 acc: 90.285%\n",
            "[3,  2000] loss: 1.08444 acc: 91.350%\n",
            "[3,  3000] loss: 0.93721 acc: 92.295%\n",
            "[3,  4000] loss: 0.82078 acc: 93.090%\n",
            "[3,  5000] loss: 0.72077 acc: 93.825%\n",
            "[3,  6000] loss: 0.65132 acc: 94.010%\n",
            "[3,  7000] loss: 0.58481 acc: 94.300%\n",
            "[4,  1000] loss: 0.48572 acc: 95.230%\n",
            "[4,  2000] loss: 0.44957 acc: 95.530%\n",
            "[4,  3000] loss: 0.41514 acc: 95.720%\n",
            "[4,  4000] loss: 0.39848 acc: 95.655%\n",
            "[4,  5000] loss: 0.39431 acc: 95.390%\n",
            "[4,  6000] loss: 0.35762 acc: 96.005%\n",
            "[4,  7000] loss: 0.34167 acc: 95.995%\n",
            "[5,  1000] loss: 0.30087 acc: 96.410%\n",
            "[5,  2000] loss: 0.29301 acc: 96.435%\n",
            "[5,  3000] loss: 0.28220 acc: 96.575%\n",
            "[5,  4000] loss: 0.26863 acc: 96.700%\n",
            "[5,  5000] loss: 0.26762 acc: 96.655%\n",
            "[5,  6000] loss: 0.25984 acc: 96.745%\n",
            "[5,  7000] loss: 0.25597 acc: 96.655%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "odSxfPhhDWBu",
        "outputId": "80290d52-4f36-4e8a-a044-1ebe67118499"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOuTSe6DnvB"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKIgoqbDw43",
        "outputId": "07d81870-038a-4563-fa71-a68c57947e2b"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = LSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      LSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = LSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.14879 acc: 88.275%\n",
            "[2,  1000] loss: 1.15740 acc: 88.100%\n",
            "[3,  1000] loss: 1.14069 acc: 88.100%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "YNzAl0F9EBqz",
        "outputId": "70487e40-fbf0-4462-d677-ec6f15e5bdd7"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVHwMT7H0wf"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsCornerDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      image = image * cv2.cornerHarris(image, 2, 3, 0.04)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVR__AEdH2VL"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "cornerTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdEzjgrH5oS"
      },
      "source": [
        "# init model with 40 classes on output layer\n",
        "# batch size = 10\n",
        "# 2 is max chars in sequence\n",
        "# put model in gpu if available\n",
        "CornerLSTMModel = convLSTM(40, 10, 2).to(device)\n",
        "#print(LSTMModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "ifHMtLREIDt1",
        "outputId": "d8e8edec-fb1b-41e7-d1b5-1ea7b0bd7422"
      },
      "source": [
        "testItem, testLabel = next(iter(cornerTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = CornerLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABRklEQVR4nI2TPU4DMRCFP292owWiDVEkOv5SgkKHuMM2SFtF4q9JhRDiCFwAkChyAFoaJERFAaKAloKWI9DSDoXXXs8aCaax38ybN/aMTQYJFHAOQMk2MD8lw9kA+sCIEh4oa++Q0A5gLHLGFKk9br0GuAu5wmQHeJwgVKfWl8MqIDararbswhbGCUrtlkBt0W6+BbFG2wwYjM+wHBGvkwOkHKokqQlSKwCvgfSv1UZRaXePy5mFS+0a4Y1SlsPoSh8fAhDbBtOkG7XufcbllcCtQiBXLQJdFee91e5U5/PSrnAMCaZhPKOncbSuBfLorCcUCg8iRk+huSi6kEQpdt52lr1ux81TTMgBfG8z03h9kxXDFWkeVvDE/muzP+LDtqNSKOGL+wDv87QJb0F/swT4EBAYFwA3oCbAWt6BjQuavhf+k8MPi65y/Dl+f5kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F414365AB50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted U<EOS> for U<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrgUswNJbJ9",
        "outputId": "c481b633-7822-445a-cd71-1e5c5286df0e"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "LSTMtestLoss = []\n",
        "LSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      LSTMtestLoss.append(running_loss)\n",
        "      LSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 5.56157 acc: 48.400%\n",
            "[1,  2000] loss: 4.47824 acc: 50.000%\n",
            "[1,  3000] loss: 4.39765 acc: 50.000%\n",
            "[1,  4000] loss: 4.30606 acc: 50.000%\n",
            "[1,  5000] loss: 4.19796 acc: 50.000%\n",
            "[1,  6000] loss: 4.07397 acc: 50.000%\n",
            "[1,  7000] loss: 3.94622 acc: 50.030%\n",
            "[2,  1000] loss: 3.68766 acc: 50.540%\n",
            "[2,  2000] loss: 3.55134 acc: 51.310%\n",
            "[2,  3000] loss: 3.40052 acc: 53.245%\n",
            "[2,  4000] loss: 3.23160 acc: 56.195%\n",
            "[2,  5000] loss: 3.06545 acc: 59.390%\n",
            "[2,  6000] loss: 2.87635 acc: 62.235%\n",
            "[2,  7000] loss: 2.66348 acc: 65.705%\n",
            "[3,  1000] loss: 2.30184 acc: 71.535%\n",
            "[3,  2000] loss: 2.09118 acc: 74.880%\n",
            "[3,  3000] loss: 1.90122 acc: 78.030%\n",
            "[3,  4000] loss: 1.70102 acc: 80.930%\n",
            "[3,  5000] loss: 1.53972 acc: 83.295%\n",
            "[3,  6000] loss: 1.37654 acc: 85.670%\n",
            "[3,  7000] loss: 1.23783 acc: 87.500%\n",
            "[4,  1000] loss: 1.00445 acc: 90.135%\n",
            "[4,  2000] loss: 0.89318 acc: 91.390%\n",
            "[4,  3000] loss: 0.82866 acc: 91.710%\n",
            "[4,  4000] loss: 0.74795 acc: 92.360%\n",
            "[4,  5000] loss: 0.68584 acc: 92.930%\n",
            "[4,  6000] loss: 0.63772 acc: 93.425%\n",
            "[4,  7000] loss: 0.58845 acc: 93.590%\n",
            "[5,  1000] loss: 0.49379 acc: 94.715%\n",
            "[5,  2000] loss: 0.46836 acc: 94.775%\n",
            "[5,  3000] loss: 0.43274 acc: 95.055%\n",
            "[5,  4000] loss: 0.41549 acc: 95.290%\n",
            "[5,  5000] loss: 0.39713 acc: 95.315%\n",
            "[5,  6000] loss: 0.37941 acc: 95.330%\n",
            "[5,  7000] loss: 0.36549 acc: 95.495%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "w7yE-KSGJstq",
        "outputId": "670656bd-fca6-4200-a6a1-422925619d14"
      },
      "source": [
        "print(len(testAcc))\n",
        "print(len(LSTMtestAcc))\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(testAcc, color=\"tab:orange\", label=\"Raw Data\")\n",
        "ax.plot(LSTMtestAcc, color=\"tab:blue\", label=\"Corner Detection\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR2cb85uGhEm"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsCornerDataset(X=conv_val_data, classToNum=invertedLabelDict)\n",
        "cornerVal_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhV9wHjEGGNx",
        "outputId": "5349f2b9-eba7-4606-d2be-68a3f770127e"
      },
      "source": [
        "num_epochs = 3\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(cornerVal_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = CornerLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      CornerLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = CornerLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      #loss.backward()\n",
        "      #opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      valLoss.append(running_loss)\n",
        "      valAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.25047 acc: 86.995%\n",
            "[2,  1000] loss: 1.24245 acc: 86.915%\n",
            "[3,  1000] loss: 1.24736 acc: 86.915%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "wGU8SBiRGT8m",
        "outputId": "161a322c-e867-4139-f067-45d2e7aa5305"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(valAcc, color=\"tab:orange\")\n",
        "ax.set_ylim([0,100])\n",
        "ax.set_title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUwklEQVR4nO3de7SldX3f8fcnjIggygAjEkAuOikLkqpkShCtUbEVUTMksQRRHAgtQU2qMdGY2KrLJK1ZqysYm1ZLBRwsQShRoUZU5BKrCDooV0EZUC4TLiNytxqRb//Yv8HNyTkz++x99pnhx/u11ln7eX6/5/Ldv/PMZz/nefbek6pCktSXn9vcBUiSFp7hLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdiy5JJXlOm/5Ikv84yrJj7Of1Sb4wbp3S45nhrnlL8rkk75+lfWWSO5IsGXVbVXVCVf3pAtS0V3sheHTfVXV6Vf3rSbe9kX3uneSRJB+e1j6kcRnuGsdq4A1JMqP9aOD0qnp4M9S0ObwRuAf4rSRPXswdJ9lqMfenxx/DXeP4NLAT8C83NCRZCrwaOC3JgUm+muTeJLcn+eskW8+2oSQfS/JnQ/PvaOv8Q5LfnrHsq5J8M8n9SW5N8r6h7i+1x3uTPJjkBUmOSfLlofUPTvL1JPe1x4OH+i5O8qdJvpLkgSRfSLLzXAPQXtjeCPwH4CfAa2b0r0xyRav1xiSHtvYdk5zant89ST7d2h9Ta2sbvnz1sSQfTvLZJA8BL93EeJDkRUkuab+HW9s+/kWSO4dfHJL8RpIr53quenwy3DVvVfX/gLMYhNsGRwDXV9WVwE+B3wd2Bl4AHAK8eVPbbQH4h8C/ApYDL5+xyENtnzsArwLelOTw1vfi9rhDVT21qr46Y9s7An8HfIjBC9NfAn+XZKehxY4CjgWeAWzdapnLi4DdgU8wGItVQ/s6EDgNeEer9cXA91r3x4Ftgf3bfk7cyD5mOgr4c2B74MtsZDyS7AmcB/xXYBnwPOCKqvo6cDcwfLnq6FavOmK4a1yrgdcm2abNv7G1UVWXV9WlVfVwVX0P+B/Ar46wzSOAU6vqmqp6CHjfcGdVXVxVV1fVI1V1FXDGiNuFQfjdUFUfb3WdAVzPY8+4T62q7wy9eD1vI9tbBZxXVfcAfwMcmuQZre844JSqOr/Vuq6qrk+yK/BK4ISquqeqflJVfz9i/QDnVNVX2jZ/tInxOAr4YlWd0fZzd1Vd0fpWA2+AR1/0XtGegzpiuGssVfVl4PvA4UmeDRxIC4gkv5DkM+3m6v3Af2JwFr8pPw/cOjR/83Bnkl9JclGS9UnuA04Ycbsbtn3zjLabgd2G5u8Ymv4h8NTZNpTkKcC/AU4HaH8l3MIgUAH2AG6cZdU9gB+0F4RxDI/NpsZjrhoA/hfwmiTbMXhB/b9VdfuYNWkLZbhrEqcxOGN/A/D5qrqztX+YwVnx8qp6GvAnwMybr7O5nUEobfCsGf1/A5wL7FFVTwc+MrTdTX296T8Ae85oexawboS6Zvp14GnAf28vYHcweJHYcGnmVuDZs6x3K7Bjkh1m6XuIweUaAJI8c5ZlZj7HjY3HXDVQVeuArwK/weCSzMdnW06Pb4a7JnEag+vi/452SabZHrgfeDDJvsCbRtzeWcAxSfZLsi3w3hn92zM48/1Ru6591FDfeuARYJ85tv1Z4BeSHJVkSZLfAvYDPjNibcNWAacAv8Tg0s3zgBcCz03yS8DJwLFJDknyc0l2S7JvOzs+j8GLwtIkT0qy4V7BlcD+SZ7XLnW9b4Q6NjYepwMvT3JEe747JRm+zHQa8M72HD45xhhoC2e4a2ztevolwHYMziA3+EMGQfMA8D+BM0fc3nnAB4ELgbXtcdibgfcneQB4D4MXgw3r/pDBzcavtHeHHDRj23czeDfPHzC4ofhO4NVV9f1RatsgyW4MbhB/sKruGPq5HPgcsKqqvsbgxuyJwH3A3/OzvxqOZvDumuuBu4C3tfq+A7wf+CJwA4MbppuysfG4BTisPd8fAFcAzx1a91Otpk+1sVNn4n/WIT0xJbkR+J2q+uLmrkULzzN36QkoyW8yuIY/868jdWKT4Z7klCR3JblmqG3HJOcnuaE9Lm3tSfKhJGuTXJXkgGkWL2n+klzM4Kb3W6rqkc1cjqZklDP3jwGHzmh7F3BBVS0HLmjzMHgP7/L2czyDA0jSFqSqXlJVz6iqz2/uWjQ9mwz3qvoSgxsyw1bys3dHrAYOH2o/rQYuBXZoH9yQJC2ikb+9b4Zdhj70cAewS5vejcd+0OK21vZPPiCR5HgGZ/dst912v7zvvvuOWYokPTFdfvnl36+qZbP1jRvuj6qqSjLvt9xU1UnASQArVqyoNWvWTFqKJD2hJJn5qetHjftumTs3XG5pj3e19nU89hOGuzPeJwAlSRMYN9zP5WcftV4FnDPU/sb2rpmDgPv8zgpJWnybvCyT5AzgJcDOSW5j8JHwDwBnJTmOwZcvHdEW/yyDT8WtZfDFS8dOoWZJ0iZsMtyr6nVzdB0yy7IFvGXSoiRJk/ETqpLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdWrK5C5jIj+6HH98/1JDH9ifz78uM5cbu2xLqWOTx2NgYSFpUj+9wv/xUOP89m7sKbdKW8qK7AHVIC+0Vfw4HHL3gm318h/uzD4GnLB1MV83oHJofuW/Gcv9kvbn6xtnXuHWM2Dd2HXO0L3od0x6PudaZx3rSQth5+VQ2+/gO92f+4uBHkvQY3lCVpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1KGJwj3J7ye5Nsk1Sc5Isk2SvZNclmRtkjOTbL1QxUqSRjN2uCfZDfj3wIqq+kVgK+BI4C+AE6vqOcA9wHELUagkaXSTXpZZAjwlyRJgW+B24GXA2a1/NXD4hPuQJM3T2OFeVeuA/wLcwiDU7wMuB+6tqofbYrcBu822fpLjk6xJsmb9+vXjliFJmsUkl2WWAiuBvYGfB7YDDh11/ao6qapWVNWKZcuWjVuGJGkWk1yWeTnw3apaX1U/AT4JvBDYoV2mAdgdWDdhjZKkeZok3G8BDkqybZIAhwDfAi4CXtuWWQWcM1mJkqT5muSa+2UMbpx+A7i6besk4I+AtydZC+wEnLwAdUqS5mGi/2avqt4LvHdG803AgZNsV5I0GT+hKkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDE4V7kh2SnJ3k+iTXJXlBkh2TnJ/khva4dKGKlSSNZtIz978CPldV+wLPBa4D3gVcUFXLgQvavCRpEY0d7kmeDrwYOBmgqv6xqu4FVgKr22KrgcMnLVKSND+TnLnvDawHTk3yzSQfTbIdsEtV3d6WuQPYZbaVkxyfZE2SNevXr5+gDEnSTJOE+xLgAODDVfV84CFmXIKpqgJqtpWr6qSqWlFVK5YtWzZBGZKkmSYJ99uA26rqsjZ/NoOwvzPJrgDt8a7JSpQkzdfY4V5VdwC3JvlnrekQ4FvAucCq1rYKOGeiCiVJ87ZkwvV/Dzg9ydbATcCxDF4wzkpyHHAzcMSE+5AkzdNE4V5VVwArZuk6ZJLtSpIm4ydUJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDk0c7km2SvLNJJ9p83snuSzJ2iRnJtl68jIlSfOxEGfubwWuG5r/C+DEqnoOcA9w3ALsQ5I0DxOFe5LdgVcBH23zAV4GnN0WWQ0cPsk+JEnzN+mZ+weBdwKPtPmdgHur6uE2fxuw22wrJjk+yZoka9avXz9hGZKkYWOHe5JXA3dV1eXjrF9VJ1XViqpasWzZsnHLkCTNYskE674Q+LUkhwHbAE8D/grYIcmSdva+O7Bu8jIlSfMx9pl7Vf1xVe1eVXsBRwIXVtXrgYuA17bFVgHnTFylJGlepvE+9z8C3p5kLYNr8CdPYR+SpI2Y5LLMo6rqYuDiNn0TcOBCbFeSNB4/oSpJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ2OHe5I9klyU5FtJrk3y1ta+Y5Lzk9zQHpcuXLmSpFFMcub+MPAHVbUfcBDwliT7Ae8CLqiq5cAFbV6StIjGDvequr2qvtGmHwCuA3YDVgKr22KrgcMnLVKSND8Lcs09yV7A84HLgF2q6vbWdQewyxzrHJ9kTZI169evX4gyJEnNxOGe5KnA3wJvq6r7h/uqqoCabb2qOqmqVlTVimXLlk1ahiRpyEThnuRJDIL99Kr6ZGu+M8murX9X4K7JSpQkzdck75YJcDJwXVX95VDXucCqNr0KOGf88iRJ41gywbovBI4Grk5yRWv7E+ADwFlJjgNuBo6YrERJ0nyNHe5V9WUgc3QfMu52JUmT8xOqktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtSh6YS7kkOTfLtJGuTvGsa+5AkzW3Bwz3JVsB/A14J7Ae8Lsl+C70fSdLcpnHmfiCwtqpuqqp/BD4BrJzCfiRJc1gyhW3uBtw6NH8b8CszF0pyPHB8m30wybfH3N/OwPfHXHearGt+rGv+ttTarGt+Jqlrz7k6phHuI6mqk4CTJt1OkjVVtWIBSlpQ1jU/1jV/W2pt1jU/06prGpdl1gF7DM3v3tokSYtkGuH+dWB5kr2TbA0cCZw7hf1Ikuaw4JdlqurhJL8LfB7YCjilqq5d6P0MmfjSzpRY1/xY1/xtqbVZ1/xMpa5U1TS2K0najPyEqiR1yHCXpA5t0eG+qa8xSPLkJGe2/suS7DXU98et/dtJXrHIdb09ybeSXJXkgiR7DvX9NMkV7WdBbzSPUNcxSdYP7f/fDvWtSnJD+1m1yHWdOFTTd5LcO9Q3zfE6JcldSa6Zoz9JPtTqvirJAUN9UxmvEWp6favl6iSXJHnuUN/3WvsVSdYsVE3zqO0lSe4b+n29Z6hval9JMkJd7xiq6Zp2TO3Y+qYyZkn2SHJRy4Frk7x1lmWme3xV1Rb5w+Bm7I3APsDWwJXAfjOWeTPwkTZ9JHBmm96vLf9kYO+2na0Wsa6XAtu26TdtqKvNP7gZx+sY4K9nWXdH4Kb2uLRNL12sumYs/3sMbsJPdbzatl8MHABcM0f/YcB5QICDgMsWYbw2VdPBG/bF4Cs+Lhvq+x6w82Ycr5cAn5n0GFjoumYs+xrgwmmPGbArcECb3h74ziz/Hqd6fG3JZ+6jfI3BSmB1mz4bOCRJWvsnqurHVfVdYG3b3qLUVVUXVdUP2+ylDN7rP22TfO3DK4Dzq+oHVXUPcD5w6Gaq63XAGQu0742qqi8BP9jIIiuB02rgUmCHJLsyxfHaVE1VdUnbJyzesbVh35sar7lM9StJ5lnXohxfVXV7VX2jTT8AXMfg0/vDpnp8bcnhPtvXGMwcnEeXqaqHgfuAnUZcd5p1DTuOwavzBtskWZPk0iSHL1BN86nrN9ufgGcn2fBhsy1ivNrlq72BC4eapzVeo5ir9mmO13zMPLYK+EKSyzP4eo/N4QVJrkxyXpL9W9sWMV5JtmUQkn871Dz1McvgcvHzgctmdE31+NpsXz/wRJDkDcAK4FeHmvesqnVJ9gEuTHJ1Vd24SCX9H+CMqvpxkt9h8FfPyxZp36M4Eji7qn461LY5x2uLleSlDML9RUPNL2pj9Qzg/CTXt7PaxfINBr+vB5McBnwaWL6I+9+U1wBfqarhs/ypjlmSpzJ4MXlbVd2/UNsdxZZ85j7K1xg8ukySJcDTgbtHXHeadZHk5cC7gV+rqh9vaK+qde3xJuBiBq/oi1JXVd09VMtHgV8edd1p1jXkSGb8yTzF8RrFXLVv1q/YSPLPGfz+VlbV3Rvah8bqLuBTLNylyJFU1f1V9WCb/izwpCQ7s+V8JcnGjq8FH7MkT2IQ7KdX1SdnWWS6x9dC30hYwBsSSxjcSNibn92E2X/GMm/hsTdUz2rT+/PYG6o3sXA3VEep6/kMbiAtn9G+FHhym94ZuIEFurE0Yl27Dk3/OnBp/ewGzndbfUvb9I6LVVdbbl8GN7eyGOM1tI+9mPsG4at47A2vr017vEao6VkM7iEdPKN9O2D7oelLgEMXcqxGqO2ZG35/DELyljZ2Ix0D06qr9T+dwXX57RZjzNrzPg344EaWmerxtaC/+CkcSIcxuMt8I/Du1vZ+BmfDANsA/7sd7F8D9hla991tvW8Dr1zkur4I3Alc0X7Obe0HA1e3g/tq4LhFrus/A9e2/V8E7Du07m+3cVwLHLuYdbX59wEfmLHetMfrDOB24CcMrmseB5wAnND6w+A/nrmx7X/FtMdrhJo+CtwzdGytae37tHG6sv2O372QYzVibb87dHxdytAL0GzHwGLV1ZY5hsGbLIbXm9qYMbhcVsBVQ7+rwxbz+PLrBySpQ1vyNXdJ0pgMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktSh/w8wg4R/0FFaEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "k56MY3j5mDtY",
        "outputId": "cee22130-eba9-457b-c113-9048ea3cedc2"
      },
      "source": [
        "# experiment with effects of FAST  detection\n",
        "print(\"original image\")\n",
        "input = np.float32(conv_train_data[61345][0])\n",
        "#input = cv2.cvtColor(input, cv2.COLOR_GRAY2RGB)\n",
        "cv2_imshow(input)\n",
        "#print(input.shape)\n",
        "corners = cv2.goodFeaturesToTrack(input,25,0.01,10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "out = np.zeros((32,32), dtype=float)\n",
        "for i in corners:\n",
        "  x,y = i.ravel()\n",
        "  cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "print(\"corner detection\")\n",
        "cv2_imshow(out)\n",
        "\n",
        "combo = np.concatenate([input,out],1)\n",
        "print(\"concatenated\")\n",
        "cv2_imshow(np.float32(combo))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACWklEQVR4nG1TTUiUURQ95773fTPmP5IUQSHRJjNqkUJEhFi5yiAYatcmomgRZCAkBG2CfqgWgVCLQGvZKohIESrCXdBgkS0UQVCxwvydme+92+J9I4nztvdw3jnnngtUeAYEYcFKQ0khAgCkbAGQBAUABVKZwoBChpmtMCcMYetoRbbypxq3nbz7PlcFsLLOzNGXqwVdO08BTIUv6p7Mq3fq38WpDpAGAAEBTNw9uqbqE6/56gAgARghIWR8oH+xpMnCs1ze52tDLITYMlPdxbl11T+vO+PsA83XUEw5OQoQof7Ksvdu8kYj0DzuxqsAgQEkjSPTdm9Fk7Xn7TENuxb1S0MwKQAIi57BuRWXzPQ2AxD0azIgJAjrIaqSRN05sf7bcn0GpD99KeEHT0AZdGLH7RnV1cc3EzfYujtz4kdJv+4CIBJ8snWopE5HsmeLyfrk2NPZRH/3GZCQkEPtI3VO13vihhfOe02c/3W1KfUHANH1laJzOtQo2PlGk8Ls/NjlCALChB+2T2iiWrwACHqd/364rUUgwaAAtNe806JOHycs2qb15z4ClpJmTBxacKo6dYyIwMytVT/ctJEeBJb3vXq/NFgrgAFbFkq+rz0SA5OWU15550p34nT51Q+dc3Nvc3v2dzalFeka/Th8rqrcDuydLnmnycSUP8Wwrig2WdgNAM98+qve+aXPHVYAguIEHtS0cp7VRxql4+DASKGAqAQDRGmvAkEEgUW2RiAhSJbPbQNCIWjDFg1gyP8xUl4BAWPSKkM2kQhBMJwEARhsup9wAwQAC+IffyXYbeG0Qp0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FD7B6EAA1D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corner detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAANklEQVR4nGNgGAWUgP9wFhMO+f9YxbGZQG+AzWYmFPnB63bCAK/bmHD77j8WFor8/6EcKqQCADKlEPb+mSiDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FD7B6EAA250>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "concatenated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACoUlEQVR4nJ2VT0gVURTGv+/cmXnP/I8kRVBItMmMWqQQESFWrjIIpHZtIooWQQZCQtAm6A/VIhBqEWgtWwURKUJFuAt6WGQLRRBUrDD/Pt/ce1rMe/q0p46e5cyc33z3nO+cCxQIA4LwwEIvNwrJIgQASNk0gCQoACiQrUkwoJBRrreFfMIQXhk9kc3rj8JsO3n3fWsRwK3VMXH05Vxa589TALP5dCl7MqHOqnsXMObvSQOAgAAmaO6bV3Wh01RxPAAJwAgJIYMDHVMZDSeftaZcqhTI2eK/0DwAxMuhyi6OL6j+ed0YJB9oqoSyVglUlwmR4wTwUX5lxjk7dKMSqB6wA0WArFXFPAWQrF0SdfdmNZx/Xh/QsGlKv1TEa6IAIDy0dI3P2nC0rRqAoEPDTiFjEDwHUZXQb24Vz32bKU+AdKcvhfzgCCh1Q4SAwI7bo6pzj2+Gtqt2d+LEj4x+3QVglZkLw0iwtjujVnuTZxfDhaH+p2Oh/m43IFd2Mb/2K/JR+kit1YWWoOKFdU5D635drcr2Z2MFgH99dtFa7a4U7HyjYXpsov+yDwFjjQKJ7YMaqi5eAARt1n0/XFcjkKhBMSaa3jVndVFHjhMe6kb05z4CHmXZo+vn49CkVdXhY4QPJm7NuZ6qJXfFUODxvlPnprtKBTBgzWTGtdf7YmCyy3HN2mVDXjlrM3eCbL2LH1prx9+27tnfWBU9Kdy9PG5T38eec0VLw793JOOshoPD7lS0I9ZQkOMa+IFJwlsC8Mynv+qsm/7c4K1bgSifoFiBQ8704lh8pFIaDnb2ptPwM+ufH4AB/Oxeiog+BB6SJQJBrHnm6l6RQtCLpiyGFw2Zz5DcCBAwJoYCiW7SfBFCEDQ5fRtLWPlZtOMJINYF/w8Z8+1Yf7dcRQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7FD7B6EAA3D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O47MxWFyVMSZ"
      },
      "source": [
        "# dataset class with corner detection added (may make things more difficult)\n",
        "class handwrittenCharsGFTTDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # apply corner detection to image\n",
        "      image = np.float32(self.images[index])\n",
        "      corners = cv2.goodFeaturesToTrack(image,25,0.01,10)\n",
        "      if (corners is not None):\n",
        "        corners = np.int0(corners)\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "        for i in corners:\n",
        "          x,y = i.ravel()\n",
        "          cv2.circle(out, (x,y), 1, 255, -1)\n",
        "\n",
        "      else:\n",
        "        out = np.zeros((32,32), dtype=float)\n",
        "\n",
        "      combo = np.concatenate([image,out],1)\n",
        "      #cv2_imshow(combo)\n",
        "      image = torch.tensor(combo)\n",
        "      #print(image.shape)\n",
        "\n",
        "      char = self.classToNum[self.labels[index]]\n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      label = tensor([char, end])\n",
        "\n",
        "      #image = self.transform(image)\n",
        "      #print(image.shape)\n",
        "      #cv2_imshow(np.float32(image))\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      #T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g07HWPEHXJ53"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsGFTTDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "GFTTTrain_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)\n",
        "GFTTLSTMModel = convLSTM(40, 10, 2).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "lKe4Rj7RXW6E",
        "outputId": "217aedfe-355e-44e3-fde8-0d646eecddf4"
      },
      "source": [
        "testItem, testLabel = next(iter(GFTTTrain_dl))\n",
        "#print(f\"Feature batch shape: {testItem.size()}\")\n",
        "#print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "#print(testItem[0].cpu().numpy()[0].shape)\n",
        "image = testItem[0].cpu().numpy()\n",
        "cv2_imshow(image)\n",
        "\n",
        "hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "output2, hidden = GFTTLSTMModel(testItem.to(device), hidden)\n",
        "\n",
        "pred1 = labelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = labelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2, \"for\", \"\".join(toChars(testLabel, labelDict)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAAAAACH9iFYAAACXElEQVR4nJ2VPWgVURCFvzN334uJColNNGj8I5JSSOUf2FtGBQXrFAFFawsry5QWYh1BexURsREbsTCgQRQtNGqQRPJjTN7ujsXNi7v5e5ucbmHn3DPnnpkLBQiZAVj8NkNURSCsFAbMCEKoOoEkTBhgCAIyahXqChL2/HKQgzxk6g42P4V8k2JXgSDko2fHH3wkS45eTkk8Hdilm7ctL/9erqdkkZ7lqXua+lLu3sg9m3jVTVL43deI8WILIeu8m/yt42rYRffZxyOfpigKWKtgFZYNN/qnsz/3DiKw5rVUgjAQujKTTQzaVioLCMbVJX93JGzp6GYxBDg2mb4+QNG8qhAIXZ/LX3aG6vErERjUJv1FD5BsYQZWELBTb7OxAWTbUgDWNu7fzyxr2YaJ6nrjP09LhtiKBklx/HYMe2MYxXGMMlDZi40GywDphs9dCgkoLoa4XErzvM40QEws4tBMNiKogQjxIrXaivUVGNTE4FP/0mMl9wyrksh4WOfzdLEnshmGqZmFKm5aoOu+/zgXTIb19vS29x2OB0uhZbWQs/PJCRua6ri2VG/Y8dn6t76F94x+fbTglDfCRmh7mHn+2/PUPXNfnMvd09wbnztWX2MZ0dLE8trw+dSmfXQ+uyOwxSxJPKkNnZxalG949eArK+rWjI9d2N0hKDwNor2thYFN7r0z/qHO/4dEkiySxQ42W+yA+hcmOhNDMYEWN4OABAltkL8CAfv3STQzjOJyVZyHRK0VGEksTQAFmuEJKFQJkhDIkBFtiCKEMOqtgwSx48gS+y6Qt16u/wCvHrQH9mLMYgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x32 at 0x7FD7B7B3B6D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-34643acf8563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# predict twice with same image but different hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFTTLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-dbd35ecf87f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# resnet layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 1, 2, 2], but got 3-dimensional input of size [10, 32, 64] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "CB_1EGIYXvw_",
        "outputId": "e119946b-2baf-45c2-830d-1d4e922b074b"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(CornerLSTMModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 5\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "GFTTLSTMtestLoss = []\n",
        "GFTTLSTMtestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(GFTTTrain_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    hidden = GFTTLSTMModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      GFTTLSTMModel.zero_grad()\n",
        "      #hidden[0].detach_()\n",
        "      #hidden[1].detach_()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, hidden = GFTTLSTMModel(images, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    if i % 1000 == 999:\n",
        "      GFTTLSTMtestLoss.append(running_loss)\n",
        "      GFTTLSTMtestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 10))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 7.33560 acc: 1.300%\n",
            "[1,  2000] loss: 7.33449 acc: 1.350%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-239-367cfeb3a891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo3EtyKa3Ukl",
        "outputId": "912204e1-4b83-495e-b908-9ed9d7689370"
      },
      "source": [
        "!unzip captcha.zip"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  captcha.zip\n",
            "   creating: captcha/\n",
            "  inflating: captcha/captchaTrainSmall.npy  \n",
            "  inflating: captcha/captchaValSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-6hB3gsc3gNF",
        "outputId": "d77f763e-1124-4416-ae3f-183b8bf41093"
      },
      "source": [
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "with open(\"captcha/captchaTrainSmall.npy\", \"rb\") as f:\n",
        "    capt_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"captcha/captchaValSmall.npy\", \"rb\") as f:\n",
        "    capt_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "#capt_val_data = capt_val_data[:13000]\n",
        "\n",
        "print(\"training data size:\", len(capt_train_data))\n",
        "print(\"validation data size:\",len(capt_val_data))\n",
        "\n",
        "item = capt_train_data[123]\n",
        "\n",
        "print(\"training data shape:\", item[0].shape)\n",
        "cv2_imshow(item[0])\n",
        "print(\"data type of image =\", type(item[0]))\n",
        "print(\"training data label:\", item[1])\n",
        "print(\"each index in dataset has image (50x200) and char label\")\n",
        "print(item[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 308\n",
            "validation data size: 14\n",
            "training data shape: (50, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=200x50 at 0x7FD7B799E2D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: 8p7n\n",
            "each index in dataset has image (50x200) and char label\n",
            "[[192 192 192 ... 230 230 230]\n",
            " [192 192 192 ... 230 230 230]\n",
            " [192 192 192 ... 230 230 230]\n",
            " ...\n",
            " [230 230 230 ... 230 230 230]\n",
            " [230 230 230 ... 230 230 230]\n",
            " [230 230 230 ... 230 230 230]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W03RFn0LzeR",
        "outputId": "e166750e-167c-43f0-9023-7420324a2cf2"
      },
      "source": [
        "labels = []\n",
        "# used for stopping prediction of recurrent layers\n",
        "labels.append(\"<EOS>\") \n",
        "for i in capt_train_data:\n",
        "  label = i[1]\n",
        "  for char in label:\n",
        "    if char not in labels:\n",
        "      labels.append(char)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in capt_val_data:\n",
        "  label = i[1]\n",
        "  for char in label:\n",
        "    if char not in labels:\n",
        "      labels.append(char)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "captLabelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  captLabelDict[i] = labels[i]\n",
        "\n",
        "print(captLabelDict)\n",
        "invertedCaptDict = {y:x for x,y in captLabelDict.items()}\n",
        "print(invertedCaptDict)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 37 labels in the training dataset\n",
            "there are 37 labels in the validation dataset\n",
            "{0: '<EOS>', 1: '0', 2: '1', 3: 's', 4: 'e', 5: '3', 6: 'a', 7: '4', 8: 'r', 9: '5', 10: 'm', 11: 'b', 12: 'g', 13: 'v', 14: 'f', 15: 'i', 16: 'u', 17: 'j', 18: 'k', 19: '9', 20: 'l', 21: 't', 22: 'x', 23: '7', 24: 'w', 25: 'z', 26: '2', 27: 'y', 28: '8', 29: 'd', 30: 'h', 31: 'n', 32: 'q', 33: 'o', 34: 'p', 35: '6', 36: 'c'}\n",
            "{'<EOS>': 0, '0': 1, '1': 2, 's': 3, 'e': 4, '3': 5, 'a': 6, '4': 7, 'r': 8, '5': 9, 'm': 10, 'b': 11, 'g': 12, 'v': 13, 'f': 14, 'i': 15, 'u': 16, 'j': 17, 'k': 18, '9': 19, 'l': 20, 't': 21, 'x': 22, '7': 23, 'w': 24, 'z': 25, '2': 26, 'y': 27, '8': 28, 'd': 29, 'h': 30, 'n': 31, 'q': 32, 'o': 33, 'p': 34, '6': 35, 'c': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtmB4UNuKzJi"
      },
      "source": [
        "# dataset class\n",
        "class captchaDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      # turn all chars of label into numbers\n",
        "      chars = []\n",
        "      for i in range(len(self.labels[index])):\n",
        "        char = self.classToNum[self.labels[index][i]]\n",
        "        chars.append(char)\n",
        "      # don't forget end of sentence \n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      chars.append(end)\n",
        "      # turn it all into a tensor\n",
        "      label = tensor(chars)\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9KspcibMjRy"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = captchaDataset(X=capt_train_data, classToNum=invertedCaptDict)\n",
        "capt_train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True, drop_last=True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoLfH33-N0CY"
      },
      "source": [
        "# recurrent conv model to look at image and predict chars until all are read\n",
        "# uses resnet structure\n",
        "\n",
        "# Captcha version!\n",
        "\n",
        "class captchaLSTM(nn.Module):\n",
        "  def __init__(self, numClasses, batchSize, maxLen, lstmDepth):\n",
        "    super(captchaLSTM, self).__init__()\n",
        "    self.numClasses = numClasses\n",
        "    self.batchSize = batchSize\n",
        "    self.maxLen = maxLen\n",
        "    self.lstmDepth = lstmDepth\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    # 1536 is resulting size from captcha images\n",
        "    self.l1 = nn.Linear(1536, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    # input size, hidden size, num layers\n",
        "    self.lstm1 = nn.LSTM(256, 256, self.lstmDepth)\n",
        "    # turn values to 0 with probability 0.2\n",
        "    self.drop1 = nn.Dropout(0.2)\n",
        "    self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x, h1):\n",
        "    # resnet layers\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    # reduce size of data and add dropout for better generalization\n",
        "    x = self.l1(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.drop2(x)\n",
        "\n",
        "    #print(x.shape)\n",
        "    \n",
        "    # reshape image encoding so it has time dim on front\n",
        "    x = x.reshape(1, self.batchSize, 256)\n",
        "\n",
        "    x, h1 = self.lstm1(x, h1)\n",
        "\n",
        "    # turn output to classes\n",
        "    x = self.l3(x)\n",
        "    return x, h1\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(self.lstmDepth, self.batchSize, 256).to(device),\n",
        "            torch.zeros(self.lstmDepth, self.batchSize, 256).to(device))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGHtkA1LNIiy"
      },
      "source": [
        "# create model instance\n",
        "# 37 possible classes to predict\n",
        "# 5 as max length pred because all images have 4 chars + eos\n",
        "captchaModel = captchaLSTM(37, 10, 5, 1).to(device)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "o9RxIIuvNRgS",
        "outputId": "51a1250f-e05c-40df-8c9a-e500c6019956"
      },
      "source": [
        "testItem, testLabel = next(iter(capt_train_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "h1 = captchaModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, h1 = captchaModel(testItem.to(device), h1)\n",
        "output2, h1 = captchaModel(testItem.to(device), h1)\n",
        "output3, h1 = captchaModel(testItem.to(device), h1)\n",
        "output4, h1 = captchaModel(testItem.to(device), h1)\n",
        "output5, h1 = captchaModel(testItem.to(device), h1)\n",
        "\n",
        "pred1 = captLabelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = captLabelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred3 = captLabelDict[output3[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred4 = captLabelDict[output4[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred5 = captLabelDict[output5[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2 + pred3 + pred4 + pred5, \"for\", \"\".join(toChars(testLabel, captLabelDict)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 50, 200])\n",
            "Labels batch shape: torch.Size([10, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=200x50 at 0x7F5B61B0A550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted gzzzz for 2zna<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHxOmXN9Nonn",
        "outputId": "f24f513f-373e-4230-b9f5-65cc077d9035"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(captchaModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 2000\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "#captchaTestLoss = []\n",
        "#captchaTestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(capt_train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    h1 = captchaModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      captchaModel.zero_grad()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, h1 = captchaModel(images, h1)\n",
        "      h1 = (h1[0].detach(), h1[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    # if last batch of epoch\n",
        "    if i == 29 :\n",
        "      captchaTestLoss.append(running_loss)\n",
        "      captchaTestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %3d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 100, running_acc))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  30] loss: 2.16470 acc: 0.650%\n",
            "[2,  30] loss: 2.16196 acc: 0.900%\n",
            "[3,  30] loss: 2.15817 acc: 1.300%\n",
            "[4,  30] loss: 2.15547 acc: 1.700%\n",
            "[5,  30] loss: 2.15384 acc: 2.100%\n",
            "[6,  30] loss: 2.15106 acc: 2.600%\n",
            "[7,  30] loss: 2.14789 acc: 2.550%\n",
            "[8,  30] loss: 2.14575 acc: 2.900%\n",
            "[9,  30] loss: 2.14292 acc: 3.250%\n",
            "[10,  30] loss: 2.13976 acc: 3.100%\n",
            "[11,  30] loss: 2.13954 acc: 3.400%\n",
            "[12,  30] loss: 2.13626 acc: 3.150%\n",
            "[13,  30] loss: 2.13269 acc: 2.950%\n",
            "[14,  30] loss: 2.13059 acc: 3.300%\n",
            "[15,  30] loss: 2.12904 acc: 2.900%\n",
            "[16,  30] loss: 2.12628 acc: 3.150%\n",
            "[17,  30] loss: 2.12372 acc: 3.050%\n",
            "[18,  30] loss: 2.12191 acc: 3.400%\n",
            "[19,  30] loss: 2.12002 acc: 2.800%\n",
            "[20,  30] loss: 2.11755 acc: 2.550%\n",
            "[21,  30] loss: 2.11526 acc: 2.600%\n",
            "[22,  30] loss: 2.11319 acc: 2.750%\n",
            "[23,  30] loss: 2.10933 acc: 2.800%\n",
            "[24,  30] loss: 2.10818 acc: 2.500%\n",
            "[25,  30] loss: 2.10645 acc: 2.750%\n",
            "[26,  30] loss: 2.10471 acc: 2.950%\n",
            "[27,  30] loss: 2.10058 acc: 2.800%\n",
            "[28,  30] loss: 2.10042 acc: 2.400%\n",
            "[29,  30] loss: 2.09843 acc: 2.350%\n",
            "[30,  30] loss: 2.09535 acc: 2.500%\n",
            "[31,  30] loss: 2.09139 acc: 2.650%\n",
            "[32,  30] loss: 2.08987 acc: 2.400%\n",
            "[33,  30] loss: 2.08773 acc: 2.400%\n",
            "[34,  30] loss: 2.08654 acc: 2.550%\n",
            "[35,  30] loss: 2.08469 acc: 2.350%\n",
            "[36,  30] loss: 2.08095 acc: 2.700%\n",
            "[37,  30] loss: 2.07802 acc: 2.700%\n",
            "[38,  30] loss: 2.07575 acc: 2.900%\n",
            "[39,  30] loss: 2.07404 acc: 2.400%\n",
            "[40,  30] loss: 2.07355 acc: 2.800%\n",
            "[41,  30] loss: 2.07082 acc: 2.500%\n",
            "[42,  30] loss: 2.06734 acc: 3.100%\n",
            "[43,  30] loss: 2.06232 acc: 3.000%\n",
            "[44,  30] loss: 2.06336 acc: 2.650%\n",
            "[45,  30] loss: 2.05971 acc: 2.800%\n",
            "[46,  30] loss: 2.05639 acc: 2.800%\n",
            "[47,  30] loss: 2.05351 acc: 2.800%\n",
            "[48,  30] loss: 2.05185 acc: 2.950%\n",
            "[49,  30] loss: 2.04876 acc: 3.100%\n",
            "[50,  30] loss: 2.04794 acc: 2.950%\n",
            "[51,  30] loss: 2.04816 acc: 2.950%\n",
            "[52,  30] loss: 2.04053 acc: 3.600%\n",
            "[53,  30] loss: 2.03706 acc: 3.350%\n",
            "[54,  30] loss: 2.03449 acc: 3.250%\n",
            "[55,  30] loss: 2.03345 acc: 3.400%\n",
            "[56,  30] loss: 2.03184 acc: 3.400%\n",
            "[57,  30] loss: 2.02663 acc: 3.500%\n",
            "[58,  30] loss: 2.02330 acc: 3.750%\n",
            "[59,  30] loss: 2.02262 acc: 3.450%\n",
            "[60,  30] loss: 2.01497 acc: 3.750%\n",
            "[61,  30] loss: 2.01296 acc: 4.000%\n",
            "[62,  30] loss: 2.00916 acc: 4.350%\n",
            "[63,  30] loss: 2.00792 acc: 3.850%\n",
            "[64,  30] loss: 2.00446 acc: 4.200%\n",
            "[65,  30] loss: 1.99944 acc: 4.200%\n",
            "[66,  30] loss: 1.99967 acc: 4.450%\n",
            "[67,  30] loss: 1.99504 acc: 4.000%\n",
            "[68,  30] loss: 1.98565 acc: 5.100%\n",
            "[69,  30] loss: 1.98761 acc: 4.750%\n",
            "[70,  30] loss: 1.98276 acc: 4.700%\n",
            "[71,  30] loss: 1.97679 acc: 4.750%\n",
            "[72,  30] loss: 1.97313 acc: 5.400%\n",
            "[73,  30] loss: 1.97179 acc: 5.750%\n",
            "[74,  30] loss: 1.96428 acc: 5.850%\n",
            "[75,  30] loss: 1.95689 acc: 6.350%\n",
            "[76,  30] loss: 1.95385 acc: 5.950%\n",
            "[77,  30] loss: 1.94822 acc: 6.000%\n",
            "[78,  30] loss: 1.94523 acc: 6.300%\n",
            "[79,  30] loss: 1.94371 acc: 6.500%\n",
            "[80,  30] loss: 1.93512 acc: 6.700%\n",
            "[81,  30] loss: 1.92756 acc: 7.400%\n",
            "[82,  30] loss: 1.92745 acc: 7.100%\n",
            "[83,  30] loss: 1.91593 acc: 7.450%\n",
            "[84,  30] loss: 1.90998 acc: 7.700%\n",
            "[85,  30] loss: 1.90504 acc: 7.800%\n",
            "[86,  30] loss: 1.89742 acc: 8.150%\n",
            "[87,  30] loss: 1.89418 acc: 8.400%\n",
            "[88,  30] loss: 1.88883 acc: 8.700%\n",
            "[89,  30] loss: 1.88299 acc: 9.500%\n",
            "[90,  30] loss: 1.87121 acc: 9.700%\n",
            "[91,  30] loss: 1.86794 acc: 10.100%\n",
            "[92,  30] loss: 1.86328 acc: 10.200%\n",
            "[93,  30] loss: 1.85506 acc: 10.050%\n",
            "[94,  30] loss: 1.84373 acc: 10.300%\n",
            "[95,  30] loss: 1.84443 acc: 9.800%\n",
            "[96,  30] loss: 1.83317 acc: 10.750%\n",
            "[97,  30] loss: 1.82204 acc: 11.550%\n",
            "[98,  30] loss: 1.80929 acc: 11.000%\n",
            "[99,  30] loss: 1.80457 acc: 11.500%\n",
            "[100,  30] loss: 1.78867 acc: 12.000%\n",
            "[101,  30] loss: 1.78083 acc: 11.650%\n",
            "[102,  30] loss: 1.77028 acc: 12.150%\n",
            "[103,  30] loss: 1.76381 acc: 11.750%\n",
            "[104,  30] loss: 1.75124 acc: 12.250%\n",
            "[105,  30] loss: 1.73711 acc: 12.400%\n",
            "[106,  30] loss: 1.73354 acc: 12.250%\n",
            "[107,  30] loss: 1.71756 acc: 11.800%\n",
            "[108,  30] loss: 1.70676 acc: 12.800%\n",
            "[109,  30] loss: 1.69984 acc: 12.700%\n",
            "[110,  30] loss: 1.67933 acc: 12.300%\n",
            "[111,  30] loss: 1.67483 acc: 13.200%\n",
            "[112,  30] loss: 1.66591 acc: 13.300%\n",
            "[113,  30] loss: 1.64359 acc: 13.100%\n",
            "[114,  30] loss: 1.63341 acc: 13.050%\n",
            "[115,  30] loss: 1.62441 acc: 12.950%\n",
            "[116,  30] loss: 1.61512 acc: 13.350%\n",
            "[117,  30] loss: 1.59678 acc: 13.450%\n",
            "[118,  30] loss: 1.58911 acc: 13.400%\n",
            "[119,  30] loss: 1.57571 acc: 12.900%\n",
            "[120,  30] loss: 1.56084 acc: 13.350%\n",
            "[121,  30] loss: 1.55328 acc: 13.350%\n",
            "[122,  30] loss: 1.53780 acc: 13.700%\n",
            "[123,  30] loss: 1.52425 acc: 13.500%\n",
            "[124,  30] loss: 1.51815 acc: 13.450%\n",
            "[125,  30] loss: 1.50114 acc: 13.350%\n",
            "[126,  30] loss: 1.49149 acc: 13.700%\n",
            "[127,  30] loss: 1.47858 acc: 13.650%\n",
            "[128,  30] loss: 1.46274 acc: 14.050%\n",
            "[129,  30] loss: 1.45723 acc: 13.900%\n",
            "[130,  30] loss: 1.42960 acc: 13.800%\n",
            "[131,  30] loss: 1.43033 acc: 13.550%\n",
            "[132,  30] loss: 1.40769 acc: 13.900%\n",
            "[133,  30] loss: 1.39831 acc: 14.050%\n",
            "[134,  30] loss: 1.39648 acc: 13.750%\n",
            "[135,  30] loss: 1.38734 acc: 14.000%\n",
            "[136,  30] loss: 1.37612 acc: 13.900%\n",
            "[137,  30] loss: 1.36352 acc: 13.750%\n",
            "[138,  30] loss: 1.34535 acc: 14.250%\n",
            "[139,  30] loss: 1.34055 acc: 13.950%\n",
            "[140,  30] loss: 1.32910 acc: 13.950%\n",
            "[141,  30] loss: 1.31051 acc: 13.800%\n",
            "[142,  30] loss: 1.31664 acc: 14.100%\n",
            "[143,  30] loss: 1.29670 acc: 14.300%\n",
            "[144,  30] loss: 1.28775 acc: 14.050%\n",
            "[145,  30] loss: 1.27871 acc: 14.300%\n",
            "[146,  30] loss: 1.26915 acc: 14.100%\n",
            "[147,  30] loss: 1.26105 acc: 13.500%\n",
            "[148,  30] loss: 1.24945 acc: 14.200%\n",
            "[149,  30] loss: 1.22899 acc: 14.250%\n",
            "[150,  30] loss: 1.22032 acc: 13.900%\n",
            "[151,  30] loss: 1.22379 acc: 14.100%\n",
            "[152,  30] loss: 1.21696 acc: 13.850%\n",
            "[153,  30] loss: 1.19971 acc: 14.500%\n",
            "[154,  30] loss: 1.19677 acc: 14.050%\n",
            "[155,  30] loss: 1.17799 acc: 13.750%\n",
            "[156,  30] loss: 1.17974 acc: 14.100%\n",
            "[157,  30] loss: 1.16864 acc: 14.050%\n",
            "[158,  30] loss: 1.15523 acc: 13.650%\n",
            "[159,  30] loss: 1.15516 acc: 14.050%\n",
            "[160,  30] loss: 1.15493 acc: 14.100%\n",
            "[161,  30] loss: 1.14601 acc: 14.050%\n",
            "[162,  30] loss: 1.13699 acc: 13.400%\n",
            "[163,  30] loss: 1.10780 acc: 14.200%\n",
            "[164,  30] loss: 1.10898 acc: 13.650%\n",
            "[165,  30] loss: 1.10813 acc: 13.550%\n",
            "[166,  30] loss: 1.09804 acc: 13.850%\n",
            "[167,  30] loss: 1.09168 acc: 13.200%\n",
            "[168,  30] loss: 1.07761 acc: 13.900%\n",
            "[169,  30] loss: 1.05371 acc: 14.300%\n",
            "[170,  30] loss: 1.07200 acc: 13.550%\n",
            "[171,  30] loss: 1.06752 acc: 13.350%\n",
            "[172,  30] loss: 1.05153 acc: 14.000%\n",
            "[173,  30] loss: 1.04935 acc: 14.000%\n",
            "[174,  30] loss: 1.04882 acc: 13.000%\n",
            "[175,  30] loss: 1.02178 acc: 13.550%\n",
            "[176,  30] loss: 1.03611 acc: 13.950%\n",
            "[177,  30] loss: 1.01488 acc: 13.500%\n",
            "[178,  30] loss: 1.01494 acc: 13.750%\n",
            "[179,  30] loss: 1.00542 acc: 13.550%\n",
            "[180,  30] loss: 0.99126 acc: 13.850%\n",
            "[181,  30] loss: 0.99098 acc: 13.350%\n",
            "[182,  30] loss: 0.98859 acc: 13.600%\n",
            "[183,  30] loss: 0.97267 acc: 13.450%\n",
            "[184,  30] loss: 0.96900 acc: 13.100%\n",
            "[185,  30] loss: 0.95952 acc: 13.650%\n",
            "[186,  30] loss: 0.96630 acc: 13.400%\n",
            "[187,  30] loss: 0.95264 acc: 12.700%\n",
            "[188,  30] loss: 0.94332 acc: 13.550%\n",
            "[189,  30] loss: 0.96122 acc: 13.250%\n",
            "[190,  30] loss: 0.94118 acc: 13.950%\n",
            "[191,  30] loss: 0.93145 acc: 13.450%\n",
            "[192,  30] loss: 0.93496 acc: 12.550%\n",
            "[193,  30] loss: 0.91574 acc: 13.600%\n",
            "[194,  30] loss: 0.90986 acc: 13.450%\n",
            "[195,  30] loss: 0.90284 acc: 13.150%\n",
            "[196,  30] loss: 0.90997 acc: 12.600%\n",
            "[197,  30] loss: 0.90055 acc: 13.250%\n",
            "[198,  30] loss: 0.90058 acc: 12.600%\n",
            "[199,  30] loss: 0.89013 acc: 13.150%\n",
            "[200,  30] loss: 0.89385 acc: 13.300%\n",
            "[201,  30] loss: 0.89143 acc: 12.900%\n",
            "[202,  30] loss: 0.87300 acc: 13.350%\n",
            "[203,  30] loss: 0.86979 acc: 12.550%\n",
            "[204,  30] loss: 0.88856 acc: 12.450%\n",
            "[205,  30] loss: 0.86733 acc: 12.950%\n",
            "[206,  30] loss: 0.86085 acc: 12.750%\n",
            "[207,  30] loss: 0.86042 acc: 12.800%\n",
            "[208,  30] loss: 0.84888 acc: 13.000%\n",
            "[209,  30] loss: 0.84436 acc: 12.700%\n",
            "[210,  30] loss: 0.84117 acc: 12.900%\n",
            "[211,  30] loss: 0.83762 acc: 13.150%\n",
            "[212,  30] loss: 0.82786 acc: 12.850%\n",
            "[213,  30] loss: 0.82794 acc: 13.750%\n",
            "[214,  30] loss: 0.83481 acc: 12.350%\n",
            "[215,  30] loss: 0.81711 acc: 13.000%\n",
            "[216,  30] loss: 0.81780 acc: 12.350%\n",
            "[217,  30] loss: 0.82042 acc: 12.500%\n",
            "[218,  30] loss: 0.80452 acc: 13.200%\n",
            "[219,  30] loss: 0.79709 acc: 13.050%\n",
            "[220,  30] loss: 0.80580 acc: 13.150%\n",
            "[221,  30] loss: 0.79543 acc: 13.400%\n",
            "[222,  30] loss: 0.79346 acc: 13.000%\n",
            "[223,  30] loss: 0.78610 acc: 13.200%\n",
            "[224,  30] loss: 0.78989 acc: 12.800%\n",
            "[225,  30] loss: 0.78370 acc: 13.400%\n",
            "[226,  30] loss: 0.77342 acc: 12.700%\n",
            "[227,  30] loss: 0.78485 acc: 12.500%\n",
            "[228,  30] loss: 0.78612 acc: 12.500%\n",
            "[229,  30] loss: 0.76996 acc: 12.400%\n",
            "[230,  30] loss: 0.77258 acc: 11.850%\n",
            "[231,  30] loss: 0.76907 acc: 12.500%\n",
            "[232,  30] loss: 0.75870 acc: 12.950%\n",
            "[233,  30] loss: 0.75743 acc: 12.950%\n",
            "[234,  30] loss: 0.75578 acc: 12.250%\n",
            "[235,  30] loss: 0.76122 acc: 12.800%\n",
            "[236,  30] loss: 0.74486 acc: 13.000%\n",
            "[237,  30] loss: 0.75283 acc: 12.700%\n",
            "[238,  30] loss: 0.73847 acc: 12.500%\n",
            "[239,  30] loss: 0.73804 acc: 12.450%\n",
            "[240,  30] loss: 0.73410 acc: 12.950%\n",
            "[241,  30] loss: 0.73274 acc: 12.550%\n",
            "[242,  30] loss: 0.73322 acc: 12.700%\n",
            "[243,  30] loss: 0.72712 acc: 12.650%\n",
            "[244,  30] loss: 0.71838 acc: 13.200%\n",
            "[245,  30] loss: 0.72500 acc: 12.750%\n",
            "[246,  30] loss: 0.72701 acc: 12.600%\n",
            "[247,  30] loss: 0.72150 acc: 12.250%\n",
            "[248,  30] loss: 0.71767 acc: 12.900%\n",
            "[249,  30] loss: 0.70580 acc: 12.750%\n",
            "[250,  30] loss: 0.71075 acc: 13.000%\n",
            "[251,  30] loss: 0.70147 acc: 12.700%\n",
            "[252,  30] loss: 0.70328 acc: 13.950%\n",
            "[253,  30] loss: 0.70005 acc: 12.650%\n",
            "[254,  30] loss: 0.70397 acc: 12.950%\n",
            "[255,  30] loss: 0.69277 acc: 13.450%\n",
            "[256,  30] loss: 0.69354 acc: 12.900%\n",
            "[257,  30] loss: 0.68873 acc: 12.850%\n",
            "[258,  30] loss: 0.68329 acc: 13.650%\n",
            "[259,  30] loss: 0.68457 acc: 12.600%\n",
            "[260,  30] loss: 0.68242 acc: 13.100%\n",
            "[261,  30] loss: 0.67739 acc: 13.300%\n",
            "[262,  30] loss: 0.68146 acc: 12.250%\n",
            "[263,  30] loss: 0.68439 acc: 13.350%\n",
            "[264,  30] loss: 0.67674 acc: 12.650%\n",
            "[265,  30] loss: 0.67212 acc: 12.550%\n",
            "[266,  30] loss: 0.68158 acc: 13.150%\n",
            "[267,  30] loss: 0.68166 acc: 12.850%\n",
            "[268,  30] loss: 0.66847 acc: 12.300%\n",
            "[269,  30] loss: 0.66247 acc: 12.800%\n",
            "[270,  30] loss: 0.65871 acc: 12.950%\n",
            "[271,  30] loss: 0.65967 acc: 14.000%\n",
            "[272,  30] loss: 0.66872 acc: 13.750%\n",
            "[273,  30] loss: 0.64560 acc: 13.450%\n",
            "[274,  30] loss: 0.65661 acc: 13.100%\n",
            "[275,  30] loss: 0.65451 acc: 12.650%\n",
            "[276,  30] loss: 0.64845 acc: 14.050%\n",
            "[277,  30] loss: 0.64923 acc: 13.750%\n",
            "[278,  30] loss: 0.65891 acc: 12.200%\n",
            "[279,  30] loss: 0.64865 acc: 13.000%\n",
            "[280,  30] loss: 0.64462 acc: 13.150%\n",
            "[281,  30] loss: 0.64489 acc: 13.750%\n",
            "[282,  30] loss: 0.63800 acc: 13.050%\n",
            "[283,  30] loss: 0.64081 acc: 13.350%\n",
            "[284,  30] loss: 0.63767 acc: 13.200%\n",
            "[285,  30] loss: 0.64398 acc: 12.750%\n",
            "[286,  30] loss: 0.63902 acc: 13.850%\n",
            "[287,  30] loss: 0.63914 acc: 13.050%\n",
            "[288,  30] loss: 0.64229 acc: 12.900%\n",
            "[289,  30] loss: 0.63095 acc: 13.050%\n",
            "[290,  30] loss: 0.63509 acc: 13.450%\n",
            "[291,  30] loss: 0.62108 acc: 14.150%\n",
            "[292,  30] loss: 0.62430 acc: 13.050%\n",
            "[293,  30] loss: 0.63292 acc: 12.950%\n",
            "[294,  30] loss: 0.63582 acc: 13.450%\n",
            "[295,  30] loss: 0.63072 acc: 13.400%\n",
            "[296,  30] loss: 0.62298 acc: 13.000%\n",
            "[297,  30] loss: 0.61635 acc: 13.800%\n",
            "[298,  30] loss: 0.60873 acc: 14.200%\n",
            "[299,  30] loss: 0.60456 acc: 14.050%\n",
            "[300,  30] loss: 0.60535 acc: 14.350%\n",
            "[301,  30] loss: 0.62097 acc: 13.400%\n",
            "[302,  30] loss: 0.62240 acc: 13.150%\n",
            "[303,  30] loss: 0.61371 acc: 12.450%\n",
            "[304,  30] loss: 0.61808 acc: 13.300%\n",
            "[305,  30] loss: 0.61014 acc: 13.800%\n",
            "[306,  30] loss: 0.60444 acc: 13.150%\n",
            "[307,  30] loss: 0.60916 acc: 13.650%\n",
            "[308,  30] loss: 0.60302 acc: 13.750%\n",
            "[309,  30] loss: 0.60427 acc: 14.250%\n",
            "[310,  30] loss: 0.60027 acc: 13.950%\n",
            "[311,  30] loss: 0.59110 acc: 14.400%\n",
            "[312,  30] loss: 0.59363 acc: 14.150%\n",
            "[313,  30] loss: 0.59109 acc: 14.300%\n",
            "[314,  30] loss: 0.59507 acc: 14.200%\n",
            "[315,  30] loss: 0.59511 acc: 13.700%\n",
            "[316,  30] loss: 0.58938 acc: 13.600%\n",
            "[317,  30] loss: 0.60615 acc: 13.300%\n",
            "[318,  30] loss: 0.59038 acc: 13.850%\n",
            "[319,  30] loss: 0.60112 acc: 13.800%\n",
            "[320,  30] loss: 0.59607 acc: 12.950%\n",
            "[321,  30] loss: 0.58758 acc: 13.700%\n",
            "[322,  30] loss: 0.59255 acc: 13.900%\n",
            "[323,  30] loss: 0.57477 acc: 14.900%\n",
            "[324,  30] loss: 0.59199 acc: 14.100%\n",
            "[325,  30] loss: 0.58581 acc: 13.950%\n",
            "[326,  30] loss: 0.58432 acc: 13.950%\n",
            "[327,  30] loss: 0.57265 acc: 14.650%\n",
            "[328,  30] loss: 0.58479 acc: 14.250%\n",
            "[329,  30] loss: 0.58210 acc: 14.300%\n",
            "[330,  30] loss: 0.58057 acc: 14.400%\n",
            "[331,  30] loss: 0.57875 acc: 13.500%\n",
            "[332,  30] loss: 0.56915 acc: 14.450%\n",
            "[333,  30] loss: 0.57768 acc: 14.000%\n",
            "[334,  30] loss: 0.57248 acc: 14.400%\n",
            "[335,  30] loss: 0.57658 acc: 13.600%\n",
            "[336,  30] loss: 0.56661 acc: 14.900%\n",
            "[337,  30] loss: 0.57837 acc: 13.400%\n",
            "[338,  30] loss: 0.56432 acc: 15.000%\n",
            "[339,  30] loss: 0.56375 acc: 14.000%\n",
            "[340,  30] loss: 0.56944 acc: 14.650%\n",
            "[341,  30] loss: 0.56220 acc: 14.550%\n",
            "[342,  30] loss: 0.56602 acc: 14.450%\n",
            "[343,  30] loss: 0.57136 acc: 14.850%\n",
            "[344,  30] loss: 0.56135 acc: 14.350%\n",
            "[345,  30] loss: 0.55928 acc: 14.700%\n",
            "[346,  30] loss: 0.56475 acc: 14.600%\n",
            "[347,  30] loss: 0.56910 acc: 13.450%\n",
            "[348,  30] loss: 0.56176 acc: 14.300%\n",
            "[349,  30] loss: 0.56189 acc: 13.600%\n",
            "[350,  30] loss: 0.55766 acc: 14.750%\n",
            "[351,  30] loss: 0.55533 acc: 14.400%\n",
            "[352,  30] loss: 0.55835 acc: 15.000%\n",
            "[353,  30] loss: 0.55481 acc: 15.100%\n",
            "[354,  30] loss: 0.55507 acc: 14.050%\n",
            "[355,  30] loss: 0.54970 acc: 14.450%\n",
            "[356,  30] loss: 0.55431 acc: 14.650%\n",
            "[357,  30] loss: 0.55833 acc: 14.200%\n",
            "[358,  30] loss: 0.55052 acc: 15.500%\n",
            "[359,  30] loss: 0.56134 acc: 14.500%\n",
            "[360,  30] loss: 0.54532 acc: 15.550%\n",
            "[361,  30] loss: 0.54909 acc: 14.950%\n",
            "[362,  30] loss: 0.54603 acc: 14.450%\n",
            "[363,  30] loss: 0.53848 acc: 15.550%\n",
            "[364,  30] loss: 0.55233 acc: 15.000%\n",
            "[365,  30] loss: 0.53986 acc: 15.150%\n",
            "[366,  30] loss: 0.54137 acc: 15.250%\n",
            "[367,  30] loss: 0.53809 acc: 15.200%\n",
            "[368,  30] loss: 0.54631 acc: 14.600%\n",
            "[369,  30] loss: 0.54606 acc: 14.850%\n",
            "[370,  30] loss: 0.54681 acc: 14.450%\n",
            "[371,  30] loss: 0.53404 acc: 15.300%\n",
            "[372,  30] loss: 0.54368 acc: 14.900%\n",
            "[373,  30] loss: 0.55235 acc: 14.250%\n",
            "[374,  30] loss: 0.54031 acc: 14.400%\n",
            "[375,  30] loss: 0.54066 acc: 15.400%\n",
            "[376,  30] loss: 0.53712 acc: 15.150%\n",
            "[377,  30] loss: 0.52852 acc: 15.200%\n",
            "[378,  30] loss: 0.54330 acc: 14.950%\n",
            "[379,  30] loss: 0.53553 acc: 15.200%\n",
            "[380,  30] loss: 0.53272 acc: 15.450%\n",
            "[381,  30] loss: 0.54207 acc: 14.250%\n",
            "[382,  30] loss: 0.52502 acc: 15.200%\n",
            "[383,  30] loss: 0.53556 acc: 15.000%\n",
            "[384,  30] loss: 0.52658 acc: 15.450%\n",
            "[385,  30] loss: 0.53270 acc: 15.700%\n",
            "[386,  30] loss: 0.53111 acc: 14.900%\n",
            "[387,  30] loss: 0.52603 acc: 16.350%\n",
            "[388,  30] loss: 0.52949 acc: 15.750%\n",
            "[389,  30] loss: 0.52747 acc: 15.250%\n",
            "[390,  30] loss: 0.53180 acc: 15.300%\n",
            "[391,  30] loss: 0.53668 acc: 15.100%\n",
            "[392,  30] loss: 0.52168 acc: 16.000%\n",
            "[393,  30] loss: 0.51600 acc: 16.100%\n",
            "[394,  30] loss: 0.52119 acc: 15.050%\n",
            "[395,  30] loss: 0.52218 acc: 15.550%\n",
            "[396,  30] loss: 0.52623 acc: 15.150%\n",
            "[397,  30] loss: 0.52344 acc: 15.550%\n",
            "[398,  30] loss: 0.52139 acc: 15.800%\n",
            "[399,  30] loss: 0.52021 acc: 15.750%\n",
            "[400,  30] loss: 0.51409 acc: 16.450%\n",
            "[401,  30] loss: 0.51752 acc: 15.200%\n",
            "[402,  30] loss: 0.51510 acc: 16.350%\n",
            "[403,  30] loss: 0.52492 acc: 15.500%\n",
            "[404,  30] loss: 0.51579 acc: 16.250%\n",
            "[405,  30] loss: 0.51760 acc: 15.100%\n",
            "[406,  30] loss: 0.50282 acc: 15.950%\n",
            "[407,  30] loss: 0.50827 acc: 16.600%\n",
            "[408,  30] loss: 0.50804 acc: 15.500%\n",
            "[409,  30] loss: 0.50922 acc: 16.150%\n",
            "[410,  30] loss: 0.51539 acc: 15.750%\n",
            "[411,  30] loss: 0.50374 acc: 16.000%\n",
            "[412,  30] loss: 0.51528 acc: 15.550%\n",
            "[413,  30] loss: 0.50401 acc: 16.750%\n",
            "[414,  30] loss: 0.50946 acc: 15.600%\n",
            "[415,  30] loss: 0.51024 acc: 16.100%\n",
            "[416,  30] loss: 0.50374 acc: 16.450%\n",
            "[417,  30] loss: 0.50970 acc: 16.300%\n",
            "[418,  30] loss: 0.50769 acc: 16.000%\n",
            "[419,  30] loss: 0.50791 acc: 15.500%\n",
            "[420,  30] loss: 0.49767 acc: 16.150%\n",
            "[421,  30] loss: 0.51580 acc: 15.150%\n",
            "[422,  30] loss: 0.50912 acc: 15.600%\n",
            "[423,  30] loss: 0.50004 acc: 16.400%\n",
            "[424,  30] loss: 0.51087 acc: 16.200%\n",
            "[425,  30] loss: 0.51599 acc: 15.550%\n",
            "[426,  30] loss: 0.49296 acc: 16.700%\n",
            "[427,  30] loss: 0.49747 acc: 16.400%\n",
            "[428,  30] loss: 0.49591 acc: 15.700%\n",
            "[429,  30] loss: 0.49979 acc: 16.150%\n",
            "[430,  30] loss: 0.50845 acc: 15.950%\n",
            "[431,  30] loss: 0.49505 acc: 16.700%\n",
            "[432,  30] loss: 0.49822 acc: 16.300%\n",
            "[433,  30] loss: 0.49852 acc: 16.550%\n",
            "[434,  30] loss: 0.49494 acc: 17.200%\n",
            "[435,  30] loss: 0.48381 acc: 16.950%\n",
            "[436,  30] loss: 0.49208 acc: 16.550%\n",
            "[437,  30] loss: 0.50179 acc: 16.550%\n",
            "[438,  30] loss: 0.48982 acc: 16.700%\n",
            "[439,  30] loss: 0.49412 acc: 16.050%\n",
            "[440,  30] loss: 0.49257 acc: 16.700%\n",
            "[441,  30] loss: 0.49217 acc: 17.100%\n",
            "[442,  30] loss: 0.49764 acc: 16.600%\n",
            "[443,  30] loss: 0.48708 acc: 17.200%\n",
            "[444,  30] loss: 0.50041 acc: 16.100%\n",
            "[445,  30] loss: 0.49057 acc: 16.700%\n",
            "[446,  30] loss: 0.49999 acc: 16.950%\n",
            "[447,  30] loss: 0.49770 acc: 16.850%\n",
            "[448,  30] loss: 0.48845 acc: 16.100%\n",
            "[449,  30] loss: 0.48206 acc: 16.350%\n",
            "[450,  30] loss: 0.48364 acc: 16.800%\n",
            "[451,  30] loss: 0.48686 acc: 17.300%\n",
            "[452,  30] loss: 0.48303 acc: 17.200%\n",
            "[453,  30] loss: 0.48568 acc: 16.900%\n",
            "[454,  30] loss: 0.49178 acc: 16.600%\n",
            "[455,  30] loss: 0.47982 acc: 17.250%\n",
            "[456,  30] loss: 0.47637 acc: 17.600%\n",
            "[457,  30] loss: 0.48033 acc: 17.200%\n",
            "[458,  30] loss: 0.48908 acc: 16.150%\n",
            "[459,  30] loss: 0.48434 acc: 17.100%\n",
            "[460,  30] loss: 0.48533 acc: 16.700%\n",
            "[461,  30] loss: 0.47449 acc: 17.350%\n",
            "[462,  30] loss: 0.47869 acc: 17.350%\n",
            "[463,  30] loss: 0.47157 acc: 17.650%\n",
            "[464,  30] loss: 0.47782 acc: 17.450%\n",
            "[465,  30] loss: 0.47837 acc: 17.250%\n",
            "[466,  30] loss: 0.47392 acc: 16.850%\n",
            "[467,  30] loss: 0.47911 acc: 16.700%\n",
            "[468,  30] loss: 0.47646 acc: 17.800%\n",
            "[469,  30] loss: 0.47464 acc: 17.350%\n",
            "[470,  30] loss: 0.47182 acc: 17.100%\n",
            "[471,  30] loss: 0.47384 acc: 17.500%\n",
            "[472,  30] loss: 0.47239 acc: 17.400%\n",
            "[473,  30] loss: 0.47465 acc: 17.400%\n",
            "[474,  30] loss: 0.46734 acc: 17.600%\n",
            "[475,  30] loss: 0.47116 acc: 18.100%\n",
            "[476,  30] loss: 0.46486 acc: 18.300%\n",
            "[477,  30] loss: 0.46666 acc: 17.600%\n",
            "[478,  30] loss: 0.47891 acc: 16.950%\n",
            "[479,  30] loss: 0.47250 acc: 17.050%\n",
            "[480,  30] loss: 0.46337 acc: 18.300%\n",
            "[481,  30] loss: 0.46707 acc: 17.650%\n",
            "[482,  30] loss: 0.47273 acc: 18.050%\n",
            "[483,  30] loss: 0.46482 acc: 18.550%\n",
            "[484,  30] loss: 0.47627 acc: 17.300%\n",
            "[485,  30] loss: 0.46605 acc: 18.150%\n",
            "[486,  30] loss: 0.46933 acc: 17.750%\n",
            "[487,  30] loss: 0.47352 acc: 17.450%\n",
            "[488,  30] loss: 0.46714 acc: 18.150%\n",
            "[489,  30] loss: 0.46875 acc: 17.600%\n",
            "[490,  30] loss: 0.45340 acc: 17.950%\n",
            "[491,  30] loss: 0.46148 acc: 17.200%\n",
            "[492,  30] loss: 0.46347 acc: 18.000%\n",
            "[493,  30] loss: 0.46737 acc: 17.150%\n",
            "[494,  30] loss: 0.46680 acc: 17.550%\n",
            "[495,  30] loss: 0.46067 acc: 18.550%\n",
            "[496,  30] loss: 0.46301 acc: 17.800%\n",
            "[497,  30] loss: 0.45574 acc: 18.350%\n",
            "[498,  30] loss: 0.46165 acc: 18.350%\n",
            "[499,  30] loss: 0.46296 acc: 18.150%\n",
            "[500,  30] loss: 0.46100 acc: 17.750%\n",
            "[501,  30] loss: 0.46150 acc: 17.700%\n",
            "[502,  30] loss: 0.45399 acc: 18.250%\n",
            "[503,  30] loss: 0.45340 acc: 18.600%\n",
            "[504,  30] loss: 0.45584 acc: 18.050%\n",
            "[505,  30] loss: 0.45108 acc: 18.750%\n",
            "[506,  30] loss: 0.45264 acc: 19.300%\n",
            "[507,  30] loss: 0.46138 acc: 17.350%\n",
            "[508,  30] loss: 0.45429 acc: 18.100%\n",
            "[509,  30] loss: 0.45127 acc: 19.050%\n",
            "[510,  30] loss: 0.45699 acc: 17.950%\n",
            "[511,  30] loss: 0.45076 acc: 18.650%\n",
            "[512,  30] loss: 0.46074 acc: 18.350%\n",
            "[513,  30] loss: 0.45707 acc: 17.650%\n",
            "[514,  30] loss: 0.45095 acc: 18.550%\n",
            "[515,  30] loss: 0.45066 acc: 18.450%\n",
            "[516,  30] loss: 0.45097 acc: 18.450%\n",
            "[517,  30] loss: 0.44956 acc: 18.150%\n",
            "[518,  30] loss: 0.45324 acc: 17.800%\n",
            "[519,  30] loss: 0.44547 acc: 18.350%\n",
            "[520,  30] loss: 0.44452 acc: 18.950%\n",
            "[521,  30] loss: 0.44582 acc: 18.500%\n",
            "[522,  30] loss: 0.44855 acc: 18.500%\n",
            "[523,  30] loss: 0.44923 acc: 18.500%\n",
            "[524,  30] loss: 0.44554 acc: 18.600%\n",
            "[525,  30] loss: 0.45157 acc: 18.650%\n",
            "[526,  30] loss: 0.44931 acc: 17.750%\n",
            "[527,  30] loss: 0.44018 acc: 19.000%\n",
            "[528,  30] loss: 0.44342 acc: 19.100%\n",
            "[529,  30] loss: 0.45560 acc: 18.300%\n",
            "[530,  30] loss: 0.44607 acc: 18.900%\n",
            "[531,  30] loss: 0.43693 acc: 18.700%\n",
            "[532,  30] loss: 0.44082 acc: 19.200%\n",
            "[533,  30] loss: 0.44367 acc: 18.650%\n",
            "[534,  30] loss: 0.43762 acc: 18.700%\n",
            "[535,  30] loss: 0.43930 acc: 19.000%\n",
            "[536,  30] loss: 0.44473 acc: 18.700%\n",
            "[537,  30] loss: 0.44796 acc: 18.850%\n",
            "[538,  30] loss: 0.43564 acc: 19.100%\n",
            "[539,  30] loss: 0.44061 acc: 18.900%\n",
            "[540,  30] loss: 0.43644 acc: 19.400%\n",
            "[541,  30] loss: 0.43778 acc: 19.100%\n",
            "[542,  30] loss: 0.43699 acc: 18.950%\n",
            "[543,  30] loss: 0.44352 acc: 19.100%\n",
            "[544,  30] loss: 0.43702 acc: 18.950%\n",
            "[545,  30] loss: 0.43918 acc: 18.550%\n",
            "[546,  30] loss: 0.43496 acc: 18.900%\n",
            "[547,  30] loss: 0.42784 acc: 19.550%\n",
            "[548,  30] loss: 0.43367 acc: 19.150%\n",
            "[549,  30] loss: 0.44679 acc: 18.550%\n",
            "[550,  30] loss: 0.42697 acc: 18.850%\n",
            "[551,  30] loss: 0.43094 acc: 19.250%\n",
            "[552,  30] loss: 0.43224 acc: 19.250%\n",
            "[553,  30] loss: 0.43906 acc: 19.250%\n",
            "[554,  30] loss: 0.43566 acc: 18.550%\n",
            "[555,  30] loss: 0.42127 acc: 20.250%\n",
            "[556,  30] loss: 0.42839 acc: 19.300%\n",
            "[557,  30] loss: 0.43514 acc: 18.300%\n",
            "[558,  30] loss: 0.43449 acc: 18.950%\n",
            "[559,  30] loss: 0.43036 acc: 19.700%\n",
            "[560,  30] loss: 0.42920 acc: 20.100%\n",
            "[561,  30] loss: 0.43356 acc: 19.050%\n",
            "[562,  30] loss: 0.42899 acc: 19.250%\n",
            "[563,  30] loss: 0.42406 acc: 20.100%\n",
            "[564,  30] loss: 0.42949 acc: 19.400%\n",
            "[565,  30] loss: 0.42191 acc: 19.250%\n",
            "[566,  30] loss: 0.42658 acc: 18.800%\n",
            "[567,  30] loss: 0.42676 acc: 18.900%\n",
            "[568,  30] loss: 0.41612 acc: 20.150%\n",
            "[569,  30] loss: 0.42469 acc: 19.950%\n",
            "[570,  30] loss: 0.42569 acc: 19.550%\n",
            "[571,  30] loss: 0.42871 acc: 19.400%\n",
            "[572,  30] loss: 0.41995 acc: 19.300%\n",
            "[573,  30] loss: 0.41955 acc: 19.750%\n",
            "[574,  30] loss: 0.42626 acc: 19.250%\n",
            "[575,  30] loss: 0.42053 acc: 19.450%\n",
            "[576,  30] loss: 0.41581 acc: 19.850%\n",
            "[577,  30] loss: 0.42291 acc: 19.650%\n",
            "[578,  30] loss: 0.42370 acc: 20.550%\n",
            "[579,  30] loss: 0.42344 acc: 19.750%\n",
            "[580,  30] loss: 0.42218 acc: 19.950%\n",
            "[581,  30] loss: 0.41773 acc: 20.650%\n",
            "[582,  30] loss: 0.42039 acc: 19.950%\n",
            "[583,  30] loss: 0.41953 acc: 19.750%\n",
            "[584,  30] loss: 0.42534 acc: 18.900%\n",
            "[585,  30] loss: 0.40786 acc: 20.650%\n",
            "[586,  30] loss: 0.41705 acc: 20.350%\n",
            "[587,  30] loss: 0.41926 acc: 20.400%\n",
            "[588,  30] loss: 0.41580 acc: 20.100%\n",
            "[589,  30] loss: 0.41154 acc: 20.250%\n",
            "[590,  30] loss: 0.40499 acc: 20.750%\n",
            "[591,  30] loss: 0.41396 acc: 20.750%\n",
            "[592,  30] loss: 0.40484 acc: 20.550%\n",
            "[593,  30] loss: 0.40951 acc: 20.150%\n",
            "[594,  30] loss: 0.41052 acc: 20.300%\n",
            "[595,  30] loss: 0.40941 acc: 20.650%\n",
            "[596,  30] loss: 0.41303 acc: 20.000%\n",
            "[597,  30] loss: 0.42091 acc: 19.650%\n",
            "[598,  30] loss: 0.41244 acc: 20.300%\n",
            "[599,  30] loss: 0.41304 acc: 19.700%\n",
            "[600,  30] loss: 0.41053 acc: 20.600%\n",
            "[601,  30] loss: 0.40707 acc: 20.700%\n",
            "[602,  30] loss: 0.40733 acc: 19.950%\n",
            "[603,  30] loss: 0.40666 acc: 20.800%\n",
            "[604,  30] loss: 0.40600 acc: 20.100%\n",
            "[605,  30] loss: 0.40434 acc: 20.750%\n",
            "[606,  30] loss: 0.40139 acc: 20.950%\n",
            "[607,  30] loss: 0.40149 acc: 20.700%\n",
            "[608,  30] loss: 0.40466 acc: 21.300%\n",
            "[609,  30] loss: 0.39887 acc: 20.800%\n",
            "[610,  30] loss: 0.39797 acc: 20.650%\n",
            "[611,  30] loss: 0.40718 acc: 21.000%\n",
            "[612,  30] loss: 0.40691 acc: 20.600%\n",
            "[613,  30] loss: 0.40692 acc: 20.000%\n",
            "[614,  30] loss: 0.41391 acc: 19.750%\n",
            "[615,  30] loss: 0.40669 acc: 20.050%\n",
            "[616,  30] loss: 0.40652 acc: 20.000%\n",
            "[617,  30] loss: 0.40340 acc: 20.150%\n",
            "[618,  30] loss: 0.40401 acc: 19.900%\n",
            "[619,  30] loss: 0.40755 acc: 20.950%\n",
            "[620,  30] loss: 0.40392 acc: 20.450%\n",
            "[621,  30] loss: 0.40333 acc: 20.900%\n",
            "[622,  30] loss: 0.39034 acc: 21.400%\n",
            "[623,  30] loss: 0.39747 acc: 21.250%\n",
            "[624,  30] loss: 0.39436 acc: 21.600%\n",
            "[625,  30] loss: 0.39735 acc: 20.700%\n",
            "[626,  30] loss: 0.39195 acc: 20.900%\n",
            "[627,  30] loss: 0.39551 acc: 20.350%\n",
            "[628,  30] loss: 0.39738 acc: 21.400%\n",
            "[629,  30] loss: 0.39435 acc: 21.450%\n",
            "[630,  30] loss: 0.38877 acc: 21.650%\n",
            "[631,  30] loss: 0.39813 acc: 20.850%\n",
            "[632,  30] loss: 0.39440 acc: 20.300%\n",
            "[633,  30] loss: 0.38763 acc: 21.500%\n",
            "[634,  30] loss: 0.39584 acc: 20.850%\n",
            "[635,  30] loss: 0.39334 acc: 21.000%\n",
            "[636,  30] loss: 0.38866 acc: 21.400%\n",
            "[637,  30] loss: 0.39446 acc: 20.900%\n",
            "[638,  30] loss: 0.40089 acc: 20.350%\n",
            "[639,  30] loss: 0.38564 acc: 21.400%\n",
            "[640,  30] loss: 0.38519 acc: 22.000%\n",
            "[641,  30] loss: 0.39391 acc: 20.650%\n",
            "[642,  30] loss: 0.38399 acc: 21.700%\n",
            "[643,  30] loss: 0.39891 acc: 20.600%\n",
            "[644,  30] loss: 0.38184 acc: 21.650%\n",
            "[645,  30] loss: 0.38546 acc: 21.800%\n",
            "[646,  30] loss: 0.38884 acc: 21.950%\n",
            "[647,  30] loss: 0.38845 acc: 21.200%\n",
            "[648,  30] loss: 0.38768 acc: 21.350%\n",
            "[649,  30] loss: 0.39067 acc: 21.350%\n",
            "[650,  30] loss: 0.38776 acc: 21.700%\n",
            "[651,  30] loss: 0.38032 acc: 21.500%\n",
            "[652,  30] loss: 0.37794 acc: 22.350%\n",
            "[653,  30] loss: 0.38622 acc: 21.400%\n",
            "[654,  30] loss: 0.38218 acc: 22.150%\n",
            "[655,  30] loss: 0.38234 acc: 22.000%\n",
            "[656,  30] loss: 0.39599 acc: 20.850%\n",
            "[657,  30] loss: 0.37951 acc: 21.900%\n",
            "[658,  30] loss: 0.38257 acc: 21.450%\n",
            "[659,  30] loss: 0.37883 acc: 21.550%\n",
            "[660,  30] loss: 0.38156 acc: 21.600%\n",
            "[661,  30] loss: 0.38064 acc: 21.650%\n",
            "[662,  30] loss: 0.37954 acc: 21.400%\n",
            "[663,  30] loss: 0.37670 acc: 22.200%\n",
            "[664,  30] loss: 0.37364 acc: 22.250%\n",
            "[665,  30] loss: 0.38000 acc: 21.500%\n",
            "[666,  30] loss: 0.37724 acc: 21.750%\n",
            "[667,  30] loss: 0.37794 acc: 21.250%\n",
            "[668,  30] loss: 0.38237 acc: 21.600%\n",
            "[669,  30] loss: 0.37857 acc: 21.100%\n",
            "[670,  30] loss: 0.37117 acc: 22.450%\n",
            "[671,  30] loss: 0.37728 acc: 22.350%\n",
            "[672,  30] loss: 0.37192 acc: 22.000%\n",
            "[673,  30] loss: 0.37571 acc: 21.850%\n",
            "[674,  30] loss: 0.37469 acc: 21.950%\n",
            "[675,  30] loss: 0.36773 acc: 22.200%\n",
            "[676,  30] loss: 0.36759 acc: 22.100%\n",
            "[677,  30] loss: 0.37070 acc: 22.400%\n",
            "[678,  30] loss: 0.37287 acc: 22.000%\n",
            "[679,  30] loss: 0.36721 acc: 22.200%\n",
            "[680,  30] loss: 0.37033 acc: 22.200%\n",
            "[681,  30] loss: 0.37357 acc: 22.450%\n",
            "[682,  30] loss: 0.36663 acc: 22.700%\n",
            "[683,  30] loss: 0.37031 acc: 22.300%\n",
            "[684,  30] loss: 0.37053 acc: 22.100%\n",
            "[685,  30] loss: 0.37003 acc: 22.050%\n",
            "[686,  30] loss: 0.37019 acc: 22.850%\n",
            "[687,  30] loss: 0.35693 acc: 23.150%\n",
            "[688,  30] loss: 0.36709 acc: 22.100%\n",
            "[689,  30] loss: 0.36475 acc: 22.850%\n",
            "[690,  30] loss: 0.36352 acc: 22.400%\n",
            "[691,  30] loss: 0.36670 acc: 21.650%\n",
            "[692,  30] loss: 0.37136 acc: 21.900%\n",
            "[693,  30] loss: 0.36657 acc: 21.950%\n",
            "[694,  30] loss: 0.36036 acc: 22.500%\n",
            "[695,  30] loss: 0.36093 acc: 22.350%\n",
            "[696,  30] loss: 0.36121 acc: 22.400%\n",
            "[697,  30] loss: 0.35363 acc: 23.050%\n",
            "[698,  30] loss: 0.35836 acc: 22.900%\n",
            "[699,  30] loss: 0.35745 acc: 23.100%\n",
            "[700,  30] loss: 0.36507 acc: 22.750%\n",
            "[701,  30] loss: 0.36057 acc: 23.000%\n",
            "[702,  30] loss: 0.36464 acc: 22.500%\n",
            "[703,  30] loss: 0.36158 acc: 22.600%\n",
            "[704,  30] loss: 0.35718 acc: 22.600%\n",
            "[705,  30] loss: 0.35730 acc: 22.750%\n",
            "[706,  30] loss: 0.36606 acc: 21.500%\n",
            "[707,  30] loss: 0.36206 acc: 22.950%\n",
            "[708,  30] loss: 0.36026 acc: 22.000%\n",
            "[709,  30] loss: 0.35580 acc: 22.850%\n",
            "[710,  30] loss: 0.35809 acc: 22.550%\n",
            "[711,  30] loss: 0.36459 acc: 22.550%\n",
            "[712,  30] loss: 0.35086 acc: 22.850%\n",
            "[713,  30] loss: 0.35773 acc: 22.350%\n",
            "[714,  30] loss: 0.35320 acc: 22.750%\n",
            "[715,  30] loss: 0.35517 acc: 23.100%\n",
            "[716,  30] loss: 0.35288 acc: 22.950%\n",
            "[717,  30] loss: 0.34701 acc: 23.250%\n",
            "[718,  30] loss: 0.35467 acc: 22.950%\n",
            "[719,  30] loss: 0.34900 acc: 23.000%\n",
            "[720,  30] loss: 0.34993 acc: 23.550%\n",
            "[721,  30] loss: 0.35317 acc: 22.300%\n",
            "[722,  30] loss: 0.34307 acc: 24.100%\n",
            "[723,  30] loss: 0.34797 acc: 23.450%\n",
            "[724,  30] loss: 0.34759 acc: 22.700%\n",
            "[725,  30] loss: 0.35071 acc: 22.600%\n",
            "[726,  30] loss: 0.34006 acc: 23.700%\n",
            "[727,  30] loss: 0.34003 acc: 23.850%\n",
            "[728,  30] loss: 0.35093 acc: 22.550%\n",
            "[729,  30] loss: 0.35227 acc: 22.700%\n",
            "[730,  30] loss: 0.34748 acc: 23.100%\n",
            "[731,  30] loss: 0.34474 acc: 23.100%\n",
            "[732,  30] loss: 0.34685 acc: 23.000%\n",
            "[733,  30] loss: 0.34116 acc: 23.150%\n",
            "[734,  30] loss: 0.34689 acc: 23.450%\n",
            "[735,  30] loss: 0.34658 acc: 23.050%\n",
            "[736,  30] loss: 0.33462 acc: 23.750%\n",
            "[737,  30] loss: 0.33959 acc: 23.250%\n",
            "[738,  30] loss: 0.33433 acc: 23.800%\n",
            "[739,  30] loss: 0.33644 acc: 23.850%\n",
            "[740,  30] loss: 0.34346 acc: 22.850%\n",
            "[741,  30] loss: 0.34487 acc: 23.500%\n",
            "[742,  30] loss: 0.33926 acc: 23.350%\n",
            "[743,  30] loss: 0.34056 acc: 23.750%\n",
            "[744,  30] loss: 0.33969 acc: 23.800%\n",
            "[745,  30] loss: 0.33998 acc: 23.600%\n",
            "[746,  30] loss: 0.33607 acc: 23.800%\n",
            "[747,  30] loss: 0.33483 acc: 22.800%\n",
            "[748,  30] loss: 0.34054 acc: 24.100%\n",
            "[749,  30] loss: 0.33919 acc: 23.700%\n",
            "[750,  30] loss: 0.33451 acc: 24.100%\n",
            "[751,  30] loss: 0.34194 acc: 23.700%\n",
            "[752,  30] loss: 0.32221 acc: 24.250%\n",
            "[753,  30] loss: 0.33462 acc: 23.750%\n",
            "[754,  30] loss: 0.32557 acc: 24.200%\n",
            "[755,  30] loss: 0.34177 acc: 22.950%\n",
            "[756,  30] loss: 0.32847 acc: 24.000%\n",
            "[757,  30] loss: 0.33291 acc: 23.700%\n",
            "[758,  30] loss: 0.33201 acc: 23.950%\n",
            "[759,  30] loss: 0.33202 acc: 24.050%\n",
            "[760,  30] loss: 0.33265 acc: 23.800%\n",
            "[761,  30] loss: 0.32526 acc: 24.100%\n",
            "[762,  30] loss: 0.33770 acc: 23.150%\n",
            "[763,  30] loss: 0.33070 acc: 24.200%\n",
            "[764,  30] loss: 0.33313 acc: 23.800%\n",
            "[765,  30] loss: 0.33287 acc: 24.050%\n",
            "[766,  30] loss: 0.32312 acc: 24.350%\n",
            "[767,  30] loss: 0.33101 acc: 23.600%\n",
            "[768,  30] loss: 0.33198 acc: 23.750%\n",
            "[769,  30] loss: 0.32249 acc: 24.300%\n",
            "[770,  30] loss: 0.33058 acc: 24.250%\n",
            "[771,  30] loss: 0.32507 acc: 23.550%\n",
            "[772,  30] loss: 0.32599 acc: 24.350%\n",
            "[773,  30] loss: 0.33205 acc: 23.950%\n",
            "[774,  30] loss: 0.32225 acc: 23.700%\n",
            "[775,  30] loss: 0.31850 acc: 24.350%\n",
            "[776,  30] loss: 0.32629 acc: 23.700%\n",
            "[777,  30] loss: 0.32676 acc: 23.700%\n",
            "[778,  30] loss: 0.31619 acc: 25.000%\n",
            "[779,  30] loss: 0.32263 acc: 23.850%\n",
            "[780,  30] loss: 0.32162 acc: 24.450%\n",
            "[781,  30] loss: 0.31074 acc: 25.150%\n",
            "[782,  30] loss: 0.31527 acc: 24.350%\n",
            "[783,  30] loss: 0.32193 acc: 23.950%\n",
            "[784,  30] loss: 0.32485 acc: 23.700%\n",
            "[785,  30] loss: 0.31562 acc: 24.700%\n",
            "[786,  30] loss: 0.31824 acc: 24.300%\n",
            "[787,  30] loss: 0.31753 acc: 25.200%\n",
            "[788,  30] loss: 0.31526 acc: 24.250%\n",
            "[789,  30] loss: 0.32396 acc: 24.150%\n",
            "[790,  30] loss: 0.31438 acc: 24.100%\n",
            "[791,  30] loss: 0.31666 acc: 24.600%\n",
            "[792,  30] loss: 0.30922 acc: 24.500%\n",
            "[793,  30] loss: 0.31687 acc: 24.700%\n",
            "[794,  30] loss: 0.31971 acc: 23.950%\n",
            "[795,  30] loss: 0.31198 acc: 24.250%\n",
            "[796,  30] loss: 0.30418 acc: 24.900%\n",
            "[797,  30] loss: 0.30650 acc: 25.050%\n",
            "[798,  30] loss: 0.30917 acc: 25.100%\n",
            "[799,  30] loss: 0.30634 acc: 24.600%\n",
            "[800,  30] loss: 0.31376 acc: 23.750%\n",
            "[801,  30] loss: 0.30761 acc: 25.050%\n",
            "[802,  30] loss: 0.30422 acc: 25.100%\n",
            "[803,  30] loss: 0.31298 acc: 24.500%\n",
            "[804,  30] loss: 0.31472 acc: 24.600%\n",
            "[805,  30] loss: 0.30987 acc: 24.600%\n",
            "[806,  30] loss: 0.31353 acc: 24.550%\n",
            "[807,  30] loss: 0.30966 acc: 24.500%\n",
            "[808,  30] loss: 0.31163 acc: 24.800%\n",
            "[809,  30] loss: 0.30806 acc: 24.650%\n",
            "[810,  30] loss: 0.30830 acc: 24.900%\n",
            "[811,  30] loss: 0.29997 acc: 25.250%\n",
            "[812,  30] loss: 0.29914 acc: 25.150%\n",
            "[813,  30] loss: 0.30063 acc: 24.900%\n",
            "[814,  30] loss: 0.30168 acc: 24.750%\n",
            "[815,  30] loss: 0.30051 acc: 25.250%\n",
            "[816,  30] loss: 0.30541 acc: 25.250%\n",
            "[817,  30] loss: 0.30576 acc: 24.750%\n",
            "[818,  30] loss: 0.30210 acc: 25.150%\n",
            "[819,  30] loss: 0.29979 acc: 24.750%\n",
            "[820,  30] loss: 0.30865 acc: 24.650%\n",
            "[821,  30] loss: 0.30049 acc: 25.200%\n",
            "[822,  30] loss: 0.29725 acc: 25.550%\n",
            "[823,  30] loss: 0.30478 acc: 24.900%\n",
            "[824,  30] loss: 0.29624 acc: 24.650%\n",
            "[825,  30] loss: 0.29547 acc: 25.350%\n",
            "[826,  30] loss: 0.30172 acc: 25.000%\n",
            "[827,  30] loss: 0.29588 acc: 25.100%\n",
            "[828,  30] loss: 0.30430 acc: 25.000%\n",
            "[829,  30] loss: 0.30374 acc: 25.100%\n",
            "[830,  30] loss: 0.29531 acc: 25.450%\n",
            "[831,  30] loss: 0.30057 acc: 25.050%\n",
            "[832,  30] loss: 0.29397 acc: 25.400%\n",
            "[833,  30] loss: 0.30070 acc: 25.500%\n",
            "[834,  30] loss: 0.29129 acc: 25.500%\n",
            "[835,  30] loss: 0.29436 acc: 25.500%\n",
            "[836,  30] loss: 0.29112 acc: 25.550%\n",
            "[837,  30] loss: 0.30207 acc: 24.850%\n",
            "[838,  30] loss: 0.29283 acc: 25.500%\n",
            "[839,  30] loss: 0.28861 acc: 25.600%\n",
            "[840,  30] loss: 0.28282 acc: 25.950%\n",
            "[841,  30] loss: 0.28951 acc: 25.600%\n",
            "[842,  30] loss: 0.30099 acc: 24.650%\n",
            "[843,  30] loss: 0.29628 acc: 25.400%\n",
            "[844,  30] loss: 0.29309 acc: 25.200%\n",
            "[845,  30] loss: 0.29838 acc: 25.000%\n",
            "[846,  30] loss: 0.29328 acc: 25.550%\n",
            "[847,  30] loss: 0.28597 acc: 25.750%\n",
            "[848,  30] loss: 0.28590 acc: 24.850%\n",
            "[849,  30] loss: 0.28935 acc: 25.250%\n",
            "[850,  30] loss: 0.29161 acc: 25.450%\n",
            "[851,  30] loss: 0.27786 acc: 25.900%\n",
            "[852,  30] loss: 0.28771 acc: 25.500%\n",
            "[853,  30] loss: 0.28960 acc: 25.900%\n",
            "[854,  30] loss: 0.28487 acc: 25.300%\n",
            "[855,  30] loss: 0.28989 acc: 25.450%\n",
            "[856,  30] loss: 0.28555 acc: 25.700%\n",
            "[857,  30] loss: 0.28108 acc: 25.950%\n",
            "[858,  30] loss: 0.29043 acc: 25.550%\n",
            "[859,  30] loss: 0.28716 acc: 25.550%\n",
            "[860,  30] loss: 0.28363 acc: 25.500%\n",
            "[861,  30] loss: 0.28381 acc: 25.350%\n",
            "[862,  30] loss: 0.28271 acc: 26.150%\n",
            "[863,  30] loss: 0.28373 acc: 25.150%\n",
            "[864,  30] loss: 0.28358 acc: 25.300%\n",
            "[865,  30] loss: 0.28205 acc: 25.300%\n",
            "[866,  30] loss: 0.27474 acc: 26.050%\n",
            "[867,  30] loss: 0.27874 acc: 26.050%\n",
            "[868,  30] loss: 0.27289 acc: 26.700%\n",
            "[869,  30] loss: 0.28192 acc: 25.000%\n",
            "[870,  30] loss: 0.27305 acc: 26.500%\n",
            "[871,  30] loss: 0.27427 acc: 25.150%\n",
            "[872,  30] loss: 0.28419 acc: 25.750%\n",
            "[873,  30] loss: 0.27758 acc: 26.300%\n",
            "[874,  30] loss: 0.27214 acc: 25.650%\n",
            "[875,  30] loss: 0.28093 acc: 25.800%\n",
            "[876,  30] loss: 0.26891 acc: 26.300%\n",
            "[877,  30] loss: 0.27380 acc: 26.200%\n",
            "[878,  30] loss: 0.27313 acc: 26.050%\n",
            "[879,  30] loss: 0.27475 acc: 26.300%\n",
            "[880,  30] loss: 0.26466 acc: 26.600%\n",
            "[881,  30] loss: 0.27237 acc: 25.900%\n",
            "[882,  30] loss: 0.26932 acc: 26.100%\n",
            "[883,  30] loss: 0.27775 acc: 25.700%\n",
            "[884,  30] loss: 0.26410 acc: 26.200%\n",
            "[885,  30] loss: 0.26645 acc: 26.100%\n",
            "[886,  30] loss: 0.27254 acc: 25.550%\n",
            "[887,  30] loss: 0.27140 acc: 25.800%\n",
            "[888,  30] loss: 0.27232 acc: 25.700%\n",
            "[889,  30] loss: 0.26668 acc: 26.550%\n",
            "[890,  30] loss: 0.27104 acc: 25.850%\n",
            "[891,  30] loss: 0.26641 acc: 26.500%\n",
            "[892,  30] loss: 0.26091 acc: 26.000%\n",
            "[893,  30] loss: 0.26843 acc: 26.350%\n",
            "[894,  30] loss: 0.27320 acc: 25.800%\n",
            "[895,  30] loss: 0.26674 acc: 25.900%\n",
            "[896,  30] loss: 0.26990 acc: 26.300%\n",
            "[897,  30] loss: 0.25887 acc: 26.600%\n",
            "[898,  30] loss: 0.26761 acc: 26.100%\n",
            "[899,  30] loss: 0.25491 acc: 26.550%\n",
            "[900,  30] loss: 0.26105 acc: 26.850%\n",
            "[901,  30] loss: 0.26375 acc: 26.350%\n",
            "[902,  30] loss: 0.26003 acc: 26.400%\n",
            "[903,  30] loss: 0.26022 acc: 26.050%\n",
            "[904,  30] loss: 0.26365 acc: 26.550%\n",
            "[905,  30] loss: 0.26369 acc: 25.850%\n",
            "[906,  30] loss: 0.25906 acc: 26.950%\n",
            "[907,  30] loss: 0.25624 acc: 26.750%\n",
            "[908,  30] loss: 0.26422 acc: 26.250%\n",
            "[909,  30] loss: 0.26044 acc: 26.300%\n",
            "[910,  30] loss: 0.25866 acc: 26.800%\n",
            "[911,  30] loss: 0.26088 acc: 26.350%\n",
            "[912,  30] loss: 0.25868 acc: 26.750%\n",
            "[913,  30] loss: 0.25815 acc: 26.650%\n",
            "[914,  30] loss: 0.26297 acc: 26.150%\n",
            "[915,  30] loss: 0.26105 acc: 26.600%\n",
            "[916,  30] loss: 0.24876 acc: 27.150%\n",
            "[917,  30] loss: 0.26110 acc: 26.250%\n",
            "[918,  30] loss: 0.25359 acc: 26.750%\n",
            "[919,  30] loss: 0.25115 acc: 26.850%\n",
            "[920,  30] loss: 0.25505 acc: 26.450%\n",
            "[921,  30] loss: 0.25742 acc: 26.500%\n",
            "[922,  30] loss: 0.25080 acc: 26.200%\n",
            "[923,  30] loss: 0.25266 acc: 26.950%\n",
            "[924,  30] loss: 0.25412 acc: 26.550%\n",
            "[925,  30] loss: 0.25651 acc: 26.350%\n",
            "[926,  30] loss: 0.24956 acc: 26.950%\n",
            "[927,  30] loss: 0.25734 acc: 26.600%\n",
            "[928,  30] loss: 0.25088 acc: 26.300%\n",
            "[929,  30] loss: 0.25092 acc: 26.700%\n",
            "[930,  30] loss: 0.25189 acc: 26.700%\n",
            "[931,  30] loss: 0.25414 acc: 25.900%\n",
            "[932,  30] loss: 0.25290 acc: 27.050%\n",
            "[933,  30] loss: 0.24634 acc: 26.750%\n",
            "[934,  30] loss: 0.24329 acc: 26.800%\n",
            "[935,  30] loss: 0.25141 acc: 26.600%\n",
            "[936,  30] loss: 0.24273 acc: 26.600%\n",
            "[937,  30] loss: 0.25148 acc: 26.550%\n",
            "[938,  30] loss: 0.24657 acc: 26.900%\n",
            "[939,  30] loss: 0.24872 acc: 26.900%\n",
            "[940,  30] loss: 0.24361 acc: 26.750%\n",
            "[941,  30] loss: 0.24567 acc: 26.850%\n",
            "[942,  30] loss: 0.24537 acc: 27.100%\n",
            "[943,  30] loss: 0.24264 acc: 27.200%\n",
            "[944,  30] loss: 0.24077 acc: 27.250%\n",
            "[945,  30] loss: 0.23887 acc: 27.150%\n",
            "[946,  30] loss: 0.23847 acc: 27.300%\n",
            "[947,  30] loss: 0.23637 acc: 27.650%\n",
            "[948,  30] loss: 0.24400 acc: 27.000%\n",
            "[949,  30] loss: 0.23519 acc: 27.350%\n",
            "[950,  30] loss: 0.23581 acc: 27.300%\n",
            "[951,  30] loss: 0.23789 acc: 26.850%\n",
            "[952,  30] loss: 0.24709 acc: 26.500%\n",
            "[953,  30] loss: 0.24262 acc: 27.250%\n",
            "[954,  30] loss: 0.23810 acc: 27.000%\n",
            "[955,  30] loss: 0.23717 acc: 27.150%\n",
            "[956,  30] loss: 0.24010 acc: 27.200%\n",
            "[957,  30] loss: 0.24699 acc: 26.450%\n",
            "[958,  30] loss: 0.23844 acc: 27.000%\n",
            "[959,  30] loss: 0.24138 acc: 27.050%\n",
            "[960,  30] loss: 0.23211 acc: 27.600%\n",
            "[961,  30] loss: 0.24256 acc: 27.150%\n",
            "[962,  30] loss: 0.22537 acc: 27.750%\n",
            "[963,  30] loss: 0.22928 acc: 27.500%\n",
            "[964,  30] loss: 0.23736 acc: 26.900%\n",
            "[965,  30] loss: 0.24564 acc: 27.000%\n",
            "[966,  30] loss: 0.24414 acc: 26.750%\n",
            "[967,  30] loss: 0.23305 acc: 27.300%\n",
            "[968,  30] loss: 0.22953 acc: 27.750%\n",
            "[969,  30] loss: 0.23337 acc: 26.750%\n",
            "[970,  30] loss: 0.23629 acc: 27.000%\n",
            "[971,  30] loss: 0.22898 acc: 27.000%\n",
            "[972,  30] loss: 0.22806 acc: 27.100%\n",
            "[973,  30] loss: 0.23280 acc: 27.250%\n",
            "[974,  30] loss: 0.23721 acc: 26.700%\n",
            "[975,  30] loss: 0.23442 acc: 26.900%\n",
            "[976,  30] loss: 0.22958 acc: 27.300%\n",
            "[977,  30] loss: 0.22848 acc: 27.000%\n",
            "[978,  30] loss: 0.22959 acc: 27.500%\n",
            "[979,  30] loss: 0.22685 acc: 27.100%\n",
            "[980,  30] loss: 0.23299 acc: 27.050%\n",
            "[981,  30] loss: 0.22443 acc: 27.500%\n",
            "[982,  30] loss: 0.22966 acc: 27.400%\n",
            "[983,  30] loss: 0.22889 acc: 27.400%\n",
            "[984,  30] loss: 0.22742 acc: 27.350%\n",
            "[985,  30] loss: 0.22744 acc: 27.200%\n",
            "[986,  30] loss: 0.22381 acc: 27.700%\n",
            "[987,  30] loss: 0.23243 acc: 26.950%\n",
            "[988,  30] loss: 0.21847 acc: 27.750%\n",
            "[989,  30] loss: 0.22486 acc: 27.300%\n",
            "[990,  30] loss: 0.22025 acc: 27.600%\n",
            "[991,  30] loss: 0.21837 acc: 27.550%\n",
            "[992,  30] loss: 0.22037 acc: 27.200%\n",
            "[993,  30] loss: 0.22050 acc: 27.250%\n",
            "[994,  30] loss: 0.22458 acc: 27.400%\n",
            "[995,  30] loss: 0.22017 acc: 27.600%\n",
            "[996,  30] loss: 0.22171 acc: 27.950%\n",
            "[997,  30] loss: 0.21716 acc: 27.600%\n",
            "[998,  30] loss: 0.21241 acc: 27.550%\n",
            "[999,  30] loss: 0.21723 acc: 27.400%\n",
            "[1000,  30] loss: 0.21428 acc: 28.150%\n",
            "[1001,  30] loss: 0.21822 acc: 27.650%\n",
            "[1002,  30] loss: 0.22220 acc: 27.100%\n",
            "[1003,  30] loss: 0.21862 acc: 27.500%\n",
            "[1004,  30] loss: 0.21552 acc: 27.650%\n",
            "[1005,  30] loss: 0.21177 acc: 27.950%\n",
            "[1006,  30] loss: 0.22756 acc: 27.200%\n",
            "[1007,  30] loss: 0.21568 acc: 27.350%\n",
            "[1008,  30] loss: 0.21709 acc: 27.500%\n",
            "[1009,  30] loss: 0.20572 acc: 27.800%\n",
            "[1010,  30] loss: 0.21651 acc: 27.350%\n",
            "[1011,  30] loss: 0.21365 acc: 27.700%\n",
            "[1012,  30] loss: 0.21337 acc: 27.450%\n",
            "[1013,  30] loss: 0.21038 acc: 27.650%\n",
            "[1014,  30] loss: 0.21602 acc: 27.350%\n",
            "[1015,  30] loss: 0.21744 acc: 27.350%\n",
            "[1016,  30] loss: 0.21782 acc: 27.750%\n",
            "[1017,  30] loss: 0.22198 acc: 27.000%\n",
            "[1018,  30] loss: 0.21303 acc: 27.600%\n",
            "[1019,  30] loss: 0.21056 acc: 27.950%\n",
            "[1020,  30] loss: 0.20858 acc: 27.800%\n",
            "[1021,  30] loss: 0.20313 acc: 28.200%\n",
            "[1022,  30] loss: 0.20876 acc: 27.950%\n",
            "[1023,  30] loss: 0.21815 acc: 27.550%\n",
            "[1024,  30] loss: 0.20590 acc: 28.000%\n",
            "[1025,  30] loss: 0.20566 acc: 27.850%\n",
            "[1026,  30] loss: 0.20691 acc: 28.250%\n",
            "[1027,  30] loss: 0.20879 acc: 28.250%\n",
            "[1028,  30] loss: 0.20470 acc: 28.350%\n",
            "[1029,  30] loss: 0.20020 acc: 28.000%\n",
            "[1030,  30] loss: 0.20813 acc: 27.850%\n",
            "[1031,  30] loss: 0.21238 acc: 27.650%\n",
            "[1032,  30] loss: 0.21068 acc: 27.600%\n",
            "[1033,  30] loss: 0.20953 acc: 27.300%\n",
            "[1034,  30] loss: 0.20705 acc: 28.300%\n",
            "[1035,  30] loss: 0.20587 acc: 27.700%\n",
            "[1036,  30] loss: 0.20164 acc: 27.950%\n",
            "[1037,  30] loss: 0.19961 acc: 27.900%\n",
            "[1038,  30] loss: 0.20992 acc: 27.450%\n",
            "[1039,  30] loss: 0.20678 acc: 27.750%\n",
            "[1040,  30] loss: 0.21035 acc: 27.900%\n",
            "[1041,  30] loss: 0.19984 acc: 28.100%\n",
            "[1042,  30] loss: 0.20497 acc: 27.750%\n",
            "[1043,  30] loss: 0.19604 acc: 28.500%\n",
            "[1044,  30] loss: 0.21151 acc: 27.600%\n",
            "[1045,  30] loss: 0.19943 acc: 28.300%\n",
            "[1046,  30] loss: 0.20188 acc: 27.650%\n",
            "[1047,  30] loss: 0.20469 acc: 28.100%\n",
            "[1048,  30] loss: 0.19922 acc: 28.200%\n",
            "[1049,  30] loss: 0.19353 acc: 28.100%\n",
            "[1050,  30] loss: 0.20296 acc: 28.050%\n",
            "[1051,  30] loss: 0.19604 acc: 28.100%\n",
            "[1052,  30] loss: 0.20076 acc: 28.000%\n",
            "[1053,  30] loss: 0.19990 acc: 27.900%\n",
            "[1054,  30] loss: 0.20125 acc: 27.850%\n",
            "[1055,  30] loss: 0.19566 acc: 27.900%\n",
            "[1056,  30] loss: 0.19406 acc: 28.050%\n",
            "[1057,  30] loss: 0.19398 acc: 28.050%\n",
            "[1058,  30] loss: 0.19249 acc: 27.750%\n",
            "[1059,  30] loss: 0.19348 acc: 28.100%\n",
            "[1060,  30] loss: 0.19469 acc: 27.850%\n",
            "[1061,  30] loss: 0.18571 acc: 28.400%\n",
            "[1062,  30] loss: 0.19086 acc: 28.400%\n",
            "[1063,  30] loss: 0.19027 acc: 28.300%\n",
            "[1064,  30] loss: 0.19671 acc: 27.800%\n",
            "[1065,  30] loss: 0.19662 acc: 28.050%\n",
            "[1066,  30] loss: 0.19368 acc: 28.300%\n",
            "[1067,  30] loss: 0.18841 acc: 28.200%\n",
            "[1068,  30] loss: 0.19349 acc: 28.150%\n",
            "[1069,  30] loss: 0.19310 acc: 28.100%\n",
            "[1070,  30] loss: 0.19292 acc: 28.100%\n",
            "[1071,  30] loss: 0.19177 acc: 28.050%\n",
            "[1072,  30] loss: 0.19245 acc: 28.400%\n",
            "[1073,  30] loss: 0.19211 acc: 27.950%\n",
            "[1074,  30] loss: 0.18922 acc: 28.000%\n",
            "[1075,  30] loss: 0.19818 acc: 27.650%\n",
            "[1076,  30] loss: 0.18093 acc: 28.450%\n",
            "[1077,  30] loss: 0.19004 acc: 27.850%\n",
            "[1078,  30] loss: 0.18839 acc: 28.450%\n",
            "[1079,  30] loss: 0.18346 acc: 28.300%\n",
            "[1080,  30] loss: 0.18917 acc: 28.100%\n",
            "[1081,  30] loss: 0.18731 acc: 28.150%\n",
            "[1082,  30] loss: 0.18310 acc: 28.100%\n",
            "[1083,  30] loss: 0.19038 acc: 28.000%\n",
            "[1084,  30] loss: 0.18703 acc: 28.400%\n",
            "[1085,  30] loss: 0.18485 acc: 28.500%\n",
            "[1086,  30] loss: 0.18811 acc: 28.100%\n",
            "[1087,  30] loss: 0.18743 acc: 28.650%\n",
            "[1088,  30] loss: 0.18665 acc: 28.400%\n",
            "[1089,  30] loss: 0.18183 acc: 28.650%\n",
            "[1090,  30] loss: 0.18697 acc: 28.350%\n",
            "[1091,  30] loss: 0.18040 acc: 28.750%\n",
            "[1092,  30] loss: 0.17859 acc: 28.550%\n",
            "[1093,  30] loss: 0.18391 acc: 28.400%\n",
            "[1094,  30] loss: 0.17836 acc: 28.650%\n",
            "[1095,  30] loss: 0.18190 acc: 28.500%\n",
            "[1096,  30] loss: 0.17888 acc: 28.400%\n",
            "[1097,  30] loss: 0.18357 acc: 28.200%\n",
            "[1098,  30] loss: 0.18411 acc: 28.450%\n",
            "[1099,  30] loss: 0.18181 acc: 28.300%\n",
            "[1100,  30] loss: 0.18001 acc: 28.300%\n",
            "[1101,  30] loss: 0.18295 acc: 28.300%\n",
            "[1102,  30] loss: 0.18178 acc: 28.450%\n",
            "[1103,  30] loss: 0.18225 acc: 28.600%\n",
            "[1104,  30] loss: 0.17198 acc: 29.000%\n",
            "[1105,  30] loss: 0.16972 acc: 28.650%\n",
            "[1106,  30] loss: 0.18678 acc: 28.350%\n",
            "[1107,  30] loss: 0.17597 acc: 28.700%\n",
            "[1108,  30] loss: 0.17741 acc: 28.500%\n",
            "[1109,  30] loss: 0.18067 acc: 28.300%\n",
            "[1110,  30] loss: 0.18257 acc: 28.250%\n",
            "[1111,  30] loss: 0.17054 acc: 28.750%\n",
            "[1112,  30] loss: 0.18166 acc: 28.150%\n",
            "[1113,  30] loss: 0.18041 acc: 28.150%\n",
            "[1114,  30] loss: 0.17723 acc: 28.550%\n",
            "[1115,  30] loss: 0.17793 acc: 28.400%\n",
            "[1116,  30] loss: 0.17825 acc: 28.300%\n",
            "[1117,  30] loss: 0.17184 acc: 28.450%\n",
            "[1118,  30] loss: 0.17790 acc: 28.550%\n",
            "[1119,  30] loss: 0.17701 acc: 28.450%\n",
            "[1120,  30] loss: 0.17498 acc: 28.800%\n",
            "[1121,  30] loss: 0.17599 acc: 28.000%\n",
            "[1122,  30] loss: 0.17958 acc: 28.300%\n",
            "[1123,  30] loss: 0.17300 acc: 28.400%\n",
            "[1124,  30] loss: 0.17495 acc: 28.250%\n",
            "[1125,  30] loss: 0.17409 acc: 28.450%\n",
            "[1126,  30] loss: 0.17565 acc: 28.400%\n",
            "[1127,  30] loss: 0.17665 acc: 28.200%\n",
            "[1128,  30] loss: 0.17476 acc: 28.750%\n",
            "[1129,  30] loss: 0.17041 acc: 28.850%\n",
            "[1130,  30] loss: 0.17621 acc: 28.200%\n",
            "[1131,  30] loss: 0.17102 acc: 28.350%\n",
            "[1132,  30] loss: 0.17623 acc: 28.500%\n",
            "[1133,  30] loss: 0.17025 acc: 28.700%\n",
            "[1134,  30] loss: 0.16848 acc: 28.850%\n",
            "[1135,  30] loss: 0.17523 acc: 28.550%\n",
            "[1136,  30] loss: 0.17618 acc: 28.400%\n",
            "[1137,  30] loss: 0.16233 acc: 28.900%\n",
            "[1138,  30] loss: 0.16833 acc: 28.450%\n",
            "[1139,  30] loss: 0.16724 acc: 28.700%\n",
            "[1140,  30] loss: 0.16683 acc: 28.850%\n",
            "[1141,  30] loss: 0.16549 acc: 28.650%\n",
            "[1142,  30] loss: 0.16979 acc: 28.150%\n",
            "[1143,  30] loss: 0.16505 acc: 28.750%\n",
            "[1144,  30] loss: 0.16407 acc: 28.500%\n",
            "[1145,  30] loss: 0.16889 acc: 28.450%\n",
            "[1146,  30] loss: 0.16804 acc: 28.500%\n",
            "[1147,  30] loss: 0.16241 acc: 28.800%\n",
            "[1148,  30] loss: 0.16682 acc: 28.700%\n",
            "[1149,  30] loss: 0.16681 acc: 28.600%\n",
            "[1150,  30] loss: 0.16363 acc: 28.450%\n",
            "[1151,  30] loss: 0.16580 acc: 28.950%\n",
            "[1152,  30] loss: 0.16685 acc: 28.750%\n",
            "[1153,  30] loss: 0.16480 acc: 28.750%\n",
            "[1154,  30] loss: 0.16138 acc: 28.700%\n",
            "[1155,  30] loss: 0.16364 acc: 28.650%\n",
            "[1156,  30] loss: 0.16801 acc: 28.700%\n",
            "[1157,  30] loss: 0.16497 acc: 28.300%\n",
            "[1158,  30] loss: 0.16244 acc: 29.100%\n",
            "[1159,  30] loss: 0.15296 acc: 28.900%\n",
            "[1160,  30] loss: 0.16160 acc: 28.800%\n",
            "[1161,  30] loss: 0.16650 acc: 28.950%\n",
            "[1162,  30] loss: 0.15941 acc: 28.950%\n",
            "[1163,  30] loss: 0.16037 acc: 28.450%\n",
            "[1164,  30] loss: 0.16042 acc: 29.100%\n",
            "[1165,  30] loss: 0.16337 acc: 28.750%\n",
            "[1166,  30] loss: 0.16491 acc: 28.800%\n",
            "[1167,  30] loss: 0.15514 acc: 28.550%\n",
            "[1168,  30] loss: 0.16303 acc: 28.300%\n",
            "[1169,  30] loss: 0.16113 acc: 28.800%\n",
            "[1170,  30] loss: 0.15293 acc: 29.000%\n",
            "[1171,  30] loss: 0.16175 acc: 29.000%\n",
            "[1172,  30] loss: 0.16066 acc: 28.850%\n",
            "[1173,  30] loss: 0.15684 acc: 28.750%\n",
            "[1174,  30] loss: 0.15816 acc: 28.900%\n",
            "[1175,  30] loss: 0.15729 acc: 28.800%\n",
            "[1176,  30] loss: 0.15284 acc: 29.300%\n",
            "[1177,  30] loss: 0.15495 acc: 28.750%\n",
            "[1178,  30] loss: 0.15667 acc: 29.200%\n",
            "[1179,  30] loss: 0.15910 acc: 28.950%\n",
            "[1180,  30] loss: 0.15499 acc: 28.750%\n",
            "[1181,  30] loss: 0.15700 acc: 28.750%\n",
            "[1182,  30] loss: 0.15464 acc: 29.000%\n",
            "[1183,  30] loss: 0.15296 acc: 28.950%\n",
            "[1184,  30] loss: 0.14805 acc: 29.250%\n",
            "[1185,  30] loss: 0.15975 acc: 28.450%\n",
            "[1186,  30] loss: 0.15605 acc: 28.600%\n",
            "[1187,  30] loss: 0.14877 acc: 28.800%\n",
            "[1188,  30] loss: 0.15630 acc: 28.800%\n",
            "[1189,  30] loss: 0.15251 acc: 28.700%\n",
            "[1190,  30] loss: 0.15215 acc: 29.250%\n",
            "[1191,  30] loss: 0.15493 acc: 29.050%\n",
            "[1192,  30] loss: 0.15289 acc: 28.900%\n",
            "[1193,  30] loss: 0.15252 acc: 28.950%\n",
            "[1194,  30] loss: 0.14936 acc: 29.050%\n",
            "[1195,  30] loss: 0.15294 acc: 28.800%\n",
            "[1196,  30] loss: 0.15481 acc: 28.750%\n",
            "[1197,  30] loss: 0.14840 acc: 29.300%\n",
            "[1198,  30] loss: 0.15161 acc: 29.000%\n",
            "[1199,  30] loss: 0.14964 acc: 28.650%\n",
            "[1200,  30] loss: 0.14672 acc: 29.000%\n",
            "[1201,  30] loss: 0.14230 acc: 29.300%\n",
            "[1202,  30] loss: 0.15409 acc: 28.750%\n",
            "[1203,  30] loss: 0.14804 acc: 28.750%\n",
            "[1204,  30] loss: 0.14941 acc: 28.750%\n",
            "[1205,  30] loss: 0.14630 acc: 28.850%\n",
            "[1206,  30] loss: 0.14621 acc: 29.000%\n",
            "[1207,  30] loss: 0.15125 acc: 28.850%\n",
            "[1208,  30] loss: 0.14700 acc: 29.050%\n",
            "[1209,  30] loss: 0.14448 acc: 29.250%\n",
            "[1210,  30] loss: 0.14845 acc: 28.900%\n",
            "[1211,  30] loss: 0.14880 acc: 28.700%\n",
            "[1212,  30] loss: 0.14642 acc: 29.000%\n",
            "[1213,  30] loss: 0.14718 acc: 28.800%\n",
            "[1214,  30] loss: 0.14891 acc: 28.650%\n",
            "[1215,  30] loss: 0.14450 acc: 29.050%\n",
            "[1216,  30] loss: 0.14122 acc: 29.200%\n",
            "[1217,  30] loss: 0.14594 acc: 28.800%\n",
            "[1218,  30] loss: 0.14425 acc: 29.100%\n",
            "[1219,  30] loss: 0.14447 acc: 28.750%\n",
            "[1220,  30] loss: 0.14436 acc: 29.000%\n",
            "[1221,  30] loss: 0.14846 acc: 28.850%\n",
            "[1222,  30] loss: 0.14654 acc: 28.800%\n",
            "[1223,  30] loss: 0.14544 acc: 28.950%\n",
            "[1224,  30] loss: 0.14657 acc: 28.900%\n",
            "[1225,  30] loss: 0.14179 acc: 29.350%\n",
            "[1226,  30] loss: 0.14095 acc: 29.300%\n",
            "[1227,  30] loss: 0.14042 acc: 29.100%\n",
            "[1228,  30] loss: 0.14145 acc: 29.150%\n",
            "[1229,  30] loss: 0.13955 acc: 29.250%\n",
            "[1230,  30] loss: 0.14120 acc: 29.300%\n",
            "[1231,  30] loss: 0.14049 acc: 29.100%\n",
            "[1232,  30] loss: 0.14480 acc: 29.050%\n",
            "[1233,  30] loss: 0.14119 acc: 28.750%\n",
            "[1234,  30] loss: 0.14419 acc: 28.950%\n",
            "[1235,  30] loss: 0.14393 acc: 29.150%\n",
            "[1236,  30] loss: 0.14318 acc: 29.100%\n",
            "[1237,  30] loss: 0.13668 acc: 29.350%\n",
            "[1238,  30] loss: 0.13956 acc: 29.250%\n",
            "[1239,  30] loss: 0.13897 acc: 29.300%\n",
            "[1240,  30] loss: 0.14268 acc: 28.900%\n",
            "[1241,  30] loss: 0.14160 acc: 29.050%\n",
            "[1242,  30] loss: 0.14289 acc: 28.750%\n",
            "[1243,  30] loss: 0.13769 acc: 29.400%\n",
            "[1244,  30] loss: 0.13870 acc: 29.150%\n",
            "[1245,  30] loss: 0.14107 acc: 28.950%\n",
            "[1246,  30] loss: 0.13694 acc: 29.050%\n",
            "[1247,  30] loss: 0.13994 acc: 29.050%\n",
            "[1248,  30] loss: 0.13569 acc: 29.350%\n",
            "[1249,  30] loss: 0.14145 acc: 28.800%\n",
            "[1250,  30] loss: 0.13293 acc: 29.200%\n",
            "[1251,  30] loss: 0.13420 acc: 29.150%\n",
            "[1252,  30] loss: 0.13629 acc: 29.000%\n",
            "[1253,  30] loss: 0.13782 acc: 29.250%\n",
            "[1254,  30] loss: 0.13163 acc: 29.500%\n",
            "[1255,  30] loss: 0.13472 acc: 29.000%\n",
            "[1256,  30] loss: 0.13212 acc: 29.100%\n",
            "[1257,  30] loss: 0.13228 acc: 29.350%\n",
            "[1258,  30] loss: 0.13367 acc: 28.850%\n",
            "[1259,  30] loss: 0.13196 acc: 29.100%\n",
            "[1260,  30] loss: 0.13139 acc: 29.000%\n",
            "[1261,  30] loss: 0.13310 acc: 29.450%\n",
            "[1262,  30] loss: 0.12998 acc: 29.250%\n",
            "[1263,  30] loss: 0.13512 acc: 29.100%\n",
            "[1264,  30] loss: 0.13274 acc: 29.250%\n",
            "[1265,  30] loss: 0.14561 acc: 28.850%\n",
            "[1266,  30] loss: 0.12813 acc: 29.650%\n",
            "[1267,  30] loss: 0.13605 acc: 29.000%\n",
            "[1268,  30] loss: 0.13089 acc: 29.300%\n",
            "[1269,  30] loss: 0.12436 acc: 29.500%\n",
            "[1270,  30] loss: 0.13095 acc: 29.050%\n",
            "[1271,  30] loss: 0.13277 acc: 29.300%\n",
            "[1272,  30] loss: 0.13589 acc: 29.250%\n",
            "[1273,  30] loss: 0.12505 acc: 29.450%\n",
            "[1274,  30] loss: 0.12955 acc: 29.250%\n",
            "[1275,  30] loss: 0.12419 acc: 29.450%\n",
            "[1276,  30] loss: 0.12854 acc: 29.050%\n",
            "[1277,  30] loss: 0.12454 acc: 29.400%\n",
            "[1278,  30] loss: 0.13393 acc: 29.250%\n",
            "[1279,  30] loss: 0.13088 acc: 28.950%\n",
            "[1280,  30] loss: 0.12805 acc: 29.250%\n",
            "[1281,  30] loss: 0.12345 acc: 29.200%\n",
            "[1282,  30] loss: 0.12873 acc: 29.250%\n",
            "[1283,  30] loss: 0.13105 acc: 29.300%\n",
            "[1284,  30] loss: 0.12983 acc: 29.250%\n",
            "[1285,  30] loss: 0.13000 acc: 29.350%\n",
            "[1286,  30] loss: 0.12458 acc: 29.250%\n",
            "[1287,  30] loss: 0.12790 acc: 29.350%\n",
            "[1288,  30] loss: 0.12453 acc: 29.450%\n",
            "[1289,  30] loss: 0.12682 acc: 29.100%\n",
            "[1290,  30] loss: 0.12671 acc: 29.350%\n",
            "[1291,  30] loss: 0.12672 acc: 29.050%\n",
            "[1292,  30] loss: 0.12833 acc: 29.200%\n",
            "[1293,  30] loss: 0.12505 acc: 29.350%\n",
            "[1294,  30] loss: 0.12433 acc: 29.650%\n",
            "[1295,  30] loss: 0.12491 acc: 29.200%\n",
            "[1296,  30] loss: 0.12514 acc: 29.550%\n",
            "[1297,  30] loss: 0.12389 acc: 29.300%\n",
            "[1298,  30] loss: 0.12706 acc: 29.200%\n",
            "[1299,  30] loss: 0.12399 acc: 29.150%\n",
            "[1300,  30] loss: 0.12813 acc: 29.300%\n",
            "[1301,  30] loss: 0.12547 acc: 29.550%\n",
            "[1302,  30] loss: 0.12405 acc: 29.300%\n",
            "[1303,  30] loss: 0.12477 acc: 29.350%\n",
            "[1304,  30] loss: 0.12594 acc: 29.200%\n",
            "[1305,  30] loss: 0.11997 acc: 29.550%\n",
            "[1306,  30] loss: 0.12432 acc: 29.250%\n",
            "[1307,  30] loss: 0.12463 acc: 29.350%\n",
            "[1308,  30] loss: 0.12149 acc: 29.400%\n",
            "[1309,  30] loss: 0.12369 acc: 29.350%\n",
            "[1310,  30] loss: 0.11807 acc: 29.350%\n",
            "[1311,  30] loss: 0.11941 acc: 29.350%\n",
            "[1312,  30] loss: 0.12438 acc: 29.150%\n",
            "[1313,  30] loss: 0.11916 acc: 29.400%\n",
            "[1314,  30] loss: 0.11655 acc: 29.650%\n",
            "[1315,  30] loss: 0.12355 acc: 29.150%\n",
            "[1316,  30] loss: 0.11744 acc: 29.700%\n",
            "[1317,  30] loss: 0.12298 acc: 29.250%\n",
            "[1318,  30] loss: 0.11847 acc: 29.350%\n",
            "[1319,  30] loss: 0.11738 acc: 29.550%\n",
            "[1320,  30] loss: 0.11865 acc: 29.400%\n",
            "[1321,  30] loss: 0.12377 acc: 29.400%\n",
            "[1322,  30] loss: 0.11959 acc: 29.400%\n",
            "[1323,  30] loss: 0.11528 acc: 29.200%\n",
            "[1324,  30] loss: 0.12396 acc: 29.400%\n",
            "[1325,  30] loss: 0.11782 acc: 29.200%\n",
            "[1326,  30] loss: 0.12133 acc: 29.300%\n",
            "[1327,  30] loss: 0.12229 acc: 29.450%\n",
            "[1328,  30] loss: 0.11849 acc: 29.400%\n",
            "[1329,  30] loss: 0.11720 acc: 29.500%\n",
            "[1330,  30] loss: 0.11822 acc: 29.550%\n",
            "[1331,  30] loss: 0.12177 acc: 29.250%\n",
            "[1332,  30] loss: 0.11428 acc: 29.400%\n",
            "[1333,  30] loss: 0.11626 acc: 29.200%\n",
            "[1334,  30] loss: 0.12139 acc: 29.500%\n",
            "[1335,  30] loss: 0.11928 acc: 29.350%\n",
            "[1336,  30] loss: 0.11593 acc: 29.700%\n",
            "[1337,  30] loss: 0.11703 acc: 29.450%\n",
            "[1338,  30] loss: 0.11496 acc: 29.650%\n",
            "[1339,  30] loss: 0.12116 acc: 29.250%\n",
            "[1340,  30] loss: 0.11302 acc: 29.450%\n",
            "[1341,  30] loss: 0.11191 acc: 29.600%\n",
            "[1342,  30] loss: 0.11308 acc: 29.600%\n",
            "[1343,  30] loss: 0.11671 acc: 29.350%\n",
            "[1344,  30] loss: 0.11561 acc: 29.350%\n",
            "[1345,  30] loss: 0.11408 acc: 29.450%\n",
            "[1346,  30] loss: 0.11371 acc: 29.250%\n",
            "[1347,  30] loss: 0.11283 acc: 29.400%\n",
            "[1348,  30] loss: 0.12000 acc: 29.100%\n",
            "[1349,  30] loss: 0.11311 acc: 29.450%\n",
            "[1350,  30] loss: 0.11306 acc: 29.550%\n",
            "[1351,  30] loss: 0.11199 acc: 29.650%\n",
            "[1352,  30] loss: 0.11570 acc: 29.350%\n",
            "[1353,  30] loss: 0.11143 acc: 29.250%\n",
            "[1354,  30] loss: 0.11551 acc: 29.300%\n",
            "[1355,  30] loss: 0.10792 acc: 29.650%\n",
            "[1356,  30] loss: 0.11013 acc: 29.300%\n",
            "[1357,  30] loss: 0.10918 acc: 29.350%\n",
            "[1358,  30] loss: 0.10832 acc: 29.700%\n",
            "[1359,  30] loss: 0.11247 acc: 29.500%\n",
            "[1360,  30] loss: 0.11286 acc: 29.400%\n",
            "[1361,  30] loss: 0.11101 acc: 29.550%\n",
            "[1362,  30] loss: 0.11252 acc: 29.650%\n",
            "[1363,  30] loss: 0.11491 acc: 29.400%\n",
            "[1364,  30] loss: 0.10547 acc: 29.400%\n",
            "[1365,  30] loss: 0.10731 acc: 29.700%\n",
            "[1366,  30] loss: 0.11099 acc: 29.450%\n",
            "[1367,  30] loss: 0.11173 acc: 29.400%\n",
            "[1368,  30] loss: 0.10683 acc: 29.650%\n",
            "[1369,  30] loss: 0.11342 acc: 29.500%\n",
            "[1370,  30] loss: 0.11003 acc: 29.350%\n",
            "[1371,  30] loss: 0.11487 acc: 29.150%\n",
            "[1372,  30] loss: 0.11031 acc: 29.600%\n",
            "[1373,  30] loss: 0.10770 acc: 29.450%\n",
            "[1374,  30] loss: 0.11160 acc: 29.400%\n",
            "[1375,  30] loss: 0.10699 acc: 29.350%\n",
            "[1376,  30] loss: 0.10671 acc: 29.450%\n",
            "[1377,  30] loss: 0.10676 acc: 29.400%\n",
            "[1378,  30] loss: 0.10865 acc: 29.500%\n",
            "[1379,  30] loss: 0.10890 acc: 29.400%\n",
            "[1380,  30] loss: 0.10892 acc: 29.500%\n",
            "[1381,  30] loss: 0.10789 acc: 29.500%\n",
            "[1382,  30] loss: 0.11090 acc: 29.250%\n",
            "[1383,  30] loss: 0.10993 acc: 29.300%\n",
            "[1384,  30] loss: 0.11034 acc: 29.400%\n",
            "[1385,  30] loss: 0.10865 acc: 29.550%\n",
            "[1386,  30] loss: 0.10576 acc: 29.350%\n",
            "[1387,  30] loss: 0.10922 acc: 29.500%\n",
            "[1388,  30] loss: 0.11166 acc: 29.250%\n",
            "[1389,  30] loss: 0.10589 acc: 29.800%\n",
            "[1390,  30] loss: 0.10781 acc: 29.200%\n",
            "[1391,  30] loss: 0.10337 acc: 29.450%\n",
            "[1392,  30] loss: 0.10289 acc: 29.800%\n",
            "[1393,  30] loss: 0.10453 acc: 29.550%\n",
            "[1394,  30] loss: 0.10600 acc: 29.400%\n",
            "[1395,  30] loss: 0.10709 acc: 29.550%\n",
            "[1396,  30] loss: 0.10691 acc: 29.500%\n",
            "[1397,  30] loss: 0.10400 acc: 29.550%\n",
            "[1398,  30] loss: 0.10575 acc: 29.600%\n",
            "[1399,  30] loss: 0.10101 acc: 29.500%\n",
            "[1400,  30] loss: 0.10018 acc: 29.750%\n",
            "[1401,  30] loss: 0.10074 acc: 29.650%\n",
            "[1402,  30] loss: 0.10189 acc: 29.550%\n",
            "[1403,  30] loss: 0.09823 acc: 29.800%\n",
            "[1404,  30] loss: 0.10612 acc: 29.400%\n",
            "[1405,  30] loss: 0.09900 acc: 29.700%\n",
            "[1406,  30] loss: 0.10100 acc: 29.500%\n",
            "[1407,  30] loss: 0.10303 acc: 29.300%\n",
            "[1408,  30] loss: 0.10636 acc: 29.500%\n",
            "[1409,  30] loss: 0.10308 acc: 29.500%\n",
            "[1410,  30] loss: 0.10376 acc: 29.550%\n",
            "[1411,  30] loss: 0.10270 acc: 29.450%\n",
            "[1412,  30] loss: 0.10234 acc: 29.450%\n",
            "[1413,  30] loss: 0.10269 acc: 29.650%\n",
            "[1414,  30] loss: 0.09873 acc: 29.800%\n",
            "[1415,  30] loss: 0.09991 acc: 29.550%\n",
            "[1416,  30] loss: 0.10434 acc: 29.300%\n",
            "[1417,  30] loss: 0.09620 acc: 29.600%\n",
            "[1418,  30] loss: 0.10340 acc: 29.450%\n",
            "[1419,  30] loss: 0.09914 acc: 29.500%\n",
            "[1420,  30] loss: 0.09442 acc: 29.800%\n",
            "[1421,  30] loss: 0.10519 acc: 29.250%\n",
            "[1422,  30] loss: 0.09530 acc: 29.800%\n",
            "[1423,  30] loss: 0.09581 acc: 29.800%\n",
            "[1424,  30] loss: 0.10043 acc: 29.600%\n",
            "[1425,  30] loss: 0.10199 acc: 29.600%\n",
            "[1426,  30] loss: 0.10214 acc: 29.450%\n",
            "[1427,  30] loss: 0.10439 acc: 29.650%\n",
            "[1428,  30] loss: 0.10480 acc: 29.300%\n",
            "[1429,  30] loss: 0.09859 acc: 29.600%\n",
            "[1430,  30] loss: 0.09764 acc: 29.650%\n",
            "[1431,  30] loss: 0.10017 acc: 29.450%\n",
            "[1432,  30] loss: 0.09818 acc: 29.850%\n",
            "[1433,  30] loss: 0.09950 acc: 29.550%\n",
            "[1434,  30] loss: 0.09749 acc: 29.650%\n",
            "[1435,  30] loss: 0.09835 acc: 29.700%\n",
            "[1436,  30] loss: 0.10284 acc: 29.450%\n",
            "[1437,  30] loss: 0.09195 acc: 29.800%\n",
            "[1438,  30] loss: 0.09821 acc: 29.500%\n",
            "[1439,  30] loss: 0.10164 acc: 29.350%\n",
            "[1440,  30] loss: 0.09725 acc: 29.650%\n",
            "[1441,  30] loss: 0.10059 acc: 29.500%\n",
            "[1442,  30] loss: 0.10185 acc: 29.450%\n",
            "[1443,  30] loss: 0.09435 acc: 29.550%\n",
            "[1444,  30] loss: 0.09969 acc: 29.350%\n",
            "[1445,  30] loss: 0.09609 acc: 29.700%\n",
            "[1446,  30] loss: 0.09681 acc: 29.550%\n",
            "[1447,  30] loss: 0.09720 acc: 29.350%\n",
            "[1448,  30] loss: 0.10005 acc: 29.450%\n",
            "[1449,  30] loss: 0.09344 acc: 29.800%\n",
            "[1450,  30] loss: 0.09272 acc: 29.650%\n",
            "[1451,  30] loss: 0.09165 acc: 29.500%\n",
            "[1452,  30] loss: 0.09483 acc: 29.600%\n",
            "[1453,  30] loss: 0.09106 acc: 29.900%\n",
            "[1454,  30] loss: 0.09826 acc: 29.650%\n",
            "[1455,  30] loss: 0.09117 acc: 29.850%\n",
            "[1456,  30] loss: 0.09578 acc: 29.600%\n",
            "[1457,  30] loss: 0.09647 acc: 29.750%\n",
            "[1458,  30] loss: 0.09690 acc: 29.400%\n",
            "[1459,  30] loss: 0.09539 acc: 29.400%\n",
            "[1460,  30] loss: 0.09374 acc: 29.550%\n",
            "[1461,  30] loss: 0.09122 acc: 29.750%\n",
            "[1462,  30] loss: 0.09398 acc: 29.550%\n",
            "[1463,  30] loss: 0.09693 acc: 29.600%\n",
            "[1464,  30] loss: 0.09503 acc: 29.450%\n",
            "[1465,  30] loss: 0.09190 acc: 29.800%\n",
            "[1466,  30] loss: 0.09224 acc: 29.550%\n",
            "[1467,  30] loss: 0.09413 acc: 29.500%\n",
            "[1468,  30] loss: 0.09186 acc: 29.750%\n",
            "[1469,  30] loss: 0.09203 acc: 29.750%\n",
            "[1470,  30] loss: 0.09080 acc: 29.650%\n",
            "[1471,  30] loss: 0.09206 acc: 29.750%\n",
            "[1472,  30] loss: 0.08965 acc: 29.650%\n",
            "[1473,  30] loss: 0.09426 acc: 29.450%\n",
            "[1474,  30] loss: 0.09385 acc: 29.350%\n",
            "[1475,  30] loss: 0.08889 acc: 29.650%\n",
            "[1476,  30] loss: 0.09266 acc: 29.650%\n",
            "[1477,  30] loss: 0.09267 acc: 29.750%\n",
            "[1478,  30] loss: 0.09173 acc: 29.450%\n",
            "[1479,  30] loss: 0.09152 acc: 29.700%\n",
            "[1480,  30] loss: 0.09346 acc: 29.500%\n",
            "[1481,  30] loss: 0.09263 acc: 29.650%\n",
            "[1482,  30] loss: 0.09004 acc: 29.800%\n",
            "[1483,  30] loss: 0.08908 acc: 29.700%\n",
            "[1484,  30] loss: 0.09254 acc: 29.500%\n",
            "[1485,  30] loss: 0.08972 acc: 29.550%\n",
            "[1486,  30] loss: 0.09186 acc: 29.500%\n",
            "[1487,  30] loss: 0.09195 acc: 29.550%\n",
            "[1488,  30] loss: 0.09251 acc: 29.700%\n",
            "[1489,  30] loss: 0.09002 acc: 29.550%\n",
            "[1490,  30] loss: 0.08751 acc: 29.800%\n",
            "[1491,  30] loss: 0.09066 acc: 29.650%\n",
            "[1492,  30] loss: 0.08466 acc: 29.800%\n",
            "[1493,  30] loss: 0.08876 acc: 29.700%\n",
            "[1494,  30] loss: 0.08983 acc: 29.650%\n",
            "[1495,  30] loss: 0.09037 acc: 29.650%\n",
            "[1496,  30] loss: 0.08835 acc: 29.650%\n",
            "[1497,  30] loss: 0.08599 acc: 29.850%\n",
            "[1498,  30] loss: 0.08872 acc: 29.750%\n",
            "[1499,  30] loss: 0.09015 acc: 29.550%\n",
            "[1500,  30] loss: 0.08935 acc: 29.650%\n",
            "[1501,  30] loss: 0.08897 acc: 29.750%\n",
            "[1502,  30] loss: 0.08624 acc: 29.700%\n",
            "[1503,  30] loss: 0.08646 acc: 29.800%\n",
            "[1504,  30] loss: 0.09123 acc: 29.650%\n",
            "[1505,  30] loss: 0.08804 acc: 29.600%\n",
            "[1506,  30] loss: 0.09258 acc: 29.550%\n",
            "[1507,  30] loss: 0.08896 acc: 29.550%\n",
            "[1508,  30] loss: 0.09043 acc: 29.700%\n",
            "[1509,  30] loss: 0.08937 acc: 29.550%\n",
            "[1510,  30] loss: 0.08644 acc: 29.650%\n",
            "[1511,  30] loss: 0.08873 acc: 29.650%\n",
            "[1512,  30] loss: 0.08693 acc: 29.800%\n",
            "[1513,  30] loss: 0.08697 acc: 29.650%\n",
            "[1514,  30] loss: 0.08555 acc: 29.700%\n",
            "[1515,  30] loss: 0.08704 acc: 29.700%\n",
            "[1516,  30] loss: 0.08678 acc: 29.650%\n",
            "[1517,  30] loss: 0.08841 acc: 29.700%\n",
            "[1518,  30] loss: 0.09141 acc: 29.500%\n",
            "[1519,  30] loss: 0.09047 acc: 29.650%\n",
            "[1520,  30] loss: 0.08978 acc: 29.600%\n",
            "[1521,  30] loss: 0.08436 acc: 29.750%\n",
            "[1522,  30] loss: 0.08351 acc: 29.850%\n",
            "[1523,  30] loss: 0.08785 acc: 29.650%\n",
            "[1524,  30] loss: 0.08606 acc: 29.550%\n",
            "[1525,  30] loss: 0.08327 acc: 29.800%\n",
            "[1526,  30] loss: 0.08449 acc: 29.500%\n",
            "[1527,  30] loss: 0.08498 acc: 29.400%\n",
            "[1528,  30] loss: 0.08589 acc: 29.400%\n",
            "[1529,  30] loss: 0.08508 acc: 29.700%\n",
            "[1530,  30] loss: 0.08509 acc: 29.750%\n",
            "[1531,  30] loss: 0.08314 acc: 29.750%\n",
            "[1532,  30] loss: 0.08576 acc: 29.800%\n",
            "[1533,  30] loss: 0.08311 acc: 29.750%\n",
            "[1534,  30] loss: 0.08513 acc: 29.650%\n",
            "[1535,  30] loss: 0.08409 acc: 29.800%\n",
            "[1536,  30] loss: 0.08470 acc: 29.600%\n",
            "[1537,  30] loss: 0.08165 acc: 29.700%\n",
            "[1538,  30] loss: 0.08188 acc: 29.900%\n",
            "[1539,  30] loss: 0.08059 acc: 29.650%\n",
            "[1540,  30] loss: 0.08231 acc: 29.700%\n",
            "[1541,  30] loss: 0.08080 acc: 29.750%\n",
            "[1542,  30] loss: 0.08043 acc: 29.800%\n",
            "[1543,  30] loss: 0.08111 acc: 29.800%\n",
            "[1544,  30] loss: 0.08364 acc: 29.750%\n",
            "[1545,  30] loss: 0.07902 acc: 29.850%\n",
            "[1546,  30] loss: 0.07876 acc: 29.650%\n",
            "[1547,  30] loss: 0.08162 acc: 29.700%\n",
            "[1548,  30] loss: 0.08282 acc: 29.800%\n",
            "[1549,  30] loss: 0.08360 acc: 29.700%\n",
            "[1550,  30] loss: 0.08514 acc: 29.850%\n",
            "[1551,  30] loss: 0.07571 acc: 29.900%\n",
            "[1552,  30] loss: 0.08071 acc: 29.650%\n",
            "[1553,  30] loss: 0.08296 acc: 29.650%\n",
            "[1554,  30] loss: 0.08026 acc: 29.700%\n",
            "[1555,  30] loss: 0.07985 acc: 29.700%\n",
            "[1556,  30] loss: 0.07989 acc: 29.800%\n",
            "[1557,  30] loss: 0.08394 acc: 29.850%\n",
            "[1558,  30] loss: 0.08057 acc: 29.850%\n",
            "[1559,  30] loss: 0.08409 acc: 29.700%\n",
            "[1560,  30] loss: 0.07873 acc: 29.850%\n",
            "[1561,  30] loss: 0.07893 acc: 29.850%\n",
            "[1562,  30] loss: 0.07795 acc: 29.750%\n",
            "[1563,  30] loss: 0.08291 acc: 29.700%\n",
            "[1564,  30] loss: 0.08125 acc: 29.700%\n",
            "[1565,  30] loss: 0.07998 acc: 29.850%\n",
            "[1566,  30] loss: 0.08221 acc: 29.750%\n",
            "[1567,  30] loss: 0.08148 acc: 29.750%\n",
            "[1568,  30] loss: 0.08314 acc: 29.600%\n",
            "[1569,  30] loss: 0.08225 acc: 29.650%\n",
            "[1570,  30] loss: 0.08073 acc: 29.800%\n",
            "[1571,  30] loss: 0.07869 acc: 29.850%\n",
            "[1572,  30] loss: 0.07790 acc: 29.900%\n",
            "[1573,  30] loss: 0.08475 acc: 29.650%\n",
            "[1574,  30] loss: 0.08180 acc: 29.900%\n",
            "[1575,  30] loss: 0.07938 acc: 29.850%\n",
            "[1576,  30] loss: 0.08027 acc: 29.650%\n",
            "[1577,  30] loss: 0.07770 acc: 29.850%\n",
            "[1578,  30] loss: 0.08077 acc: 29.750%\n",
            "[1579,  30] loss: 0.07610 acc: 29.750%\n",
            "[1580,  30] loss: 0.07970 acc: 29.750%\n",
            "[1581,  30] loss: 0.07262 acc: 29.700%\n",
            "[1582,  30] loss: 0.07863 acc: 29.600%\n",
            "[1583,  30] loss: 0.07444 acc: 29.900%\n",
            "[1584,  30] loss: 0.07258 acc: 29.900%\n",
            "[1585,  30] loss: 0.07614 acc: 29.750%\n",
            "[1586,  30] loss: 0.07692 acc: 29.800%\n",
            "[1587,  30] loss: 0.07960 acc: 29.850%\n",
            "[1588,  30] loss: 0.07276 acc: 29.900%\n",
            "[1589,  30] loss: 0.08120 acc: 29.800%\n",
            "[1590,  30] loss: 0.08188 acc: 29.800%\n",
            "[1591,  30] loss: 0.07398 acc: 29.850%\n",
            "[1592,  30] loss: 0.07796 acc: 29.600%\n",
            "[1593,  30] loss: 0.07483 acc: 29.700%\n",
            "[1594,  30] loss: 0.07748 acc: 29.800%\n",
            "[1595,  30] loss: 0.07975 acc: 29.800%\n",
            "[1596,  30] loss: 0.07382 acc: 29.900%\n",
            "[1597,  30] loss: 0.07041 acc: 29.950%\n",
            "[1598,  30] loss: 0.07644 acc: 29.750%\n",
            "[1599,  30] loss: 0.07372 acc: 29.650%\n",
            "[1600,  30] loss: 0.08088 acc: 29.600%\n",
            "[1601,  30] loss: 0.07344 acc: 29.850%\n",
            "[1602,  30] loss: 0.07462 acc: 29.850%\n",
            "[1603,  30] loss: 0.08153 acc: 29.700%\n",
            "[1604,  30] loss: 0.07545 acc: 29.700%\n",
            "[1605,  30] loss: 0.07242 acc: 29.950%\n",
            "[1606,  30] loss: 0.07282 acc: 29.900%\n",
            "[1607,  30] loss: 0.07531 acc: 29.700%\n",
            "[1608,  30] loss: 0.07389 acc: 29.800%\n",
            "[1609,  30] loss: 0.08017 acc: 29.750%\n",
            "[1610,  30] loss: 0.07351 acc: 29.600%\n",
            "[1611,  30] loss: 0.07549 acc: 29.800%\n",
            "[1612,  30] loss: 0.07167 acc: 29.850%\n",
            "[1613,  30] loss: 0.07118 acc: 29.950%\n",
            "[1614,  30] loss: 0.07353 acc: 29.800%\n",
            "[1615,  30] loss: 0.07711 acc: 29.700%\n",
            "[1616,  30] loss: 0.07732 acc: 29.700%\n",
            "[1617,  30] loss: 0.07107 acc: 29.850%\n",
            "[1618,  30] loss: 0.07346 acc: 29.700%\n",
            "[1619,  30] loss: 0.07314 acc: 29.600%\n",
            "[1620,  30] loss: 0.07235 acc: 29.900%\n",
            "[1621,  30] loss: 0.07118 acc: 29.750%\n",
            "[1622,  30] loss: 0.07177 acc: 29.850%\n",
            "[1623,  30] loss: 0.07108 acc: 29.850%\n",
            "[1624,  30] loss: 0.07443 acc: 29.700%\n",
            "[1625,  30] loss: 0.07325 acc: 29.850%\n",
            "[1626,  30] loss: 0.07283 acc: 29.800%\n",
            "[1627,  30] loss: 0.07104 acc: 29.900%\n",
            "[1628,  30] loss: 0.07435 acc: 29.700%\n",
            "[1629,  30] loss: 0.07571 acc: 29.650%\n",
            "[1630,  30] loss: 0.07136 acc: 29.850%\n",
            "[1631,  30] loss: 0.07445 acc: 29.900%\n",
            "[1632,  30] loss: 0.07579 acc: 29.500%\n",
            "[1633,  30] loss: 0.06775 acc: 29.800%\n",
            "[1634,  30] loss: 0.07671 acc: 29.600%\n",
            "[1635,  30] loss: 0.07171 acc: 29.650%\n",
            "[1636,  30] loss: 0.07298 acc: 29.650%\n",
            "[1637,  30] loss: 0.07071 acc: 29.900%\n",
            "[1638,  30] loss: 0.07185 acc: 29.800%\n",
            "[1639,  30] loss: 0.07218 acc: 29.750%\n",
            "[1640,  30] loss: 0.07405 acc: 29.750%\n",
            "[1641,  30] loss: 0.07267 acc: 29.650%\n",
            "[1642,  30] loss: 0.06834 acc: 29.900%\n",
            "[1643,  30] loss: 0.07389 acc: 29.850%\n",
            "[1644,  30] loss: 0.06976 acc: 29.850%\n",
            "[1645,  30] loss: 0.06804 acc: 29.850%\n",
            "[1646,  30] loss: 0.07141 acc: 29.750%\n",
            "[1647,  30] loss: 0.07005 acc: 29.800%\n",
            "[1648,  30] loss: 0.07049 acc: 29.850%\n",
            "[1649,  30] loss: 0.06909 acc: 29.900%\n",
            "[1650,  30] loss: 0.07049 acc: 29.800%\n",
            "[1651,  30] loss: 0.07133 acc: 29.700%\n",
            "[1652,  30] loss: 0.06908 acc: 29.600%\n",
            "[1653,  30] loss: 0.06922 acc: 29.800%\n",
            "[1654,  30] loss: 0.06655 acc: 29.800%\n",
            "[1655,  30] loss: 0.07072 acc: 29.900%\n",
            "[1656,  30] loss: 0.07031 acc: 29.750%\n",
            "[1657,  30] loss: 0.07128 acc: 29.850%\n",
            "[1658,  30] loss: 0.07064 acc: 29.750%\n",
            "[1659,  30] loss: 0.06966 acc: 29.950%\n",
            "[1660,  30] loss: 0.07095 acc: 29.800%\n",
            "[1661,  30] loss: 0.06965 acc: 29.700%\n",
            "[1662,  30] loss: 0.06714 acc: 29.800%\n",
            "[1663,  30] loss: 0.06647 acc: 29.800%\n",
            "[1664,  30] loss: 0.07104 acc: 29.700%\n",
            "[1665,  30] loss: 0.06960 acc: 29.700%\n",
            "[1666,  30] loss: 0.06711 acc: 29.900%\n",
            "[1667,  30] loss: 0.06911 acc: 29.650%\n",
            "[1668,  30] loss: 0.07001 acc: 29.850%\n",
            "[1669,  30] loss: 0.07123 acc: 29.650%\n",
            "[1670,  30] loss: 0.06586 acc: 29.850%\n",
            "[1671,  30] loss: 0.07059 acc: 29.750%\n",
            "[1672,  30] loss: 0.06558 acc: 29.900%\n",
            "[1673,  30] loss: 0.06475 acc: 29.800%\n",
            "[1674,  30] loss: 0.06476 acc: 30.000%\n",
            "[1675,  30] loss: 0.07107 acc: 29.950%\n",
            "[1676,  30] loss: 0.06443 acc: 29.800%\n",
            "[1677,  30] loss: 0.06797 acc: 29.900%\n",
            "[1678,  30] loss: 0.06840 acc: 29.850%\n",
            "[1679,  30] loss: 0.06588 acc: 29.900%\n",
            "[1680,  30] loss: 0.06795 acc: 29.800%\n",
            "[1681,  30] loss: 0.06761 acc: 29.800%\n",
            "[1682,  30] loss: 0.06915 acc: 29.900%\n",
            "[1683,  30] loss: 0.06772 acc: 29.850%\n",
            "[1684,  30] loss: 0.06539 acc: 29.900%\n",
            "[1685,  30] loss: 0.06696 acc: 29.750%\n",
            "[1686,  30] loss: 0.06978 acc: 29.700%\n",
            "[1687,  30] loss: 0.06419 acc: 29.850%\n",
            "[1688,  30] loss: 0.06757 acc: 29.700%\n",
            "[1689,  30] loss: 0.06391 acc: 29.800%\n",
            "[1690,  30] loss: 0.06637 acc: 29.800%\n",
            "[1691,  30] loss: 0.06771 acc: 29.750%\n",
            "[1692,  30] loss: 0.06449 acc: 29.850%\n",
            "[1693,  30] loss: 0.06825 acc: 29.750%\n",
            "[1694,  30] loss: 0.06771 acc: 29.700%\n",
            "[1695,  30] loss: 0.06426 acc: 29.850%\n",
            "[1696,  30] loss: 0.06826 acc: 29.650%\n",
            "[1697,  30] loss: 0.06442 acc: 29.800%\n",
            "[1698,  30] loss: 0.06443 acc: 29.800%\n",
            "[1699,  30] loss: 0.06275 acc: 29.900%\n",
            "[1700,  30] loss: 0.06624 acc: 29.900%\n",
            "[1701,  30] loss: 0.06313 acc: 29.950%\n",
            "[1702,  30] loss: 0.06609 acc: 29.750%\n",
            "[1703,  30] loss: 0.06221 acc: 29.900%\n",
            "[1704,  30] loss: 0.06469 acc: 29.800%\n",
            "[1705,  30] loss: 0.06614 acc: 29.850%\n",
            "[1706,  30] loss: 0.06286 acc: 29.950%\n",
            "[1707,  30] loss: 0.06669 acc: 29.950%\n",
            "[1708,  30] loss: 0.06380 acc: 29.800%\n",
            "[1709,  30] loss: 0.06360 acc: 29.850%\n",
            "[1710,  30] loss: 0.06380 acc: 29.900%\n",
            "[1711,  30] loss: 0.06275 acc: 29.850%\n",
            "[1712,  30] loss: 0.06409 acc: 29.800%\n",
            "[1713,  30] loss: 0.06704 acc: 29.800%\n",
            "[1714,  30] loss: 0.06743 acc: 29.650%\n",
            "[1715,  30] loss: 0.06770 acc: 29.650%\n",
            "[1716,  30] loss: 0.06376 acc: 29.850%\n",
            "[1717,  30] loss: 0.06194 acc: 29.900%\n",
            "[1718,  30] loss: 0.06463 acc: 29.900%\n",
            "[1719,  30] loss: 0.06773 acc: 29.600%\n",
            "[1720,  30] loss: 0.06316 acc: 29.850%\n",
            "[1721,  30] loss: 0.06538 acc: 29.750%\n",
            "[1722,  30] loss: 0.06437 acc: 29.800%\n",
            "[1723,  30] loss: 0.06498 acc: 29.650%\n",
            "[1724,  30] loss: 0.06419 acc: 29.850%\n",
            "[1725,  30] loss: 0.06594 acc: 29.750%\n",
            "[1726,  30] loss: 0.06232 acc: 29.850%\n",
            "[1727,  30] loss: 0.06424 acc: 29.850%\n",
            "[1728,  30] loss: 0.06403 acc: 29.850%\n",
            "[1729,  30] loss: 0.06431 acc: 29.850%\n",
            "[1730,  30] loss: 0.06282 acc: 29.900%\n",
            "[1731,  30] loss: 0.05785 acc: 29.900%\n",
            "[1732,  30] loss: 0.06133 acc: 29.800%\n",
            "[1733,  30] loss: 0.06286 acc: 29.850%\n",
            "[1734,  30] loss: 0.06557 acc: 29.850%\n",
            "[1735,  30] loss: 0.05936 acc: 29.900%\n",
            "[1736,  30] loss: 0.06136 acc: 29.950%\n",
            "[1737,  30] loss: 0.06134 acc: 29.850%\n",
            "[1738,  30] loss: 0.06067 acc: 29.900%\n",
            "[1739,  30] loss: 0.05990 acc: 29.850%\n",
            "[1740,  30] loss: 0.05819 acc: 29.850%\n",
            "[1741,  30] loss: 0.06457 acc: 29.700%\n",
            "[1742,  30] loss: 0.06256 acc: 29.800%\n",
            "[1743,  30] loss: 0.06016 acc: 29.900%\n",
            "[1744,  30] loss: 0.06441 acc: 29.800%\n",
            "[1745,  30] loss: 0.05686 acc: 29.850%\n",
            "[1746,  30] loss: 0.06506 acc: 29.750%\n",
            "[1747,  30] loss: 0.05847 acc: 29.850%\n",
            "[1748,  30] loss: 0.05826 acc: 29.850%\n",
            "[1749,  30] loss: 0.06264 acc: 29.850%\n",
            "[1750,  30] loss: 0.06164 acc: 29.850%\n",
            "[1751,  30] loss: 0.06573 acc: 29.600%\n",
            "[1752,  30] loss: 0.06067 acc: 29.900%\n",
            "[1753,  30] loss: 0.06190 acc: 29.950%\n",
            "[1754,  30] loss: 0.05907 acc: 29.900%\n",
            "[1755,  30] loss: 0.05751 acc: 29.800%\n",
            "[1756,  30] loss: 0.06411 acc: 29.800%\n",
            "[1757,  30] loss: 0.05930 acc: 29.850%\n",
            "[1758,  30] loss: 0.06250 acc: 29.750%\n",
            "[1759,  30] loss: 0.06058 acc: 29.850%\n",
            "[1760,  30] loss: 0.06187 acc: 29.900%\n",
            "[1761,  30] loss: 0.06007 acc: 29.950%\n",
            "[1762,  30] loss: 0.05944 acc: 29.800%\n",
            "[1763,  30] loss: 0.06039 acc: 29.850%\n",
            "[1764,  30] loss: 0.06182 acc: 29.850%\n",
            "[1765,  30] loss: 0.05914 acc: 29.850%\n",
            "[1766,  30] loss: 0.06059 acc: 29.850%\n",
            "[1767,  30] loss: 0.06119 acc: 29.850%\n",
            "[1768,  30] loss: 0.06363 acc: 29.900%\n",
            "[1769,  30] loss: 0.05964 acc: 29.800%\n",
            "[1770,  30] loss: 0.06070 acc: 29.850%\n",
            "[1771,  30] loss: 0.06179 acc: 29.850%\n",
            "[1772,  30] loss: 0.05939 acc: 29.950%\n",
            "[1773,  30] loss: 0.06111 acc: 29.800%\n",
            "[1774,  30] loss: 0.06194 acc: 29.850%\n",
            "[1775,  30] loss: 0.05822 acc: 29.850%\n",
            "[1776,  30] loss: 0.06107 acc: 29.950%\n",
            "[1777,  30] loss: 0.06038 acc: 29.850%\n",
            "[1778,  30] loss: 0.05642 acc: 29.950%\n",
            "[1779,  30] loss: 0.05936 acc: 29.750%\n",
            "[1780,  30] loss: 0.05904 acc: 29.800%\n",
            "[1781,  30] loss: 0.05657 acc: 29.900%\n",
            "[1782,  30] loss: 0.05980 acc: 29.850%\n",
            "[1783,  30] loss: 0.05747 acc: 29.850%\n",
            "[1784,  30] loss: 0.05883 acc: 29.750%\n",
            "[1785,  30] loss: 0.05963 acc: 29.800%\n",
            "[1786,  30] loss: 0.05723 acc: 29.900%\n",
            "[1787,  30] loss: 0.06068 acc: 29.850%\n",
            "[1788,  30] loss: 0.05753 acc: 29.950%\n",
            "[1789,  30] loss: 0.05673 acc: 29.850%\n",
            "[1790,  30] loss: 0.05922 acc: 29.500%\n",
            "[1791,  30] loss: 0.06008 acc: 29.750%\n",
            "[1792,  30] loss: 0.06022 acc: 29.850%\n",
            "[1793,  30] loss: 0.05819 acc: 29.850%\n",
            "[1794,  30] loss: 0.05823 acc: 29.900%\n",
            "[1795,  30] loss: 0.05868 acc: 29.850%\n",
            "[1796,  30] loss: 0.05618 acc: 29.950%\n",
            "[1797,  30] loss: 0.05680 acc: 29.950%\n",
            "[1798,  30] loss: 0.05800 acc: 30.000%\n",
            "[1799,  30] loss: 0.05696 acc: 29.900%\n",
            "[1800,  30] loss: 0.05907 acc: 29.900%\n",
            "[1801,  30] loss: 0.05801 acc: 29.850%\n",
            "[1802,  30] loss: 0.05550 acc: 30.000%\n",
            "[1803,  30] loss: 0.05618 acc: 29.850%\n",
            "[1804,  30] loss: 0.05613 acc: 29.900%\n",
            "[1805,  30] loss: 0.05773 acc: 29.850%\n",
            "[1806,  30] loss: 0.05584 acc: 29.850%\n",
            "[1807,  30] loss: 0.05783 acc: 29.900%\n",
            "[1808,  30] loss: 0.05801 acc: 29.800%\n",
            "[1809,  30] loss: 0.05642 acc: 29.900%\n",
            "[1810,  30] loss: 0.05553 acc: 29.900%\n",
            "[1811,  30] loss: 0.05518 acc: 30.000%\n",
            "[1812,  30] loss: 0.05378 acc: 29.900%\n",
            "[1813,  30] loss: 0.05897 acc: 29.750%\n",
            "[1814,  30] loss: 0.05804 acc: 29.700%\n",
            "[1815,  30] loss: 0.05310 acc: 30.000%\n",
            "[1816,  30] loss: 0.05852 acc: 29.800%\n",
            "[1817,  30] loss: 0.05692 acc: 30.000%\n",
            "[1818,  30] loss: 0.05281 acc: 30.000%\n",
            "[1819,  30] loss: 0.05724 acc: 29.850%\n",
            "[1820,  30] loss: 0.05547 acc: 29.900%\n",
            "[1821,  30] loss: 0.05974 acc: 29.800%\n",
            "[1822,  30] loss: 0.05893 acc: 29.850%\n",
            "[1823,  30] loss: 0.05768 acc: 29.850%\n",
            "[1824,  30] loss: 0.05849 acc: 29.800%\n",
            "[1825,  30] loss: 0.05301 acc: 29.850%\n",
            "[1826,  30] loss: 0.05816 acc: 29.800%\n",
            "[1827,  30] loss: 0.05624 acc: 29.800%\n",
            "[1828,  30] loss: 0.05340 acc: 29.900%\n",
            "[1829,  30] loss: 0.05349 acc: 29.900%\n",
            "[1830,  30] loss: 0.05446 acc: 29.750%\n",
            "[1831,  30] loss: 0.05503 acc: 29.850%\n",
            "[1832,  30] loss: 0.05327 acc: 29.900%\n",
            "[1833,  30] loss: 0.05177 acc: 29.950%\n",
            "[1834,  30] loss: 0.05415 acc: 29.850%\n",
            "[1835,  30] loss: 0.05523 acc: 29.800%\n",
            "[1836,  30] loss: 0.05574 acc: 29.800%\n",
            "[1837,  30] loss: 0.05464 acc: 29.900%\n",
            "[1838,  30] loss: 0.05590 acc: 29.700%\n",
            "[1839,  30] loss: 0.05554 acc: 29.750%\n",
            "[1840,  30] loss: 0.05106 acc: 29.900%\n",
            "[1841,  30] loss: 0.05354 acc: 29.900%\n",
            "[1842,  30] loss: 0.05282 acc: 29.850%\n",
            "[1843,  30] loss: 0.05883 acc: 29.750%\n",
            "[1844,  30] loss: 0.05361 acc: 29.850%\n",
            "[1845,  30] loss: 0.05619 acc: 29.700%\n",
            "[1846,  30] loss: 0.05344 acc: 29.800%\n",
            "[1847,  30] loss: 0.05176 acc: 29.900%\n",
            "[1848,  30] loss: 0.05441 acc: 29.850%\n",
            "[1849,  30] loss: 0.05661 acc: 29.850%\n",
            "[1850,  30] loss: 0.05410 acc: 29.850%\n",
            "[1851,  30] loss: 0.05550 acc: 29.950%\n",
            "[1852,  30] loss: 0.05547 acc: 29.850%\n",
            "[1853,  30] loss: 0.05297 acc: 29.850%\n",
            "[1854,  30] loss: 0.05509 acc: 29.850%\n",
            "[1855,  30] loss: 0.05257 acc: 29.950%\n",
            "[1856,  30] loss: 0.05047 acc: 29.800%\n",
            "[1857,  30] loss: 0.05550 acc: 29.900%\n",
            "[1858,  30] loss: 0.05542 acc: 29.800%\n",
            "[1859,  30] loss: 0.05475 acc: 29.900%\n",
            "[1860,  30] loss: 0.05336 acc: 29.950%\n",
            "[1861,  30] loss: 0.05208 acc: 29.800%\n",
            "[1862,  30] loss: 0.05170 acc: 29.900%\n",
            "[1863,  30] loss: 0.05554 acc: 29.850%\n",
            "[1864,  30] loss: 0.05662 acc: 29.850%\n",
            "[1865,  30] loss: 0.05312 acc: 29.850%\n",
            "[1866,  30] loss: 0.05370 acc: 29.900%\n",
            "[1867,  30] loss: 0.05385 acc: 29.900%\n",
            "[1868,  30] loss: 0.05105 acc: 29.950%\n",
            "[1869,  30] loss: 0.05250 acc: 29.850%\n",
            "[1870,  30] loss: 0.05241 acc: 29.800%\n",
            "[1871,  30] loss: 0.05493 acc: 29.700%\n",
            "[1872,  30] loss: 0.05386 acc: 29.850%\n",
            "[1873,  30] loss: 0.05347 acc: 29.900%\n",
            "[1874,  30] loss: 0.04956 acc: 29.900%\n",
            "[1875,  30] loss: 0.05197 acc: 29.900%\n",
            "[1876,  30] loss: 0.04941 acc: 30.000%\n",
            "[1877,  30] loss: 0.05099 acc: 29.850%\n",
            "[1878,  30] loss: 0.05204 acc: 29.900%\n",
            "[1879,  30] loss: 0.05176 acc: 30.000%\n",
            "[1880,  30] loss: 0.05225 acc: 29.700%\n",
            "[1881,  30] loss: 0.05381 acc: 29.800%\n",
            "[1882,  30] loss: 0.05223 acc: 29.850%\n",
            "[1883,  30] loss: 0.04996 acc: 29.800%\n",
            "[1884,  30] loss: 0.05132 acc: 29.750%\n",
            "[1885,  30] loss: 0.05329 acc: 29.850%\n",
            "[1886,  30] loss: 0.05307 acc: 29.850%\n",
            "[1887,  30] loss: 0.05081 acc: 29.800%\n",
            "[1888,  30] loss: 0.05401 acc: 29.800%\n",
            "[1889,  30] loss: 0.05246 acc: 29.750%\n",
            "[1890,  30] loss: 0.05068 acc: 29.950%\n",
            "[1891,  30] loss: 0.05259 acc: 29.900%\n",
            "[1892,  30] loss: 0.04726 acc: 29.950%\n",
            "[1893,  30] loss: 0.04974 acc: 30.000%\n",
            "[1894,  30] loss: 0.04883 acc: 29.950%\n",
            "[1895,  30] loss: 0.05637 acc: 29.900%\n",
            "[1896,  30] loss: 0.04938 acc: 29.900%\n",
            "[1897,  30] loss: 0.05174 acc: 29.900%\n",
            "[1898,  30] loss: 0.05171 acc: 30.000%\n",
            "[1899,  30] loss: 0.05245 acc: 29.800%\n",
            "[1900,  30] loss: 0.05258 acc: 29.950%\n",
            "[1901,  30] loss: 0.04843 acc: 29.850%\n",
            "[1902,  30] loss: 0.04914 acc: 29.950%\n",
            "[1903,  30] loss: 0.05362 acc: 29.900%\n",
            "[1904,  30] loss: 0.04981 acc: 30.000%\n",
            "[1905,  30] loss: 0.05233 acc: 29.850%\n",
            "[1906,  30] loss: 0.05063 acc: 29.850%\n",
            "[1907,  30] loss: 0.04890 acc: 29.900%\n",
            "[1908,  30] loss: 0.05142 acc: 29.850%\n",
            "[1909,  30] loss: 0.04866 acc: 29.900%\n",
            "[1910,  30] loss: 0.05065 acc: 29.850%\n",
            "[1911,  30] loss: 0.05009 acc: 29.900%\n",
            "[1912,  30] loss: 0.04872 acc: 30.000%\n",
            "[1913,  30] loss: 0.05068 acc: 29.900%\n",
            "[1914,  30] loss: 0.05213 acc: 29.900%\n",
            "[1915,  30] loss: 0.04637 acc: 29.850%\n",
            "[1916,  30] loss: 0.05089 acc: 29.750%\n",
            "[1917,  30] loss: 0.05054 acc: 29.950%\n",
            "[1918,  30] loss: 0.04852 acc: 29.950%\n",
            "[1919,  30] loss: 0.04896 acc: 29.950%\n",
            "[1920,  30] loss: 0.05079 acc: 29.750%\n",
            "[1921,  30] loss: 0.05010 acc: 29.850%\n",
            "[1922,  30] loss: 0.04902 acc: 29.850%\n",
            "[1923,  30] loss: 0.04812 acc: 29.950%\n",
            "[1924,  30] loss: 0.04757 acc: 29.950%\n",
            "[1925,  30] loss: 0.04898 acc: 29.800%\n",
            "[1926,  30] loss: 0.04816 acc: 29.950%\n",
            "[1927,  30] loss: 0.05162 acc: 29.850%\n",
            "[1928,  30] loss: 0.04571 acc: 29.900%\n",
            "[1929,  30] loss: 0.04934 acc: 29.850%\n",
            "[1930,  30] loss: 0.04852 acc: 29.900%\n",
            "[1931,  30] loss: 0.05080 acc: 29.900%\n",
            "[1932,  30] loss: 0.05065 acc: 29.950%\n",
            "[1933,  30] loss: 0.05017 acc: 29.850%\n",
            "[1934,  30] loss: 0.04868 acc: 29.850%\n",
            "[1935,  30] loss: 0.04812 acc: 29.850%\n",
            "[1936,  30] loss: 0.04876 acc: 29.850%\n",
            "[1937,  30] loss: 0.04519 acc: 29.900%\n",
            "[1938,  30] loss: 0.04500 acc: 29.950%\n",
            "[1939,  30] loss: 0.04582 acc: 29.900%\n",
            "[1940,  30] loss: 0.04528 acc: 29.850%\n",
            "[1941,  30] loss: 0.04856 acc: 29.950%\n",
            "[1942,  30] loss: 0.04503 acc: 29.850%\n",
            "[1943,  30] loss: 0.04990 acc: 29.750%\n",
            "[1944,  30] loss: 0.04886 acc: 29.900%\n",
            "[1945,  30] loss: 0.04682 acc: 29.900%\n",
            "[1946,  30] loss: 0.04665 acc: 29.800%\n",
            "[1947,  30] loss: 0.04885 acc: 29.900%\n",
            "[1948,  30] loss: 0.04594 acc: 29.950%\n",
            "[1949,  30] loss: 0.04443 acc: 29.900%\n",
            "[1950,  30] loss: 0.04610 acc: 29.950%\n",
            "[1951,  30] loss: 0.04915 acc: 29.900%\n",
            "[1952,  30] loss: 0.04580 acc: 30.000%\n",
            "[1953,  30] loss: 0.04620 acc: 29.900%\n",
            "[1954,  30] loss: 0.04896 acc: 29.800%\n",
            "[1955,  30] loss: 0.04650 acc: 29.900%\n",
            "[1956,  30] loss: 0.04786 acc: 29.900%\n",
            "[1957,  30] loss: 0.04882 acc: 29.850%\n",
            "[1958,  30] loss: 0.04685 acc: 29.800%\n",
            "[1959,  30] loss: 0.04495 acc: 29.850%\n",
            "[1960,  30] loss: 0.04689 acc: 29.900%\n",
            "[1961,  30] loss: 0.04587 acc: 29.850%\n",
            "[1962,  30] loss: 0.04593 acc: 29.900%\n",
            "[1963,  30] loss: 0.04670 acc: 29.850%\n",
            "[1964,  30] loss: 0.04547 acc: 29.950%\n",
            "[1965,  30] loss: 0.04658 acc: 29.850%\n",
            "[1966,  30] loss: 0.04748 acc: 29.950%\n",
            "[1967,  30] loss: 0.04877 acc: 29.900%\n",
            "[1968,  30] loss: 0.04610 acc: 29.900%\n",
            "[1969,  30] loss: 0.04898 acc: 29.950%\n",
            "[1970,  30] loss: 0.04274 acc: 29.900%\n",
            "[1971,  30] loss: 0.04465 acc: 29.950%\n",
            "[1972,  30] loss: 0.04644 acc: 29.850%\n",
            "[1973,  30] loss: 0.04562 acc: 30.000%\n",
            "[1974,  30] loss: 0.04641 acc: 29.800%\n",
            "[1975,  30] loss: 0.04669 acc: 29.950%\n",
            "[1976,  30] loss: 0.04528 acc: 29.950%\n",
            "[1977,  30] loss: 0.04464 acc: 29.950%\n",
            "[1978,  30] loss: 0.04434 acc: 30.000%\n",
            "[1979,  30] loss: 0.04674 acc: 29.800%\n",
            "[1980,  30] loss: 0.04452 acc: 29.900%\n",
            "[1981,  30] loss: 0.04784 acc: 29.800%\n",
            "[1982,  30] loss: 0.04822 acc: 29.900%\n",
            "[1983,  30] loss: 0.04840 acc: 29.850%\n",
            "[1984,  30] loss: 0.04657 acc: 29.950%\n",
            "[1985,  30] loss: 0.04562 acc: 30.000%\n",
            "[1986,  30] loss: 0.04649 acc: 29.900%\n",
            "[1987,  30] loss: 0.04705 acc: 29.800%\n",
            "[1988,  30] loss: 0.04738 acc: 29.800%\n",
            "[1989,  30] loss: 0.04437 acc: 29.950%\n",
            "[1990,  30] loss: 0.04400 acc: 29.950%\n",
            "[1991,  30] loss: 0.05003 acc: 29.800%\n",
            "[1992,  30] loss: 0.04689 acc: 29.950%\n",
            "[1993,  30] loss: 0.04471 acc: 29.850%\n",
            "[1994,  30] loss: 0.04406 acc: 29.800%\n",
            "[1995,  30] loss: 0.04504 acc: 29.900%\n",
            "[1996,  30] loss: 0.04550 acc: 30.000%\n",
            "[1997,  30] loss: 0.04507 acc: 29.850%\n",
            "[1998,  30] loss: 0.04712 acc: 29.900%\n",
            "[1999,  30] loss: 0.04490 acc: 29.900%\n",
            "[2000,  30] loss: 0.04380 acc: 29.950%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwbcEr_AsSd3"
      },
      "source": [
        "# dataset class\n",
        "class captchaCornerDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # corner detection\n",
        "      image = np.float32(self.images[index])\n",
        "      image = image * cv2.cornerHarris(image, 2, 3, 0.04)\n",
        "      # turn all chars of label into numbers\n",
        "      chars = []\n",
        "      for i in range(len(self.labels[index])):\n",
        "        char = self.classToNum[self.labels[index][i]]\n",
        "        chars.append(char)\n",
        "      # don't forget end of sentence \n",
        "      end = self.classToNum[\"<EOS>\"]\n",
        "      chars.append(end)\n",
        "      # turn it all into a tensor\n",
        "      label = tensor(chars)\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "\n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln0okIcex2rb"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = captchaCornerDataset(X=capt_train_data, classToNum=invertedCaptDict)\n",
        "capt_corner_train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "captchaCornerModel = captchaLSTM(37, 10, 5, 1).to(device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "mW2Vdro4yAS0",
        "outputId": "714aeb4d-e4dc-42db-96a1-7d42a012eacb"
      },
      "source": [
        "testItem, testLabel = next(iter(capt_corner_train_dl))\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = testLabel.numpy()[0]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "h1 = captchaCornerModel.init_hidden()\n",
        "\n",
        "# predict twice with same image but different hidden\n",
        "output, h1 = captchaCornerModel(testItem.to(device), h1)\n",
        "output2, h1 = captchaCornerModel(testItem.to(device), h1)\n",
        "output3, h1 = captchaCornerModel(testItem.to(device), h1)\n",
        "output4, h1 = captchaCornerModel(testItem.to(device), h1)\n",
        "output5, h1 = captchaCornerModel(testItem.to(device), h1)\n",
        "\n",
        "pred1 = captLabelDict[output[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred2 = captLabelDict[output2[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred3 = captLabelDict[output3[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred4 = captLabelDict[output4[0][0].detach().cpu().numpy().argmax(0)]\n",
        "pred5 = captLabelDict[output5[0][0].detach().cpu().numpy().argmax(0)]\n",
        "\n",
        "print(\"predicted\", pred1 + pred2 + pred3 + pred4 + pred5, \"for\", \"\".join(toChars(testLabel, captLabelDict)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([10, 1, 50, 200])\n",
            "Labels batch shape: torch.Size([10, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAAyCAAAAAA8Oss9AAAEkklEQVR4nO1aO5IkNRB9r6MtLrIYXAGXiHW5EofaYF1cjgOL+TCk/JaqpndaTWBMRkx3KVUq5fdlqnoogDASAFBzxMymMqeQCK1n3iadPfR76bblKf8DutfhtLvcK4ObPlekMwZtsMfqV3QjKUmANDe1P0gYU+NLJt5k2vVjG+m4/JMgcpOO7pEkj9vzr6EPW4JQ1cJl/Bn4MjiDqzQ9MmmMBf70Te9PrhWZRybBroFfEZzDGlz6Q/nZ6ZFpz29/P+zPt6klQ08ReoSLA2AUgW9X2aifgS9IbCFlSgNCPIF214rQ0FBxrSqI6zUHK1Sec8wKtcuu1/N0L/poflD+NbxxsvrIN4+40JSvTndvi6gQpXpk5GbmlmiaA3eYbFanxpU9AyVQbfNtPrmb1IYlS0lMpflJTYkYcg27e4ocif3Ze2vLiB5RYaeycQT4Ib7loXdZ89xA7DdurZRJXHXumFEVwOGmZLrFyKlHFrm/UxEVRQgD3oSwJVsCbUTLliKLjnavigDAL//gj/2KONYy6+VK5cimOW3OLxSJJxv3Kp/35XokuMdxynxTLGkXw0GeJXCPJnWxsPpX/IYfft8kf6JDdlg98+T3gHOewVxFzwPgMQKx3vWSVvje97fKqMoYIEUlTllk7eCCChq8rJ/37pcRWd5sWZIQ1GyEZSE37W1eEnn0a0heNMzxuU2xGwBwpryJq6uIcb45wUUe5ZEAJJXRLmkvKDkiMXtnMckhIF05gCGDAwtu7W1GLhRJlwHHCAWjo4pG66I3bxT5bQXkFZQ6k3QYKWDsEMzZJ8rtbY2jy7w8YbzcG0DqZ5nEilJnaZ2CL0su5gB6K35+TgVEu4MtzoJqAQN4IPnxsFs319LvQ6B1FD5BN4BTFinqh7/cICBp2FtKemTZM1RIEPL5fI1ZP27UAYBmP9WPVy5dbRSR3DKThBaGFqWy2oneN/e9dzpEuMHdkJM2ypWsQBRnyFxnK+P9FAkOp1zvTc4H7aF7tImwMi4XKvAJWtZGQhBCiWGUByvIxjpJ3ORyROlQSD1H5LS6ZElVVLOcSG7yPf4D7EV4JPaVZmfldQVwM0ei5DQXPU9yfX1Lg3WP+U7yfGxVsdyQ07ygcDlhToqOf0mvqo73aVUHy9Z6yHKHWluQLZIsJE/E1fnUc3QPbJ2HjSjkLvp4X5f3Z6iaQiQ77jHa5x//oYecaTlqX+rCWMKnfgHe9Tfhp5+7SvTs0nHNMxS/WCm2KcHm8xJPIUheT0zcsjCNAiJ3pnpSRPQmpTnA5fA7PvWnyI4l3kk/BrobsyVQq+CwY9J4d9LSfHnkQLwO7pPtZJaOZvvoBmBETAoAWtcn/Am/IeKmx0Uqg1nnQ/sx1w3o2Ixd6WCVeEq4JNZqMcc529E6Ruc2q/uNezv4okgpE6vXi+k01RuoEOyQv01e5V8X99IN0ThZ53HdOhwn2QSmM7u8ZA2rje9XVqE1+OlF8LEFOS44/rzCPuurU/Rtc015eRusFCrv2uqxhT1on6H2nw+psX12jwfWEgdkfzd1RTbR4xbYW98/6IM+6Iz+Bf68vI/5ClBzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=200x50 at 0x7F5B6232EA10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted vsnhb for 5e1d<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT_knxzvybi-",
        "outputId": "311e3064-a96c-48a9-d5f0-2752209ca4f3"
      },
      "source": [
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(captchaCornerModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 1000\n",
        "max_len = 2\n",
        "batch_size = 10\n",
        "\n",
        "#captchaCornerTestLoss = []\n",
        "#captchaCornerTestAcc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(capt_corner_train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.permute(1,0).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    h1 = captchaCornerModel.init_hidden()\n",
        "\n",
        "    outputs = []\n",
        "    losses = []\n",
        "    for j in range(max_len):\n",
        "      captchaCornerModel.zero_grad()\n",
        "\n",
        "      #print(images.shape)\n",
        "      #print(hidden[0].shape)\n",
        "\n",
        "      output, h1 = captchaCornerModel(images, h1)\n",
        "      h1 = (h1[0].detach(), h1[1].detach())\n",
        "\n",
        "      #print(output[0].shape)\n",
        "      #print(labels[j])\n",
        "\n",
        "      loss = lossFunc(output[0], labels[j])\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      outputs.append(output[0])\n",
        "      losses.append(loss)\n",
        "\n",
        "    outputs = torch.stack(outputs)\n",
        "    running_acc += findAccuracy(outputs, labels)\n",
        "\n",
        "    #print(loss)\n",
        "    running_loss += sum(losses).item()\n",
        "    # if last batch of epoch\n",
        "    if i == 29 :\n",
        "      captchaCornerTestLoss.append(running_loss)\n",
        "      captchaCornerTestAcc.append(running_acc/10)\n",
        "      print(\"[%d, %3d] loss: %.5f acc: %.3f%%\" % (epoch + 1, i + 1, running_loss / 100, running_acc))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  30] loss: 0.23062 acc: 26.950%\n",
            "[2,  30] loss: 0.22118 acc: 27.450%\n",
            "[3,  30] loss: 0.22321 acc: 27.600%\n",
            "[4,  30] loss: 0.22269 acc: 27.300%\n",
            "[5,  30] loss: 0.22800 acc: 27.000%\n",
            "[6,  30] loss: 0.22230 acc: 27.400%\n",
            "[7,  30] loss: 0.22030 acc: 27.300%\n",
            "[8,  30] loss: 0.21852 acc: 27.650%\n",
            "[9,  30] loss: 0.22937 acc: 27.250%\n",
            "[10,  30] loss: 0.22877 acc: 27.100%\n",
            "[11,  30] loss: 0.22649 acc: 27.300%\n",
            "[12,  30] loss: 0.21782 acc: 27.650%\n",
            "[13,  30] loss: 0.22353 acc: 27.650%\n",
            "[14,  30] loss: 0.22578 acc: 26.950%\n",
            "[15,  30] loss: 0.22217 acc: 27.300%\n",
            "[16,  30] loss: 0.21766 acc: 27.600%\n",
            "[17,  30] loss: 0.21574 acc: 27.750%\n",
            "[18,  30] loss: 0.21742 acc: 27.250%\n",
            "[19,  30] loss: 0.21481 acc: 27.400%\n",
            "[20,  30] loss: 0.22159 acc: 27.600%\n",
            "[21,  30] loss: 0.21390 acc: 27.600%\n",
            "[22,  30] loss: 0.22260 acc: 27.200%\n",
            "[23,  30] loss: 0.21651 acc: 27.200%\n",
            "[24,  30] loss: 0.21620 acc: 27.950%\n",
            "[25,  30] loss: 0.21346 acc: 27.600%\n",
            "[26,  30] loss: 0.21504 acc: 27.750%\n",
            "[27,  30] loss: 0.21324 acc: 27.400%\n",
            "[28,  30] loss: 0.21596 acc: 27.200%\n",
            "[29,  30] loss: 0.21970 acc: 27.400%\n",
            "[30,  30] loss: 0.21879 acc: 27.650%\n",
            "[31,  30] loss: 0.20918 acc: 27.650%\n",
            "[32,  30] loss: 0.21710 acc: 27.600%\n",
            "[33,  30] loss: 0.21816 acc: 27.500%\n",
            "[34,  30] loss: 0.21724 acc: 27.650%\n",
            "[35,  30] loss: 0.21893 acc: 27.500%\n",
            "[36,  30] loss: 0.21493 acc: 27.400%\n",
            "[37,  30] loss: 0.20946 acc: 27.800%\n",
            "[38,  30] loss: 0.21634 acc: 27.150%\n",
            "[39,  30] loss: 0.21217 acc: 27.550%\n",
            "[40,  30] loss: 0.21005 acc: 27.700%\n",
            "[41,  30] loss: 0.21247 acc: 27.350%\n",
            "[42,  30] loss: 0.21144 acc: 27.450%\n",
            "[43,  30] loss: 0.21289 acc: 27.400%\n",
            "[44,  30] loss: 0.20460 acc: 27.350%\n",
            "[45,  30] loss: 0.21049 acc: 27.600%\n",
            "[46,  30] loss: 0.20860 acc: 27.650%\n",
            "[47,  30] loss: 0.21196 acc: 27.500%\n",
            "[48,  30] loss: 0.20136 acc: 28.050%\n",
            "[49,  30] loss: 0.20272 acc: 28.000%\n",
            "[50,  30] loss: 0.20877 acc: 27.350%\n",
            "[51,  30] loss: 0.20056 acc: 28.100%\n",
            "[52,  30] loss: 0.20731 acc: 28.150%\n",
            "[53,  30] loss: 0.20621 acc: 27.550%\n",
            "[54,  30] loss: 0.20368 acc: 27.600%\n",
            "[55,  30] loss: 0.20147 acc: 27.750%\n",
            "[56,  30] loss: 0.20544 acc: 27.850%\n",
            "[57,  30] loss: 0.20907 acc: 27.650%\n",
            "[58,  30] loss: 0.20379 acc: 27.850%\n",
            "[59,  30] loss: 0.20388 acc: 27.750%\n",
            "[60,  30] loss: 0.20200 acc: 27.950%\n",
            "[61,  30] loss: 0.20650 acc: 27.950%\n",
            "[62,  30] loss: 0.19684 acc: 28.050%\n",
            "[63,  30] loss: 0.20737 acc: 27.500%\n",
            "[64,  30] loss: 0.20503 acc: 28.350%\n",
            "[65,  30] loss: 0.20064 acc: 28.050%\n",
            "[66,  30] loss: 0.19639 acc: 28.050%\n",
            "[67,  30] loss: 0.19579 acc: 27.800%\n",
            "[68,  30] loss: 0.19584 acc: 28.050%\n",
            "[69,  30] loss: 0.19567 acc: 28.100%\n",
            "[70,  30] loss: 0.20023 acc: 27.750%\n",
            "[71,  30] loss: 0.20140 acc: 27.650%\n",
            "[72,  30] loss: 0.19619 acc: 27.750%\n",
            "[73,  30] loss: 0.19842 acc: 27.650%\n",
            "[74,  30] loss: 0.19621 acc: 27.700%\n",
            "[75,  30] loss: 0.19276 acc: 27.800%\n",
            "[76,  30] loss: 0.18843 acc: 28.600%\n",
            "[77,  30] loss: 0.18849 acc: 28.550%\n",
            "[78,  30] loss: 0.19671 acc: 27.950%\n",
            "[79,  30] loss: 0.19147 acc: 27.850%\n",
            "[80,  30] loss: 0.19711 acc: 27.950%\n",
            "[81,  30] loss: 0.19038 acc: 28.200%\n",
            "[82,  30] loss: 0.19022 acc: 28.150%\n",
            "[83,  30] loss: 0.19260 acc: 28.100%\n",
            "[84,  30] loss: 0.19140 acc: 28.200%\n",
            "[85,  30] loss: 0.18786 acc: 28.600%\n",
            "[86,  30] loss: 0.19891 acc: 27.750%\n",
            "[87,  30] loss: 0.19570 acc: 27.650%\n",
            "[88,  30] loss: 0.19125 acc: 27.900%\n",
            "[89,  30] loss: 0.19325 acc: 27.950%\n",
            "[90,  30] loss: 0.19221 acc: 28.100%\n",
            "[91,  30] loss: 0.19447 acc: 28.150%\n",
            "[92,  30] loss: 0.18730 acc: 28.700%\n",
            "[93,  30] loss: 0.18914 acc: 28.300%\n",
            "[94,  30] loss: 0.18834 acc: 28.200%\n",
            "[95,  30] loss: 0.18894 acc: 28.250%\n",
            "[96,  30] loss: 0.18981 acc: 27.900%\n",
            "[97,  30] loss: 0.19202 acc: 27.950%\n",
            "[98,  30] loss: 0.18850 acc: 28.200%\n",
            "[99,  30] loss: 0.18789 acc: 28.300%\n",
            "[100,  30] loss: 0.18525 acc: 28.200%\n",
            "[101,  30] loss: 0.19036 acc: 28.000%\n",
            "[102,  30] loss: 0.18675 acc: 27.650%\n",
            "[103,  30] loss: 0.18618 acc: 28.300%\n",
            "[104,  30] loss: 0.18423 acc: 28.600%\n",
            "[105,  30] loss: 0.19107 acc: 27.950%\n",
            "[106,  30] loss: 0.18366 acc: 28.300%\n",
            "[107,  30] loss: 0.19008 acc: 27.800%\n",
            "[108,  30] loss: 0.18462 acc: 28.050%\n",
            "[109,  30] loss: 0.18699 acc: 28.200%\n",
            "[110,  30] loss: 0.18522 acc: 28.000%\n",
            "[111,  30] loss: 0.18496 acc: 28.350%\n",
            "[112,  30] loss: 0.19054 acc: 27.850%\n",
            "[113,  30] loss: 0.19122 acc: 27.750%\n",
            "[114,  30] loss: 0.18396 acc: 28.300%\n",
            "[115,  30] loss: 0.18455 acc: 28.000%\n",
            "[116,  30] loss: 0.18662 acc: 28.100%\n",
            "[117,  30] loss: 0.18432 acc: 28.250%\n",
            "[118,  30] loss: 0.18395 acc: 28.300%\n",
            "[119,  30] loss: 0.18307 acc: 27.900%\n",
            "[120,  30] loss: 0.17843 acc: 28.350%\n",
            "[121,  30] loss: 0.18033 acc: 28.450%\n",
            "[122,  30] loss: 0.18144 acc: 28.300%\n",
            "[123,  30] loss: 0.18214 acc: 28.350%\n",
            "[124,  30] loss: 0.17646 acc: 28.350%\n",
            "[125,  30] loss: 0.17698 acc: 28.250%\n",
            "[126,  30] loss: 0.18372 acc: 28.300%\n",
            "[127,  30] loss: 0.17904 acc: 28.150%\n",
            "[128,  30] loss: 0.17464 acc: 28.350%\n",
            "[129,  30] loss: 0.17777 acc: 28.550%\n",
            "[130,  30] loss: 0.17619 acc: 28.200%\n",
            "[131,  30] loss: 0.17279 acc: 28.050%\n",
            "[132,  30] loss: 0.17464 acc: 28.300%\n",
            "[133,  30] loss: 0.16890 acc: 28.800%\n",
            "[134,  30] loss: 0.18094 acc: 28.400%\n",
            "[135,  30] loss: 0.17452 acc: 28.400%\n",
            "[136,  30] loss: 0.17307 acc: 28.300%\n",
            "[137,  30] loss: 0.17505 acc: 28.500%\n",
            "[138,  30] loss: 0.17546 acc: 28.450%\n",
            "[139,  30] loss: 0.17390 acc: 28.700%\n",
            "[140,  30] loss: 0.17714 acc: 28.250%\n",
            "[141,  30] loss: 0.17623 acc: 28.750%\n",
            "[142,  30] loss: 0.17420 acc: 28.500%\n",
            "[143,  30] loss: 0.17633 acc: 28.500%\n",
            "[144,  30] loss: 0.17160 acc: 28.550%\n",
            "[145,  30] loss: 0.17625 acc: 27.800%\n",
            "[146,  30] loss: 0.16973 acc: 28.700%\n",
            "[147,  30] loss: 0.17435 acc: 28.700%\n",
            "[148,  30] loss: 0.16769 acc: 28.950%\n",
            "[149,  30] loss: 0.17566 acc: 28.150%\n",
            "[150,  30] loss: 0.16853 acc: 28.500%\n",
            "[151,  30] loss: 0.16729 acc: 28.650%\n",
            "[152,  30] loss: 0.18151 acc: 28.300%\n",
            "[153,  30] loss: 0.16380 acc: 28.650%\n",
            "[154,  30] loss: 0.17429 acc: 28.300%\n",
            "[155,  30] loss: 0.17169 acc: 28.700%\n",
            "[156,  30] loss: 0.17168 acc: 28.550%\n",
            "[157,  30] loss: 0.16895 acc: 28.700%\n",
            "[158,  30] loss: 0.17328 acc: 28.350%\n",
            "[159,  30] loss: 0.16370 acc: 29.050%\n",
            "[160,  30] loss: 0.17170 acc: 28.350%\n",
            "[161,  30] loss: 0.16756 acc: 28.350%\n",
            "[162,  30] loss: 0.16650 acc: 28.750%\n",
            "[163,  30] loss: 0.16414 acc: 28.450%\n",
            "[164,  30] loss: 0.16341 acc: 28.650%\n",
            "[165,  30] loss: 0.17051 acc: 28.650%\n",
            "[166,  30] loss: 0.16864 acc: 28.400%\n",
            "[167,  30] loss: 0.16841 acc: 28.500%\n",
            "[168,  30] loss: 0.16861 acc: 28.450%\n",
            "[169,  30] loss: 0.16488 acc: 28.550%\n",
            "[170,  30] loss: 0.16480 acc: 28.550%\n",
            "[171,  30] loss: 0.16700 acc: 28.300%\n",
            "[172,  30] loss: 0.16802 acc: 28.500%\n",
            "[173,  30] loss: 0.16385 acc: 28.900%\n",
            "[174,  30] loss: 0.16719 acc: 28.150%\n",
            "[175,  30] loss: 0.16357 acc: 28.650%\n",
            "[176,  30] loss: 0.17075 acc: 28.200%\n",
            "[177,  30] loss: 0.16745 acc: 28.400%\n",
            "[178,  30] loss: 0.16200 acc: 28.650%\n",
            "[179,  30] loss: 0.15958 acc: 28.650%\n",
            "[180,  30] loss: 0.16400 acc: 28.600%\n",
            "[181,  30] loss: 0.16350 acc: 28.800%\n",
            "[182,  30] loss: 0.15886 acc: 28.800%\n",
            "[183,  30] loss: 0.16145 acc: 28.900%\n",
            "[184,  30] loss: 0.16380 acc: 28.700%\n",
            "[185,  30] loss: 0.16280 acc: 28.700%\n",
            "[186,  30] loss: 0.16344 acc: 28.650%\n",
            "[187,  30] loss: 0.16111 acc: 28.400%\n",
            "[188,  30] loss: 0.15713 acc: 28.600%\n",
            "[189,  30] loss: 0.15469 acc: 29.000%\n",
            "[190,  30] loss: 0.15989 acc: 28.800%\n",
            "[191,  30] loss: 0.16357 acc: 28.650%\n",
            "[192,  30] loss: 0.15608 acc: 28.950%\n",
            "[193,  30] loss: 0.16043 acc: 29.000%\n",
            "[194,  30] loss: 0.15960 acc: 28.900%\n",
            "[195,  30] loss: 0.15663 acc: 28.900%\n",
            "[196,  30] loss: 0.15543 acc: 28.900%\n",
            "[197,  30] loss: 0.15642 acc: 28.900%\n",
            "[198,  30] loss: 0.15761 acc: 28.450%\n",
            "[199,  30] loss: 0.16045 acc: 28.800%\n",
            "[200,  30] loss: 0.15791 acc: 28.950%\n",
            "[201,  30] loss: 0.15598 acc: 28.850%\n",
            "[202,  30] loss: 0.15572 acc: 28.400%\n",
            "[203,  30] loss: 0.15332 acc: 29.050%\n",
            "[204,  30] loss: 0.15392 acc: 28.850%\n",
            "[205,  30] loss: 0.15729 acc: 28.900%\n",
            "[206,  30] loss: 0.15270 acc: 28.700%\n",
            "[207,  30] loss: 0.15476 acc: 28.650%\n",
            "[208,  30] loss: 0.15872 acc: 28.500%\n",
            "[209,  30] loss: 0.15593 acc: 28.850%\n",
            "[210,  30] loss: 0.15009 acc: 28.750%\n",
            "[211,  30] loss: 0.15288 acc: 28.850%\n",
            "[212,  30] loss: 0.15230 acc: 28.950%\n",
            "[213,  30] loss: 0.16229 acc: 28.450%\n",
            "[214,  30] loss: 0.15491 acc: 29.150%\n",
            "[215,  30] loss: 0.15378 acc: 28.750%\n",
            "[216,  30] loss: 0.15761 acc: 28.700%\n",
            "[217,  30] loss: 0.15474 acc: 28.750%\n",
            "[218,  30] loss: 0.15061 acc: 28.700%\n",
            "[219,  30] loss: 0.15580 acc: 28.750%\n",
            "[220,  30] loss: 0.15203 acc: 28.650%\n",
            "[221,  30] loss: 0.14831 acc: 29.000%\n",
            "[222,  30] loss: 0.15232 acc: 28.700%\n",
            "[223,  30] loss: 0.14482 acc: 29.050%\n",
            "[224,  30] loss: 0.15577 acc: 28.800%\n",
            "[225,  30] loss: 0.15023 acc: 28.850%\n",
            "[226,  30] loss: 0.15153 acc: 28.750%\n",
            "[227,  30] loss: 0.14574 acc: 29.200%\n",
            "[228,  30] loss: 0.14384 acc: 28.750%\n",
            "[229,  30] loss: 0.15023 acc: 28.800%\n",
            "[230,  30] loss: 0.14379 acc: 28.700%\n",
            "[231,  30] loss: 0.14764 acc: 28.900%\n",
            "[232,  30] loss: 0.14458 acc: 28.650%\n",
            "[233,  30] loss: 0.14828 acc: 28.700%\n",
            "[234,  30] loss: 0.15085 acc: 28.900%\n",
            "[235,  30] loss: 0.14786 acc: 29.100%\n",
            "[236,  30] loss: 0.14979 acc: 28.750%\n",
            "[237,  30] loss: 0.15056 acc: 28.700%\n",
            "[238,  30] loss: 0.14612 acc: 28.750%\n",
            "[239,  30] loss: 0.14709 acc: 29.200%\n",
            "[240,  30] loss: 0.14418 acc: 28.900%\n",
            "[241,  30] loss: 0.14955 acc: 29.100%\n",
            "[242,  30] loss: 0.14902 acc: 29.000%\n",
            "[243,  30] loss: 0.14396 acc: 29.000%\n",
            "[244,  30] loss: 0.14339 acc: 28.800%\n",
            "[245,  30] loss: 0.15389 acc: 28.550%\n",
            "[246,  30] loss: 0.14902 acc: 28.850%\n",
            "[247,  30] loss: 0.14062 acc: 29.450%\n",
            "[248,  30] loss: 0.14367 acc: 28.900%\n",
            "[249,  30] loss: 0.14165 acc: 29.250%\n",
            "[250,  30] loss: 0.14555 acc: 29.200%\n",
            "[251,  30] loss: 0.14446 acc: 28.850%\n",
            "[252,  30] loss: 0.14059 acc: 29.250%\n",
            "[253,  30] loss: 0.14840 acc: 28.850%\n",
            "[254,  30] loss: 0.14187 acc: 29.200%\n",
            "[255,  30] loss: 0.14618 acc: 28.550%\n",
            "[256,  30] loss: 0.14372 acc: 28.950%\n",
            "[257,  30] loss: 0.13736 acc: 29.200%\n",
            "[258,  30] loss: 0.13845 acc: 28.850%\n",
            "[259,  30] loss: 0.13847 acc: 29.200%\n",
            "[260,  30] loss: 0.14302 acc: 29.000%\n",
            "[261,  30] loss: 0.13973 acc: 29.050%\n",
            "[262,  30] loss: 0.14409 acc: 29.100%\n",
            "[263,  30] loss: 0.14012 acc: 29.100%\n",
            "[264,  30] loss: 0.13681 acc: 29.350%\n",
            "[265,  30] loss: 0.14263 acc: 29.000%\n",
            "[266,  30] loss: 0.14322 acc: 28.950%\n",
            "[267,  30] loss: 0.13893 acc: 29.250%\n",
            "[268,  30] loss: 0.13990 acc: 28.600%\n",
            "[269,  30] loss: 0.14220 acc: 28.900%\n",
            "[270,  30] loss: 0.13886 acc: 29.150%\n",
            "[271,  30] loss: 0.13994 acc: 28.750%\n",
            "[272,  30] loss: 0.13983 acc: 29.300%\n",
            "[273,  30] loss: 0.13956 acc: 29.150%\n",
            "[274,  30] loss: 0.13454 acc: 29.100%\n",
            "[275,  30] loss: 0.13691 acc: 29.000%\n",
            "[276,  30] loss: 0.13886 acc: 29.250%\n",
            "[277,  30] loss: 0.14338 acc: 28.600%\n",
            "[278,  30] loss: 0.14254 acc: 29.100%\n",
            "[279,  30] loss: 0.13175 acc: 29.250%\n",
            "[280,  30] loss: 0.13677 acc: 29.050%\n",
            "[281,  30] loss: 0.13421 acc: 29.050%\n",
            "[282,  30] loss: 0.13432 acc: 29.350%\n",
            "[283,  30] loss: 0.13354 acc: 29.000%\n",
            "[284,  30] loss: 0.13675 acc: 28.950%\n",
            "[285,  30] loss: 0.13095 acc: 29.350%\n",
            "[286,  30] loss: 0.13466 acc: 29.150%\n",
            "[287,  30] loss: 0.13487 acc: 29.150%\n",
            "[288,  30] loss: 0.12874 acc: 29.150%\n",
            "[289,  30] loss: 0.13154 acc: 29.300%\n",
            "[290,  30] loss: 0.13214 acc: 29.200%\n",
            "[291,  30] loss: 0.13427 acc: 29.050%\n",
            "[292,  30] loss: 0.13161 acc: 29.150%\n",
            "[293,  30] loss: 0.13253 acc: 29.100%\n",
            "[294,  30] loss: 0.13166 acc: 29.000%\n",
            "[295,  30] loss: 0.13017 acc: 29.200%\n",
            "[296,  30] loss: 0.13020 acc: 29.350%\n",
            "[297,  30] loss: 0.13434 acc: 29.000%\n",
            "[298,  30] loss: 0.13660 acc: 29.000%\n",
            "[299,  30] loss: 0.13466 acc: 29.250%\n",
            "[300,  30] loss: 0.12701 acc: 29.000%\n",
            "[301,  30] loss: 0.13510 acc: 28.950%\n",
            "[302,  30] loss: 0.13041 acc: 29.050%\n",
            "[303,  30] loss: 0.12905 acc: 29.100%\n",
            "[304,  30] loss: 0.13471 acc: 29.100%\n",
            "[305,  30] loss: 0.13601 acc: 28.950%\n",
            "[306,  30] loss: 0.13306 acc: 29.350%\n",
            "[307,  30] loss: 0.12919 acc: 29.450%\n",
            "[308,  30] loss: 0.12831 acc: 29.250%\n",
            "[309,  30] loss: 0.12915 acc: 29.200%\n",
            "[310,  30] loss: 0.12501 acc: 29.100%\n",
            "[311,  30] loss: 0.13083 acc: 29.050%\n",
            "[312,  30] loss: 0.13433 acc: 28.900%\n",
            "[313,  30] loss: 0.12994 acc: 29.400%\n",
            "[314,  30] loss: 0.12667 acc: 29.400%\n",
            "[315,  30] loss: 0.12252 acc: 29.450%\n",
            "[316,  30] loss: 0.12736 acc: 29.350%\n",
            "[317,  30] loss: 0.13104 acc: 29.400%\n",
            "[318,  30] loss: 0.12808 acc: 29.100%\n",
            "[319,  30] loss: 0.12378 acc: 29.200%\n",
            "[320,  30] loss: 0.13052 acc: 28.900%\n",
            "[321,  30] loss: 0.12569 acc: 29.150%\n",
            "[322,  30] loss: 0.12249 acc: 29.300%\n",
            "[323,  30] loss: 0.12311 acc: 29.300%\n",
            "[324,  30] loss: 0.12535 acc: 29.350%\n",
            "[325,  30] loss: 0.12857 acc: 29.200%\n",
            "[326,  30] loss: 0.12247 acc: 29.150%\n",
            "[327,  30] loss: 0.12728 acc: 29.150%\n",
            "[328,  30] loss: 0.12249 acc: 29.400%\n",
            "[329,  30] loss: 0.12456 acc: 29.250%\n",
            "[330,  30] loss: 0.12961 acc: 29.150%\n",
            "[331,  30] loss: 0.12187 acc: 29.450%\n",
            "[332,  30] loss: 0.12509 acc: 29.200%\n",
            "[333,  30] loss: 0.11991 acc: 29.250%\n",
            "[334,  30] loss: 0.12109 acc: 29.400%\n",
            "[335,  30] loss: 0.12598 acc: 29.300%\n",
            "[336,  30] loss: 0.12682 acc: 29.250%\n",
            "[337,  30] loss: 0.12284 acc: 29.200%\n",
            "[338,  30] loss: 0.12479 acc: 29.200%\n",
            "[339,  30] loss: 0.11912 acc: 29.600%\n",
            "[340,  30] loss: 0.12019 acc: 29.250%\n",
            "[341,  30] loss: 0.12058 acc: 29.300%\n",
            "[342,  30] loss: 0.12189 acc: 29.100%\n",
            "[343,  30] loss: 0.12195 acc: 29.500%\n",
            "[344,  30] loss: 0.12221 acc: 29.350%\n",
            "[345,  30] loss: 0.12156 acc: 29.500%\n",
            "[346,  30] loss: 0.12729 acc: 29.050%\n",
            "[347,  30] loss: 0.12020 acc: 29.350%\n",
            "[348,  30] loss: 0.12155 acc: 29.300%\n",
            "[349,  30] loss: 0.11832 acc: 29.400%\n",
            "[350,  30] loss: 0.12334 acc: 29.300%\n",
            "[351,  30] loss: 0.12360 acc: 29.400%\n",
            "[352,  30] loss: 0.12367 acc: 29.050%\n",
            "[353,  30] loss: 0.12180 acc: 29.150%\n",
            "[354,  30] loss: 0.12317 acc: 29.150%\n",
            "[355,  30] loss: 0.12167 acc: 29.350%\n",
            "[356,  30] loss: 0.12147 acc: 29.100%\n",
            "[357,  30] loss: 0.11764 acc: 29.350%\n",
            "[358,  30] loss: 0.11486 acc: 29.500%\n",
            "[359,  30] loss: 0.11816 acc: 29.250%\n",
            "[360,  30] loss: 0.12342 acc: 29.200%\n",
            "[361,  30] loss: 0.11765 acc: 29.450%\n",
            "[362,  30] loss: 0.12321 acc: 29.250%\n",
            "[363,  30] loss: 0.12150 acc: 29.500%\n",
            "[364,  30] loss: 0.11527 acc: 29.350%\n",
            "[365,  30] loss: 0.11602 acc: 29.500%\n",
            "[366,  30] loss: 0.11428 acc: 29.500%\n",
            "[367,  30] loss: 0.12070 acc: 29.150%\n",
            "[368,  30] loss: 0.11683 acc: 29.150%\n",
            "[369,  30] loss: 0.11192 acc: 29.450%\n",
            "[370,  30] loss: 0.11330 acc: 29.400%\n",
            "[371,  30] loss: 0.11655 acc: 29.450%\n",
            "[372,  30] loss: 0.11639 acc: 29.450%\n",
            "[373,  30] loss: 0.11563 acc: 29.350%\n",
            "[374,  30] loss: 0.11834 acc: 29.500%\n",
            "[375,  30] loss: 0.11388 acc: 29.550%\n",
            "[376,  30] loss: 0.11525 acc: 29.450%\n",
            "[377,  30] loss: 0.11576 acc: 29.550%\n",
            "[378,  30] loss: 0.11622 acc: 29.400%\n",
            "[379,  30] loss: 0.11442 acc: 29.450%\n",
            "[380,  30] loss: 0.11206 acc: 29.400%\n",
            "[381,  30] loss: 0.11009 acc: 29.600%\n",
            "[382,  30] loss: 0.10998 acc: 29.450%\n",
            "[383,  30] loss: 0.11153 acc: 29.250%\n",
            "[384,  30] loss: 0.11695 acc: 29.500%\n",
            "[385,  30] loss: 0.11482 acc: 29.350%\n",
            "[386,  30] loss: 0.11146 acc: 29.500%\n",
            "[387,  30] loss: 0.11266 acc: 29.400%\n",
            "[388,  30] loss: 0.10884 acc: 29.550%\n",
            "[389,  30] loss: 0.11503 acc: 29.150%\n",
            "[390,  30] loss: 0.11568 acc: 29.350%\n",
            "[391,  30] loss: 0.11508 acc: 29.300%\n",
            "[392,  30] loss: 0.11214 acc: 29.350%\n",
            "[393,  30] loss: 0.11363 acc: 29.200%\n",
            "[394,  30] loss: 0.11057 acc: 29.450%\n",
            "[395,  30] loss: 0.11335 acc: 29.400%\n",
            "[396,  30] loss: 0.10809 acc: 29.700%\n",
            "[397,  30] loss: 0.10955 acc: 29.600%\n",
            "[398,  30] loss: 0.11153 acc: 29.350%\n",
            "[399,  30] loss: 0.11167 acc: 29.450%\n",
            "[400,  30] loss: 0.10478 acc: 29.600%\n",
            "[401,  30] loss: 0.11365 acc: 29.450%\n",
            "[402,  30] loss: 0.11025 acc: 29.400%\n",
            "[403,  30] loss: 0.10876 acc: 29.500%\n",
            "[404,  30] loss: 0.11237 acc: 29.200%\n",
            "[405,  30] loss: 0.10982 acc: 29.550%\n",
            "[406,  30] loss: 0.11078 acc: 29.500%\n",
            "[407,  30] loss: 0.11179 acc: 29.450%\n",
            "[408,  30] loss: 0.11121 acc: 29.450%\n",
            "[409,  30] loss: 0.10708 acc: 29.500%\n",
            "[410,  30] loss: 0.10740 acc: 29.350%\n",
            "[411,  30] loss: 0.10889 acc: 29.200%\n",
            "[412,  30] loss: 0.11227 acc: 29.450%\n",
            "[413,  30] loss: 0.10598 acc: 29.500%\n",
            "[414,  30] loss: 0.11225 acc: 29.150%\n",
            "[415,  30] loss: 0.10569 acc: 29.500%\n",
            "[416,  30] loss: 0.10972 acc: 29.400%\n",
            "[417,  30] loss: 0.11173 acc: 29.300%\n",
            "[418,  30] loss: 0.10410 acc: 29.350%\n",
            "[419,  30] loss: 0.10636 acc: 29.400%\n",
            "[420,  30] loss: 0.10770 acc: 29.500%\n",
            "[421,  30] loss: 0.10603 acc: 29.500%\n",
            "[422,  30] loss: 0.10911 acc: 29.500%\n",
            "[423,  30] loss: 0.10361 acc: 29.550%\n",
            "[424,  30] loss: 0.10384 acc: 29.450%\n",
            "[425,  30] loss: 0.10526 acc: 29.500%\n",
            "[426,  30] loss: 0.10385 acc: 29.450%\n",
            "[427,  30] loss: 0.10952 acc: 29.350%\n",
            "[428,  30] loss: 0.10503 acc: 29.500%\n",
            "[429,  30] loss: 0.10297 acc: 29.550%\n",
            "[430,  30] loss: 0.10382 acc: 29.350%\n",
            "[431,  30] loss: 0.10645 acc: 29.350%\n",
            "[432,  30] loss: 0.10719 acc: 29.500%\n",
            "[433,  30] loss: 0.10839 acc: 29.400%\n",
            "[434,  30] loss: 0.09893 acc: 29.500%\n",
            "[435,  30] loss: 0.10758 acc: 29.350%\n",
            "[436,  30] loss: 0.10484 acc: 29.600%\n",
            "[437,  30] loss: 0.10209 acc: 29.550%\n",
            "[438,  30] loss: 0.10470 acc: 29.700%\n",
            "[439,  30] loss: 0.10721 acc: 29.300%\n",
            "[440,  30] loss: 0.10132 acc: 29.450%\n",
            "[441,  30] loss: 0.10191 acc: 29.450%\n",
            "[442,  30] loss: 0.10570 acc: 29.300%\n",
            "[443,  30] loss: 0.10297 acc: 29.550%\n",
            "[444,  30] loss: 0.10092 acc: 29.500%\n",
            "[445,  30] loss: 0.10614 acc: 29.550%\n",
            "[446,  30] loss: 0.09931 acc: 29.650%\n",
            "[447,  30] loss: 0.10436 acc: 29.500%\n",
            "[448,  30] loss: 0.09947 acc: 29.700%\n",
            "[449,  30] loss: 0.10069 acc: 29.450%\n",
            "[450,  30] loss: 0.10426 acc: 29.750%\n",
            "[451,  30] loss: 0.09819 acc: 29.800%\n",
            "[452,  30] loss: 0.10293 acc: 29.500%\n",
            "[453,  30] loss: 0.10192 acc: 29.350%\n",
            "[454,  30] loss: 0.10403 acc: 29.350%\n",
            "[455,  30] loss: 0.09988 acc: 29.500%\n",
            "[456,  30] loss: 0.10046 acc: 29.450%\n",
            "[457,  30] loss: 0.10225 acc: 29.550%\n",
            "[458,  30] loss: 0.09774 acc: 29.750%\n",
            "[459,  30] loss: 0.10270 acc: 29.450%\n",
            "[460,  30] loss: 0.09916 acc: 29.650%\n",
            "[461,  30] loss: 0.10458 acc: 29.500%\n",
            "[462,  30] loss: 0.10200 acc: 29.600%\n",
            "[463,  30] loss: 0.10384 acc: 29.450%\n",
            "[464,  30] loss: 0.10022 acc: 29.450%\n",
            "[465,  30] loss: 0.10256 acc: 29.650%\n",
            "[466,  30] loss: 0.09627 acc: 29.650%\n",
            "[467,  30] loss: 0.10256 acc: 29.350%\n",
            "[468,  30] loss: 0.10065 acc: 29.350%\n",
            "[469,  30] loss: 0.09874 acc: 29.500%\n",
            "[470,  30] loss: 0.09474 acc: 29.650%\n",
            "[471,  30] loss: 0.09835 acc: 29.550%\n",
            "[472,  30] loss: 0.09477 acc: 29.550%\n",
            "[473,  30] loss: 0.09729 acc: 29.650%\n",
            "[474,  30] loss: 0.09918 acc: 29.550%\n",
            "[475,  30] loss: 0.09985 acc: 29.700%\n",
            "[476,  30] loss: 0.09572 acc: 29.700%\n",
            "[477,  30] loss: 0.09562 acc: 29.550%\n",
            "[478,  30] loss: 0.09624 acc: 29.350%\n",
            "[479,  30] loss: 0.09515 acc: 29.450%\n",
            "[480,  30] loss: 0.09435 acc: 29.650%\n",
            "[481,  30] loss: 0.09762 acc: 29.550%\n",
            "[482,  30] loss: 0.09439 acc: 29.650%\n",
            "[483,  30] loss: 0.09692 acc: 29.650%\n",
            "[484,  30] loss: 0.09564 acc: 29.650%\n",
            "[485,  30] loss: 0.09321 acc: 29.500%\n",
            "[486,  30] loss: 0.09588 acc: 29.350%\n",
            "[487,  30] loss: 0.09264 acc: 29.700%\n",
            "[488,  30] loss: 0.09227 acc: 29.600%\n",
            "[489,  30] loss: 0.09679 acc: 29.650%\n",
            "[490,  30] loss: 0.09047 acc: 29.650%\n",
            "[491,  30] loss: 0.09068 acc: 29.800%\n",
            "[492,  30] loss: 0.09589 acc: 29.500%\n",
            "[493,  30] loss: 0.09813 acc: 29.800%\n",
            "[494,  30] loss: 0.09613 acc: 29.500%\n",
            "[495,  30] loss: 0.09726 acc: 29.500%\n",
            "[496,  30] loss: 0.09017 acc: 29.800%\n",
            "[497,  30] loss: 0.09268 acc: 29.650%\n",
            "[498,  30] loss: 0.09622 acc: 29.550%\n",
            "[499,  30] loss: 0.09058 acc: 29.650%\n",
            "[500,  30] loss: 0.09700 acc: 29.450%\n",
            "[501,  30] loss: 0.09218 acc: 29.650%\n",
            "[502,  30] loss: 0.09654 acc: 29.600%\n",
            "[503,  30] loss: 0.09319 acc: 29.650%\n",
            "[504,  30] loss: 0.08819 acc: 29.750%\n",
            "[505,  30] loss: 0.09400 acc: 29.550%\n",
            "[506,  30] loss: 0.09483 acc: 29.600%\n",
            "[507,  30] loss: 0.09146 acc: 29.800%\n",
            "[508,  30] loss: 0.09412 acc: 29.550%\n",
            "[509,  30] loss: 0.09287 acc: 29.600%\n",
            "[510,  30] loss: 0.09583 acc: 29.450%\n",
            "[511,  30] loss: 0.09307 acc: 29.700%\n",
            "[512,  30] loss: 0.09010 acc: 29.650%\n",
            "[513,  30] loss: 0.09574 acc: 29.450%\n",
            "[514,  30] loss: 0.08934 acc: 29.700%\n",
            "[515,  30] loss: 0.09104 acc: 29.650%\n",
            "[516,  30] loss: 0.09358 acc: 29.450%\n",
            "[517,  30] loss: 0.09227 acc: 29.650%\n",
            "[518,  30] loss: 0.08769 acc: 29.800%\n",
            "[519,  30] loss: 0.09012 acc: 29.650%\n",
            "[520,  30] loss: 0.09807 acc: 29.550%\n",
            "[521,  30] loss: 0.09021 acc: 29.700%\n",
            "[522,  30] loss: 0.08880 acc: 29.750%\n",
            "[523,  30] loss: 0.08728 acc: 29.700%\n",
            "[524,  30] loss: 0.09506 acc: 29.550%\n",
            "[525,  30] loss: 0.09006 acc: 29.650%\n",
            "[526,  30] loss: 0.09138 acc: 29.600%\n",
            "[527,  30] loss: 0.09259 acc: 29.550%\n",
            "[528,  30] loss: 0.09094 acc: 29.800%\n",
            "[529,  30] loss: 0.08876 acc: 29.600%\n",
            "[530,  30] loss: 0.08554 acc: 29.700%\n",
            "[531,  30] loss: 0.08787 acc: 29.550%\n",
            "[532,  30] loss: 0.09249 acc: 29.650%\n",
            "[533,  30] loss: 0.08934 acc: 29.650%\n",
            "[534,  30] loss: 0.09211 acc: 29.550%\n",
            "[535,  30] loss: 0.09302 acc: 29.550%\n",
            "[536,  30] loss: 0.08511 acc: 29.850%\n",
            "[537,  30] loss: 0.09032 acc: 29.700%\n",
            "[538,  30] loss: 0.08329 acc: 29.650%\n",
            "[539,  30] loss: 0.09084 acc: 29.550%\n",
            "[540,  30] loss: 0.08972 acc: 29.500%\n",
            "[541,  30] loss: 0.08432 acc: 29.700%\n",
            "[542,  30] loss: 0.08939 acc: 29.650%\n",
            "[543,  30] loss: 0.08697 acc: 29.600%\n",
            "[544,  30] loss: 0.08723 acc: 29.900%\n",
            "[545,  30] loss: 0.08994 acc: 29.750%\n",
            "[546,  30] loss: 0.08901 acc: 29.600%\n",
            "[547,  30] loss: 0.08393 acc: 29.700%\n",
            "[548,  30] loss: 0.09458 acc: 29.150%\n",
            "[549,  30] loss: 0.08569 acc: 29.850%\n",
            "[550,  30] loss: 0.08534 acc: 29.650%\n",
            "[551,  30] loss: 0.08949 acc: 29.650%\n",
            "[552,  30] loss: 0.09115 acc: 29.700%\n",
            "[553,  30] loss: 0.08903 acc: 29.600%\n",
            "[554,  30] loss: 0.08534 acc: 29.800%\n",
            "[555,  30] loss: 0.08872 acc: 29.600%\n",
            "[556,  30] loss: 0.08736 acc: 29.600%\n",
            "[557,  30] loss: 0.09041 acc: 29.450%\n",
            "[558,  30] loss: 0.08976 acc: 29.600%\n",
            "[559,  30] loss: 0.08138 acc: 29.600%\n",
            "[560,  30] loss: 0.08783 acc: 29.600%\n",
            "[561,  30] loss: 0.08136 acc: 29.900%\n",
            "[562,  30] loss: 0.08351 acc: 29.650%\n",
            "[563,  30] loss: 0.08598 acc: 29.650%\n",
            "[564,  30] loss: 0.08290 acc: 29.850%\n",
            "[565,  30] loss: 0.08648 acc: 29.700%\n",
            "[566,  30] loss: 0.08293 acc: 29.700%\n",
            "[567,  30] loss: 0.07834 acc: 29.800%\n",
            "[568,  30] loss: 0.08451 acc: 29.750%\n",
            "[569,  30] loss: 0.08435 acc: 29.800%\n",
            "[570,  30] loss: 0.08440 acc: 29.700%\n",
            "[571,  30] loss: 0.08737 acc: 29.550%\n",
            "[572,  30] loss: 0.08435 acc: 29.700%\n",
            "[573,  30] loss: 0.08644 acc: 29.700%\n",
            "[574,  30] loss: 0.08300 acc: 29.850%\n",
            "[575,  30] loss: 0.08541 acc: 29.550%\n",
            "[576,  30] loss: 0.08056 acc: 29.800%\n",
            "[577,  30] loss: 0.08224 acc: 29.750%\n",
            "[578,  30] loss: 0.08541 acc: 29.550%\n",
            "[579,  30] loss: 0.07923 acc: 29.750%\n",
            "[580,  30] loss: 0.08369 acc: 29.700%\n",
            "[581,  30] loss: 0.07975 acc: 29.700%\n",
            "[582,  30] loss: 0.08596 acc: 29.850%\n",
            "[583,  30] loss: 0.08007 acc: 29.700%\n",
            "[584,  30] loss: 0.08326 acc: 29.800%\n",
            "[585,  30] loss: 0.07825 acc: 29.800%\n",
            "[586,  30] loss: 0.07874 acc: 29.750%\n",
            "[587,  30] loss: 0.08327 acc: 29.800%\n",
            "[588,  30] loss: 0.08484 acc: 29.700%\n",
            "[589,  30] loss: 0.08339 acc: 29.650%\n",
            "[590,  30] loss: 0.08295 acc: 29.700%\n",
            "[591,  30] loss: 0.08188 acc: 29.650%\n",
            "[592,  30] loss: 0.08381 acc: 29.750%\n",
            "[593,  30] loss: 0.08698 acc: 29.600%\n",
            "[594,  30] loss: 0.08137 acc: 29.700%\n",
            "[595,  30] loss: 0.08146 acc: 29.850%\n",
            "[596,  30] loss: 0.08140 acc: 29.600%\n",
            "[597,  30] loss: 0.08334 acc: 29.600%\n",
            "[598,  30] loss: 0.08546 acc: 29.500%\n",
            "[599,  30] loss: 0.07998 acc: 29.700%\n",
            "[600,  30] loss: 0.08276 acc: 29.750%\n",
            "[601,  30] loss: 0.07733 acc: 29.850%\n",
            "[602,  30] loss: 0.07861 acc: 29.800%\n",
            "[603,  30] loss: 0.07936 acc: 29.650%\n",
            "[604,  30] loss: 0.08193 acc: 29.750%\n",
            "[605,  30] loss: 0.08154 acc: 29.500%\n",
            "[606,  30] loss: 0.07885 acc: 29.650%\n",
            "[607,  30] loss: 0.07808 acc: 29.750%\n",
            "[608,  30] loss: 0.08323 acc: 29.650%\n",
            "[609,  30] loss: 0.08007 acc: 29.750%\n",
            "[610,  30] loss: 0.07845 acc: 29.750%\n",
            "[611,  30] loss: 0.07627 acc: 29.850%\n",
            "[612,  30] loss: 0.08189 acc: 29.900%\n",
            "[613,  30] loss: 0.07996 acc: 29.700%\n",
            "[614,  30] loss: 0.07756 acc: 29.850%\n",
            "[615,  30] loss: 0.08172 acc: 29.700%\n",
            "[616,  30] loss: 0.07879 acc: 29.800%\n",
            "[617,  30] loss: 0.08167 acc: 29.600%\n",
            "[618,  30] loss: 0.08194 acc: 29.650%\n",
            "[619,  30] loss: 0.07661 acc: 29.800%\n",
            "[620,  30] loss: 0.08159 acc: 29.800%\n",
            "[621,  30] loss: 0.08029 acc: 29.650%\n",
            "[622,  30] loss: 0.08180 acc: 29.700%\n",
            "[623,  30] loss: 0.07828 acc: 29.600%\n",
            "[624,  30] loss: 0.07736 acc: 29.850%\n",
            "[625,  30] loss: 0.07862 acc: 29.600%\n",
            "[626,  30] loss: 0.08106 acc: 29.900%\n",
            "[627,  30] loss: 0.07617 acc: 29.850%\n",
            "[628,  30] loss: 0.07937 acc: 29.700%\n",
            "[629,  30] loss: 0.08008 acc: 29.750%\n",
            "[630,  30] loss: 0.07730 acc: 29.800%\n",
            "[631,  30] loss: 0.07622 acc: 29.800%\n",
            "[632,  30] loss: 0.08260 acc: 29.550%\n",
            "[633,  30] loss: 0.07810 acc: 29.600%\n",
            "[634,  30] loss: 0.07727 acc: 29.750%\n",
            "[635,  30] loss: 0.08054 acc: 29.700%\n",
            "[636,  30] loss: 0.07540 acc: 29.850%\n",
            "[637,  30] loss: 0.07969 acc: 29.900%\n",
            "[638,  30] loss: 0.07736 acc: 29.600%\n",
            "[639,  30] loss: 0.07867 acc: 29.700%\n",
            "[640,  30] loss: 0.07647 acc: 29.700%\n",
            "[641,  30] loss: 0.07267 acc: 29.850%\n",
            "[642,  30] loss: 0.07492 acc: 29.750%\n",
            "[643,  30] loss: 0.07636 acc: 29.800%\n",
            "[644,  30] loss: 0.07606 acc: 29.700%\n",
            "[645,  30] loss: 0.07523 acc: 29.700%\n",
            "[646,  30] loss: 0.07645 acc: 29.650%\n",
            "[647,  30] loss: 0.07766 acc: 29.800%\n",
            "[648,  30] loss: 0.07427 acc: 29.750%\n",
            "[649,  30] loss: 0.07154 acc: 29.650%\n",
            "[650,  30] loss: 0.07778 acc: 29.650%\n",
            "[651,  30] loss: 0.07405 acc: 29.850%\n",
            "[652,  30] loss: 0.07345 acc: 29.850%\n",
            "[653,  30] loss: 0.07522 acc: 29.800%\n",
            "[654,  30] loss: 0.07211 acc: 29.750%\n",
            "[655,  30] loss: 0.07422 acc: 29.600%\n",
            "[656,  30] loss: 0.07670 acc: 29.750%\n",
            "[657,  30] loss: 0.07258 acc: 29.800%\n",
            "[658,  30] loss: 0.07660 acc: 29.800%\n",
            "[659,  30] loss: 0.07669 acc: 29.650%\n",
            "[660,  30] loss: 0.07265 acc: 29.750%\n",
            "[661,  30] loss: 0.07474 acc: 29.500%\n",
            "[662,  30] loss: 0.07273 acc: 29.700%\n",
            "[663,  30] loss: 0.07271 acc: 29.950%\n",
            "[664,  30] loss: 0.07210 acc: 29.800%\n",
            "[665,  30] loss: 0.07322 acc: 29.750%\n",
            "[666,  30] loss: 0.06978 acc: 29.950%\n",
            "[667,  30] loss: 0.07422 acc: 29.800%\n",
            "[668,  30] loss: 0.07047 acc: 29.750%\n",
            "[669,  30] loss: 0.06902 acc: 29.900%\n",
            "[670,  30] loss: 0.07176 acc: 29.800%\n",
            "[671,  30] loss: 0.07561 acc: 29.750%\n",
            "[672,  30] loss: 0.07247 acc: 29.750%\n",
            "[673,  30] loss: 0.07318 acc: 29.800%\n",
            "[674,  30] loss: 0.07236 acc: 29.750%\n",
            "[675,  30] loss: 0.07243 acc: 29.800%\n",
            "[676,  30] loss: 0.07044 acc: 29.800%\n",
            "[677,  30] loss: 0.07474 acc: 29.650%\n",
            "[678,  30] loss: 0.07169 acc: 29.700%\n",
            "[679,  30] loss: 0.06841 acc: 29.750%\n",
            "[680,  30] loss: 0.07349 acc: 29.800%\n",
            "[681,  30] loss: 0.07198 acc: 29.750%\n",
            "[682,  30] loss: 0.07008 acc: 29.850%\n",
            "[683,  30] loss: 0.06784 acc: 29.900%\n",
            "[684,  30] loss: 0.06885 acc: 29.850%\n",
            "[685,  30] loss: 0.07190 acc: 29.800%\n",
            "[686,  30] loss: 0.06793 acc: 29.850%\n",
            "[687,  30] loss: 0.07088 acc: 29.800%\n",
            "[688,  30] loss: 0.06793 acc: 29.750%\n",
            "[689,  30] loss: 0.06828 acc: 29.850%\n",
            "[690,  30] loss: 0.06835 acc: 29.900%\n",
            "[691,  30] loss: 0.07145 acc: 29.900%\n",
            "[692,  30] loss: 0.07263 acc: 29.750%\n",
            "[693,  30] loss: 0.07201 acc: 29.900%\n",
            "[694,  30] loss: 0.07169 acc: 29.800%\n",
            "[695,  30] loss: 0.06976 acc: 29.700%\n",
            "[696,  30] loss: 0.06933 acc: 29.800%\n",
            "[697,  30] loss: 0.07296 acc: 29.650%\n",
            "[698,  30] loss: 0.07118 acc: 29.700%\n",
            "[699,  30] loss: 0.06861 acc: 29.850%\n",
            "[700,  30] loss: 0.07282 acc: 29.550%\n",
            "[701,  30] loss: 0.07464 acc: 29.700%\n",
            "[702,  30] loss: 0.06820 acc: 29.750%\n",
            "[703,  30] loss: 0.06854 acc: 29.650%\n",
            "[704,  30] loss: 0.07056 acc: 29.850%\n",
            "[705,  30] loss: 0.06595 acc: 29.800%\n",
            "[706,  30] loss: 0.06722 acc: 29.850%\n",
            "[707,  30] loss: 0.06735 acc: 29.750%\n",
            "[708,  30] loss: 0.07022 acc: 29.700%\n",
            "[709,  30] loss: 0.06552 acc: 29.800%\n",
            "[710,  30] loss: 0.06667 acc: 29.800%\n",
            "[711,  30] loss: 0.07076 acc: 29.650%\n",
            "[712,  30] loss: 0.06515 acc: 29.900%\n",
            "[713,  30] loss: 0.07143 acc: 29.650%\n",
            "[714,  30] loss: 0.06922 acc: 29.900%\n",
            "[715,  30] loss: 0.06742 acc: 29.700%\n",
            "[716,  30] loss: 0.06648 acc: 29.900%\n",
            "[717,  30] loss: 0.07080 acc: 29.850%\n",
            "[718,  30] loss: 0.06852 acc: 29.850%\n",
            "[719,  30] loss: 0.07213 acc: 29.700%\n",
            "[720,  30] loss: 0.06886 acc: 29.850%\n",
            "[721,  30] loss: 0.06689 acc: 29.850%\n",
            "[722,  30] loss: 0.06821 acc: 29.800%\n",
            "[723,  30] loss: 0.06642 acc: 29.800%\n",
            "[724,  30] loss: 0.06272 acc: 29.950%\n",
            "[725,  30] loss: 0.06913 acc: 29.850%\n",
            "[726,  30] loss: 0.06749 acc: 29.650%\n",
            "[727,  30] loss: 0.06905 acc: 29.600%\n",
            "[728,  30] loss: 0.06784 acc: 29.800%\n",
            "[729,  30] loss: 0.06867 acc: 29.750%\n",
            "[730,  30] loss: 0.06683 acc: 29.850%\n",
            "[731,  30] loss: 0.06769 acc: 29.950%\n",
            "[732,  30] loss: 0.06869 acc: 29.900%\n",
            "[733,  30] loss: 0.06545 acc: 29.850%\n",
            "[734,  30] loss: 0.06538 acc: 29.950%\n",
            "[735,  30] loss: 0.06623 acc: 29.850%\n",
            "[736,  30] loss: 0.06719 acc: 29.750%\n",
            "[737,  30] loss: 0.06320 acc: 29.850%\n",
            "[738,  30] loss: 0.06952 acc: 29.700%\n",
            "[739,  30] loss: 0.06513 acc: 29.800%\n",
            "[740,  30] loss: 0.06558 acc: 29.750%\n",
            "[741,  30] loss: 0.06457 acc: 29.950%\n",
            "[742,  30] loss: 0.06225 acc: 29.950%\n",
            "[743,  30] loss: 0.06392 acc: 29.650%\n",
            "[744,  30] loss: 0.06451 acc: 29.850%\n",
            "[745,  30] loss: 0.06403 acc: 29.900%\n",
            "[746,  30] loss: 0.07001 acc: 29.700%\n",
            "[747,  30] loss: 0.06474 acc: 29.850%\n",
            "[748,  30] loss: 0.06835 acc: 29.850%\n",
            "[749,  30] loss: 0.06241 acc: 29.900%\n",
            "[750,  30] loss: 0.06310 acc: 29.800%\n",
            "[751,  30] loss: 0.06298 acc: 29.800%\n",
            "[752,  30] loss: 0.06667 acc: 29.800%\n",
            "[753,  30] loss: 0.06927 acc: 29.700%\n",
            "[754,  30] loss: 0.06655 acc: 29.700%\n",
            "[755,  30] loss: 0.06293 acc: 29.950%\n",
            "[756,  30] loss: 0.06669 acc: 29.700%\n",
            "[757,  30] loss: 0.06561 acc: 29.700%\n",
            "[758,  30] loss: 0.06373 acc: 29.850%\n",
            "[759,  30] loss: 0.06371 acc: 29.850%\n",
            "[760,  30] loss: 0.06287 acc: 29.850%\n",
            "[761,  30] loss: 0.06417 acc: 29.900%\n",
            "[762,  30] loss: 0.06614 acc: 29.800%\n",
            "[763,  30] loss: 0.06337 acc: 29.800%\n",
            "[764,  30] loss: 0.06201 acc: 29.750%\n",
            "[765,  30] loss: 0.06033 acc: 29.900%\n",
            "[766,  30] loss: 0.06441 acc: 29.800%\n",
            "[767,  30] loss: 0.06411 acc: 29.800%\n",
            "[768,  30] loss: 0.06367 acc: 29.700%\n",
            "[769,  30] loss: 0.06241 acc: 29.900%\n",
            "[770,  30] loss: 0.06313 acc: 30.000%\n",
            "[771,  30] loss: 0.06296 acc: 29.750%\n",
            "[772,  30] loss: 0.06186 acc: 29.700%\n",
            "[773,  30] loss: 0.06592 acc: 29.650%\n",
            "[774,  30] loss: 0.06437 acc: 29.800%\n",
            "[775,  30] loss: 0.06267 acc: 29.800%\n",
            "[776,  30] loss: 0.06005 acc: 29.900%\n",
            "[777,  30] loss: 0.06355 acc: 29.750%\n",
            "[778,  30] loss: 0.06263 acc: 29.900%\n",
            "[779,  30] loss: 0.06639 acc: 29.800%\n",
            "[780,  30] loss: 0.06402 acc: 29.850%\n",
            "[781,  30] loss: 0.06386 acc: 29.800%\n",
            "[782,  30] loss: 0.06167 acc: 29.950%\n",
            "[783,  30] loss: 0.06230 acc: 29.700%\n",
            "[784,  30] loss: 0.06510 acc: 29.650%\n",
            "[785,  30] loss: 0.06269 acc: 29.900%\n",
            "[786,  30] loss: 0.06332 acc: 29.850%\n",
            "[787,  30] loss: 0.05957 acc: 29.900%\n",
            "[788,  30] loss: 0.06129 acc: 29.850%\n",
            "[789,  30] loss: 0.05860 acc: 29.900%\n",
            "[790,  30] loss: 0.06242 acc: 29.900%\n",
            "[791,  30] loss: 0.06224 acc: 29.750%\n",
            "[792,  30] loss: 0.06102 acc: 29.850%\n",
            "[793,  30] loss: 0.06363 acc: 29.850%\n",
            "[794,  30] loss: 0.06032 acc: 29.900%\n",
            "[795,  30] loss: 0.06483 acc: 29.750%\n",
            "[796,  30] loss: 0.06598 acc: 29.800%\n",
            "[797,  30] loss: 0.06159 acc: 29.750%\n",
            "[798,  30] loss: 0.06220 acc: 29.750%\n",
            "[799,  30] loss: 0.06265 acc: 29.850%\n",
            "[800,  30] loss: 0.06186 acc: 29.850%\n",
            "[801,  30] loss: 0.06195 acc: 29.800%\n",
            "[802,  30] loss: 0.06276 acc: 29.850%\n",
            "[803,  30] loss: 0.06276 acc: 29.700%\n",
            "[804,  30] loss: 0.05982 acc: 29.800%\n",
            "[805,  30] loss: 0.05947 acc: 30.000%\n",
            "[806,  30] loss: 0.06075 acc: 29.800%\n",
            "[807,  30] loss: 0.05821 acc: 29.850%\n",
            "[808,  30] loss: 0.06058 acc: 29.800%\n",
            "[809,  30] loss: 0.05975 acc: 29.900%\n",
            "[810,  30] loss: 0.05894 acc: 29.750%\n",
            "[811,  30] loss: 0.05877 acc: 29.950%\n",
            "[812,  30] loss: 0.05961 acc: 29.950%\n",
            "[813,  30] loss: 0.06313 acc: 29.750%\n",
            "[814,  30] loss: 0.06182 acc: 29.750%\n",
            "[815,  30] loss: 0.06085 acc: 29.900%\n",
            "[816,  30] loss: 0.06257 acc: 29.700%\n",
            "[817,  30] loss: 0.05962 acc: 29.800%\n",
            "[818,  30] loss: 0.06227 acc: 29.800%\n",
            "[819,  30] loss: 0.05610 acc: 30.000%\n",
            "[820,  30] loss: 0.06079 acc: 29.750%\n",
            "[821,  30] loss: 0.06107 acc: 29.850%\n",
            "[822,  30] loss: 0.06032 acc: 29.850%\n",
            "[823,  30] loss: 0.05733 acc: 29.950%\n",
            "[824,  30] loss: 0.06052 acc: 29.800%\n",
            "[825,  30] loss: 0.05836 acc: 29.900%\n",
            "[826,  30] loss: 0.06187 acc: 29.750%\n",
            "[827,  30] loss: 0.05951 acc: 29.850%\n",
            "[828,  30] loss: 0.05996 acc: 29.800%\n",
            "[829,  30] loss: 0.05934 acc: 29.850%\n",
            "[830,  30] loss: 0.05925 acc: 29.800%\n",
            "[831,  30] loss: 0.05885 acc: 29.850%\n",
            "[832,  30] loss: 0.06216 acc: 29.800%\n",
            "[833,  30] loss: 0.05917 acc: 29.800%\n",
            "[834,  30] loss: 0.05499 acc: 29.950%\n",
            "[835,  30] loss: 0.06204 acc: 29.750%\n",
            "[836,  30] loss: 0.06095 acc: 29.750%\n",
            "[837,  30] loss: 0.05803 acc: 29.850%\n",
            "[838,  30] loss: 0.06122 acc: 29.800%\n",
            "[839,  30] loss: 0.05813 acc: 29.700%\n",
            "[840,  30] loss: 0.05945 acc: 29.900%\n",
            "[841,  30] loss: 0.05777 acc: 29.950%\n",
            "[842,  30] loss: 0.05784 acc: 29.900%\n",
            "[843,  30] loss: 0.06005 acc: 29.800%\n",
            "[844,  30] loss: 0.05849 acc: 29.800%\n",
            "[845,  30] loss: 0.05922 acc: 29.800%\n",
            "[846,  30] loss: 0.05457 acc: 29.900%\n",
            "[847,  30] loss: 0.05951 acc: 29.800%\n",
            "[848,  30] loss: 0.05979 acc: 29.750%\n",
            "[849,  30] loss: 0.05930 acc: 29.800%\n",
            "[850,  30] loss: 0.05760 acc: 29.850%\n",
            "[851,  30] loss: 0.05782 acc: 29.850%\n",
            "[852,  30] loss: 0.05793 acc: 29.950%\n",
            "[853,  30] loss: 0.05796 acc: 29.750%\n",
            "[854,  30] loss: 0.05739 acc: 29.700%\n",
            "[855,  30] loss: 0.06002 acc: 29.850%\n",
            "[856,  30] loss: 0.06053 acc: 29.800%\n",
            "[857,  30] loss: 0.05579 acc: 29.850%\n",
            "[858,  30] loss: 0.05838 acc: 29.850%\n",
            "[859,  30] loss: 0.05889 acc: 29.900%\n",
            "[860,  30] loss: 0.05729 acc: 29.850%\n",
            "[861,  30] loss: 0.06000 acc: 29.900%\n",
            "[862,  30] loss: 0.05473 acc: 29.850%\n",
            "[863,  30] loss: 0.05543 acc: 29.850%\n",
            "[864,  30] loss: 0.05535 acc: 29.900%\n",
            "[865,  30] loss: 0.05825 acc: 29.800%\n",
            "[866,  30] loss: 0.05551 acc: 29.950%\n",
            "[867,  30] loss: 0.05894 acc: 29.750%\n",
            "[868,  30] loss: 0.05679 acc: 29.850%\n",
            "[869,  30] loss: 0.05654 acc: 29.800%\n",
            "[870,  30] loss: 0.05796 acc: 29.750%\n",
            "[871,  30] loss: 0.05687 acc: 30.000%\n",
            "[872,  30] loss: 0.05598 acc: 29.850%\n",
            "[873,  30] loss: 0.05826 acc: 29.750%\n",
            "[874,  30] loss: 0.05537 acc: 29.900%\n",
            "[875,  30] loss: 0.05844 acc: 29.950%\n",
            "[876,  30] loss: 0.05482 acc: 29.850%\n",
            "[877,  30] loss: 0.05519 acc: 29.900%\n",
            "[878,  30] loss: 0.05732 acc: 29.800%\n",
            "[879,  30] loss: 0.05553 acc: 30.000%\n",
            "[880,  30] loss: 0.05875 acc: 29.900%\n",
            "[881,  30] loss: 0.05345 acc: 29.850%\n",
            "[882,  30] loss: 0.05584 acc: 29.750%\n",
            "[883,  30] loss: 0.05364 acc: 29.900%\n",
            "[884,  30] loss: 0.05370 acc: 29.850%\n",
            "[885,  30] loss: 0.05393 acc: 29.850%\n",
            "[886,  30] loss: 0.05614 acc: 29.900%\n",
            "[887,  30] loss: 0.05788 acc: 29.750%\n",
            "[888,  30] loss: 0.05460 acc: 29.950%\n",
            "[889,  30] loss: 0.05374 acc: 30.000%\n",
            "[890,  30] loss: 0.05443 acc: 29.950%\n",
            "[891,  30] loss: 0.05303 acc: 29.900%\n",
            "[892,  30] loss: 0.05457 acc: 29.900%\n",
            "[893,  30] loss: 0.05769 acc: 29.700%\n",
            "[894,  30] loss: 0.05183 acc: 29.950%\n",
            "[895,  30] loss: 0.05262 acc: 29.850%\n",
            "[896,  30] loss: 0.05244 acc: 29.900%\n",
            "[897,  30] loss: 0.05259 acc: 29.850%\n",
            "[898,  30] loss: 0.05173 acc: 29.900%\n",
            "[899,  30] loss: 0.05555 acc: 29.800%\n",
            "[900,  30] loss: 0.05779 acc: 29.650%\n",
            "[901,  30] loss: 0.05505 acc: 29.800%\n",
            "[902,  30] loss: 0.05551 acc: 29.900%\n",
            "[903,  30] loss: 0.05060 acc: 29.900%\n",
            "[904,  30] loss: 0.05291 acc: 29.900%\n",
            "[905,  30] loss: 0.05414 acc: 29.850%\n",
            "[906,  30] loss: 0.05209 acc: 29.850%\n",
            "[907,  30] loss: 0.05628 acc: 29.900%\n",
            "[908,  30] loss: 0.05612 acc: 29.800%\n",
            "[909,  30] loss: 0.05146 acc: 29.900%\n",
            "[910,  30] loss: 0.05211 acc: 29.900%\n",
            "[911,  30] loss: 0.05267 acc: 29.950%\n",
            "[912,  30] loss: 0.05224 acc: 29.800%\n",
            "[913,  30] loss: 0.05082 acc: 29.950%\n",
            "[914,  30] loss: 0.05368 acc: 29.950%\n",
            "[915,  30] loss: 0.05329 acc: 29.900%\n",
            "[916,  30] loss: 0.05360 acc: 29.900%\n",
            "[917,  30] loss: 0.05739 acc: 29.950%\n",
            "[918,  30] loss: 0.05212 acc: 29.900%\n",
            "[919,  30] loss: 0.05377 acc: 29.900%\n",
            "[920,  30] loss: 0.05471 acc: 29.850%\n",
            "[921,  30] loss: 0.05522 acc: 29.700%\n",
            "[922,  30] loss: 0.05053 acc: 29.850%\n",
            "[923,  30] loss: 0.05278 acc: 29.800%\n",
            "[924,  30] loss: 0.04958 acc: 29.950%\n",
            "[925,  30] loss: 0.05148 acc: 29.800%\n",
            "[926,  30] loss: 0.05380 acc: 29.750%\n",
            "[927,  30] loss: 0.05041 acc: 29.900%\n",
            "[928,  30] loss: 0.05124 acc: 30.000%\n",
            "[929,  30] loss: 0.05576 acc: 29.800%\n",
            "[930,  30] loss: 0.05445 acc: 29.850%\n",
            "[931,  30] loss: 0.05345 acc: 29.900%\n",
            "[932,  30] loss: 0.05132 acc: 29.950%\n",
            "[933,  30] loss: 0.05175 acc: 29.800%\n",
            "[934,  30] loss: 0.05174 acc: 29.950%\n",
            "[935,  30] loss: 0.05074 acc: 29.850%\n",
            "[936,  30] loss: 0.05185 acc: 29.900%\n",
            "[937,  30] loss: 0.05041 acc: 29.900%\n",
            "[938,  30] loss: 0.05654 acc: 29.800%\n",
            "[939,  30] loss: 0.05327 acc: 29.850%\n",
            "[940,  30] loss: 0.04911 acc: 29.900%\n",
            "[941,  30] loss: 0.04847 acc: 29.950%\n",
            "[942,  30] loss: 0.05506 acc: 29.750%\n",
            "[943,  30] loss: 0.05326 acc: 29.800%\n",
            "[944,  30] loss: 0.05094 acc: 29.900%\n",
            "[945,  30] loss: 0.05151 acc: 29.800%\n",
            "[946,  30] loss: 0.04915 acc: 29.900%\n",
            "[947,  30] loss: 0.04944 acc: 29.850%\n",
            "[948,  30] loss: 0.05039 acc: 29.750%\n",
            "[949,  30] loss: 0.04964 acc: 30.000%\n",
            "[950,  30] loss: 0.05147 acc: 29.950%\n",
            "[951,  30] loss: 0.04832 acc: 29.950%\n",
            "[952,  30] loss: 0.04998 acc: 29.850%\n",
            "[953,  30] loss: 0.05607 acc: 29.850%\n",
            "[954,  30] loss: 0.04788 acc: 29.900%\n",
            "[955,  30] loss: 0.05338 acc: 29.900%\n",
            "[956,  30] loss: 0.05092 acc: 29.850%\n",
            "[957,  30] loss: 0.04937 acc: 29.850%\n",
            "[958,  30] loss: 0.04910 acc: 29.900%\n",
            "[959,  30] loss: 0.05506 acc: 29.900%\n",
            "[960,  30] loss: 0.04818 acc: 29.950%\n",
            "[961,  30] loss: 0.05282 acc: 29.750%\n",
            "[962,  30] loss: 0.04849 acc: 29.900%\n",
            "[963,  30] loss: 0.04884 acc: 29.950%\n",
            "[964,  30] loss: 0.04888 acc: 30.000%\n",
            "[965,  30] loss: 0.05125 acc: 29.850%\n",
            "[966,  30] loss: 0.05230 acc: 29.800%\n",
            "[967,  30] loss: 0.05233 acc: 29.800%\n",
            "[968,  30] loss: 0.04775 acc: 29.850%\n",
            "[969,  30] loss: 0.04725 acc: 29.900%\n",
            "[970,  30] loss: 0.04622 acc: 29.950%\n",
            "[971,  30] loss: 0.04709 acc: 29.900%\n",
            "[972,  30] loss: 0.05085 acc: 29.900%\n",
            "[973,  30] loss: 0.04847 acc: 29.900%\n",
            "[974,  30] loss: 0.05085 acc: 29.750%\n",
            "[975,  30] loss: 0.05141 acc: 29.750%\n",
            "[976,  30] loss: 0.04757 acc: 29.850%\n",
            "[977,  30] loss: 0.04817 acc: 30.000%\n",
            "[978,  30] loss: 0.05003 acc: 29.900%\n",
            "[979,  30] loss: 0.04897 acc: 29.850%\n",
            "[980,  30] loss: 0.04609 acc: 30.000%\n",
            "[981,  30] loss: 0.04810 acc: 29.950%\n",
            "[982,  30] loss: 0.04907 acc: 29.800%\n",
            "[983,  30] loss: 0.05099 acc: 29.800%\n",
            "[984,  30] loss: 0.04925 acc: 29.950%\n",
            "[985,  30] loss: 0.05107 acc: 29.850%\n",
            "[986,  30] loss: 0.04899 acc: 29.850%\n",
            "[987,  30] loss: 0.04897 acc: 29.850%\n",
            "[988,  30] loss: 0.04608 acc: 30.000%\n",
            "[989,  30] loss: 0.04919 acc: 29.900%\n",
            "[990,  30] loss: 0.04936 acc: 29.800%\n",
            "[991,  30] loss: 0.05006 acc: 29.850%\n",
            "[992,  30] loss: 0.04990 acc: 29.900%\n",
            "[993,  30] loss: 0.04843 acc: 29.900%\n",
            "[994,  30] loss: 0.04722 acc: 29.850%\n",
            "[995,  30] loss: 0.04681 acc: 29.850%\n",
            "[996,  30] loss: 0.05155 acc: 29.800%\n",
            "[997,  30] loss: 0.04521 acc: 29.800%\n",
            "[998,  30] loss: 0.05171 acc: 29.800%\n",
            "[999,  30] loss: 0.04618 acc: 29.850%\n",
            "[1000,  30] loss: 0.04840 acc: 29.900%\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "gq5uVp9c6OTy",
        "outputId": "529325d7-263e-40e6-f99c-72c8aa7c4760"
      },
      "source": [
        "captchaTestAcc = [i*10 for i in captchaTestAcc]\n",
        "captchaTestAcc = captchaTestAcc[1000:]\n",
        "captchaCornerTestAcc = [i*10 for i in captchaCornerTestAcc]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(captchaTestAcc, color=\"tab:orange\", label=\"Raw Data\")\n",
        "ax.plot(captchaCornerTestAcc, color=\"tab:blue\", label=\"Corner Detection\")\n",
        "#ax.set_ylim([0,100])\n",
        "ax.set_title(\"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}