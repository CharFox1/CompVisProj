{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHPVnsLay1QwY921JpLZkZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharFox1/CompVisProj/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL7yYumqgmQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiHcbcP4GxpL",
        "outputId": "8c5bc96e-2043-453a-dbd5-f715a913816b"
      },
      "source": [
        "!unzip handwrittenChars.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  handwrittenChars.zip\n",
            "   creating: handwrittenChars/\n",
            "   creating: handwrittenChars/.ipynb_checkpoints/\n",
            "  inflating: handwrittenChars/.ipynb_checkpoints/parseHandwrittenChars-checkpoint.ipynb  \n",
            "  inflating: handwrittenChars/parseHandwrittenChars.ipynb  \n",
            "  inflating: handwrittenChars/trainSmall.npy  \n",
            "  inflating: handwrittenChars/valSmall.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNYNEbHSiy2G",
        "outputId": "671285bb-edb4-4c79-8599-9f887dea11d2"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7d79S884AM"
      },
      "source": [
        "# resnet block to be used in models below\n",
        "# code modified from \"resnet-34-pytorch-starter-kit\"\n",
        "\n",
        "class resBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False):\n",
        "    super(resBlock, self).__init__()\n",
        "    \n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.cnn2 = nn.Sequential(\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    # if the output image will be a different size than the input\n",
        "    # must reshape residual to fit new output shape\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    # otherwise just pass it through \n",
        "    else:\n",
        "      self.shortcut = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.cnn1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x += self.shortcut(residual)\n",
        "    x = nn.ReLU(True)(x)\n",
        "    return x\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QTU6t3eT2M4"
      },
      "source": [
        "# simple conv network to test finding single characters\n",
        "# uses resnet structure\n",
        "\n",
        "class convDemoNet(nn.Module):\n",
        "  def __init__(self, numClasses):\n",
        "    super(convDemoNet, self).__init__()\n",
        "    \n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=2, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.MaxPool2d(1, 1),\n",
        "        resBlock(64, 64),\n",
        "        resBlock(64, 64, 2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        resBlock(64, 128),\n",
        "        resBlock(128, 128, 2)\n",
        "    )\n",
        "\n",
        "    self.block4 = nn.Sequential(\n",
        "        resBlock(128, 256),\n",
        "        resBlock(256, 256, 2)\n",
        "    )\n",
        "\n",
        "    self.block5 = nn.Sequential(\n",
        "        resBlock(256, 512),\n",
        "        resBlock(512, 512, 2)\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AvgPool2d(2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.l1 = nn.Linear(512, 512)\n",
        "    self.l2 = nn.Linear(512, 256)\n",
        "    self.l3 = nn.Linear(256, numClasses)\n",
        "    #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.l3(x)\n",
        "    return x"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_U9YSOskQfU"
      },
      "source": [
        "# small function to turn int index into one hot encoding\n",
        "def oneHot(num, numClasses):\n",
        "  output = [0] * numClasses\n",
        "  output[num] = 1\n",
        "  return output"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "w4mYNjCXUW_I",
        "outputId": "e1b57e69-acd6-4dbf-bd9d-cc0e549a3484"
      },
      "source": [
        "# get conv demo data (https://www.kaggle.com/vaibhao/handwritten-characters)\n",
        "from google.colab.patches import cv2_imshow #allows us to show images\n",
        "\n",
        "# grab small files (created from larger dataset)\n",
        "# this is the location they should be in the github\n",
        "# if you are running in collab, you need to import the handwrittenChars folder as a zip\n",
        "# you can unzip it with \"!unzip handwrittenChars.zip\" in a separate cell\n",
        "with open(\"handwrittenChars/trainSmall.npy\", \"rb\") as f:\n",
        "    conv_train_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "with open(\"handwrittenChars/valSmall.npy\", \"rb\") as f:\n",
        "    conv_val_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "print(\"training data size:\", len(conv_train_data))\n",
        "print(\"validation data size:\", len(conv_val_data))\n",
        "\n",
        "print(\"training data shape:\", conv_train_data[1201][0].shape)\n",
        "cv2_imshow(conv_train_data[1201][0])\n",
        "print(\"data type of image =\", type(conv_train_data[1201][0]))\n",
        "print(\"training data label:\", conv_train_data[1201][1])\n",
        "print(\"each index in dataset has image (32x32) and char label\")\n",
        "print(conv_train_data[1201][0])"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 78000\n",
            "validation data size: 13209\n",
            "training data shape: (32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABVElEQVR4nNWRsUtCURTGf/WiEHk9FwepIRV6EDx4W6O4uDjkGI36B+gQEbQ1NJgQIjQ0OBUEDkXRUpDSH5CgU4O9hpwcnlJIEXEaLPXRda9vufec853vnvNd+LcIxSdVfLbtA9bqw8y0l2DW6ybM+ZlEGCB9NLrPqAizfrMOFE5+l2wRm0xDRET2omqFzU7Mol2Ay5ZC3BYRqdVktIVHIWotQbVXjixXVaOF7byIiA3ZM4Bw0Kugl5LvXU0fBJoOpZsi4z5cJSiHk9/BiuM4ifEZzFPMrVqn+zII45UA6w/t0fOrtyK5EGBsvNpkGvKYShlj48UunnO5IAAB9/jgXsbWBCA3TAS3+yofWufOjw+71x+UIzGVDbCQl/6dDmR/O6npwE6624zNa5+qZst1XfftMKAbT5ZaIQDsF7tmZVHztk4NDiMONFsYcao9olZP+Vt/Fl8w9HTyIRS5+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FB72002B210>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data type of image = <class 'numpy.ndarray'>\n",
            "training data label: #\n",
            "each index in dataset has image (32x32) and char label\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD8KefvjlQUq",
        "outputId": "d560b548-4757-4922-89a9-c9577f8722c9"
      },
      "source": [
        "labels = []\n",
        "for i in conv_train_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the training dataset\")\n",
        "\n",
        "for i in conv_val_data:\n",
        "  label = i[1]\n",
        "  if label not in labels:\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"there are\", len(labels), \"labels in the validation dataset\")\n",
        "\n",
        "labelDict = {}\n",
        "for i in range(len(labels)):\n",
        "  labelDict[i] = labels[i]\n",
        "\n",
        "print(labelDict)\n",
        "invertedLabelDict = {y:x for x,y in labelDict.items()}\n",
        "print(invertedLabelDict)"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 39 labels in the training dataset\n",
            "there are 39 labels in the validation dataset\n",
            "{0: '#', 1: '$', 2: '&', 3: '0', 4: '1', 5: '2', 6: '3', 7: '4', 8: '5', 9: '6', 10: '7', 11: '8', 12: '9', 13: '@', 14: 'A', 15: 'B', 16: 'C', 17: 'D', 18: 'E', 19: 'F', 20: 'G', 21: 'H', 22: 'I', 23: 'J', 24: 'K', 25: 'L', 26: 'M', 27: 'N', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z'}\n",
            "{'#': 0, '$': 1, '&': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '@': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNPEtqWcwx7U"
      },
      "source": [
        "# dataset class\n",
        "class handwrittenCharsDataset(Dataset):\n",
        "    def __init__(self, X, classToNum):\n",
        "      self.classToNum = classToNum\n",
        "      self.images = []\n",
        "      self.labels = []\n",
        "      for i in X:\n",
        "        self.images.append(i[0])\n",
        "        self.labels.append(i[1])\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      label = tensor(self.classToNum[self.labels[index]])\n",
        "      #label = tensor(oneHot(labelNum, 39))\n",
        "      image = self.transform(image)\n",
        "      sample = [image, label]\n",
        "      return sample\n",
        "    \n",
        "    transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.ToTensor()                     \n",
        "    ])"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GxpW2tg1X-9"
      },
      "source": [
        "batch_size = 10\n",
        "dataset = handwrittenCharsDataset(X=conv_train_data, classToNum=invertedLabelDict)\n",
        "train_dl = DataLoader(dataset, batch_size, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "It_9ZFvtaHMU",
        "outputId": "9c9f5aa1-411d-49db-eff1-e0a328e7d479"
      },
      "source": [
        "# init model with 39 classes on output layer\n",
        "# put model in gpu if available\n",
        "convModel = convDemoNet(39).to(device)\n",
        "#print(convModel)\n",
        "\n",
        "testItem, testLabel = next(iter(train_dl))\n",
        "print(testLabel)\n",
        "print(f\"Feature batch shape: {testItem.size()}\")\n",
        "print(f\"Labels batch shape: {testLabel.size()}\")\n",
        "\n",
        "testItem = testItem[0].unsqueeze(0).to(device)\n",
        "#testLabel = labelDict[testLabel[0].numpy().argmax()]\n",
        "testLabel = labelDict[testLabel.numpy()[0]]\n",
        "\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "\n",
        "output = convModel(testItem)\n",
        "y_pred = labelDict[output.argmax(1).item()]\n",
        "print(\"predicted\", y_pred, \"for\", testLabel)"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([28, 25,  0,  0, 20, 36,  2, 18,  5,  4])\n",
            "Feature batch shape: torch.Size([10, 1, 32, 32])\n",
            "Labels batch shape: torch.Size([10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACF0lEQVR4nHWTP2tVQRDFfzOz18Rg/IfPBDFpRIwoRCQggk2MTRAbG1FbQcXCRr+A+gFMoY1E0ELtEoJFEMUgFqKkSCEBKxUDBt+LAcWQ3Ls7FvflmeT5hl3Y3Tln5xyYAZRyCyii5QVABAQDAdCAgQoiaAaCCEGgngdCnYIAGQYBBEXUEAzZbLQhIKIZSoNd1yADMw96rLfv4IG+HhQCVkozxAzCwOsifn73y5e9mHl8e2gbAKYQoNJ76s5kzb1wj9FjSimvnt2C1R1kx0eOkESi4lQ/QpJDXYln1xYBA9HR5DEtv7g1WsTasGLQfXne3/cIUhp6GGP1+ol2Ti744jkzDPbPpnRJAEFD/xevnjbZfvH+UpHGNyFh15XZP55fCHWblcli+c0x9k67u889n5gYn8+Tp7eV0qYa/SO19GGoY3g25ck9T57SSpwezFBBQJWtNz7FmX3seemFu8foae5uN4iCiLhJ9MNXf9xbiINPuqZudp7pLF5N1chyQFMpIwAicvSrj7WVZVVRASHgQAG4mLsHgLS6cAKr3YEXguRsCG2cXOV89z/CajQenEQl25heA0A9FELREqCJvMPZ1BKQMFbEW4sUiCDW2gUxfFsiCqpgzQAXK8ZqqbMdTyKp2Y6g7HiU/x7O6vOwUaQj9vPpzt3fc/EC8eYvDDALsE5Ck1oxWzfAayIr569h+j8VygYQwdbw/wJSOMXmVfVnSgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FB72BC088D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted P for P\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjWjzRPPi7hY",
        "outputId": "77fb8404-6525-46c1-d1f5-070507b454aa"
      },
      "source": [
        "import torch.optim as optim\n",
        "lossFunc = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(convModel.parameters(), lr=0.001) \n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  running_acc = 0\n",
        "  for i, data in enumerate(train_dl, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "    opt.zero_grad()\n",
        "\n",
        "    outputs = convModel(images)\n",
        "\n",
        "    loss = lossFunc(outputs, labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    running_acc += torch.sum(outputs.argmax(1)==labels)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      print(\"[%d, %5d] loss: %.5f acc: %.3f\" % (epoch + 1, i + 1, running_loss / 1000, running_acc / 1000))\n",
        "      running_loss = 0\n",
        "      running_acc = 0\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 0.067 acc: 9.735001\n",
            "[1,  2000] loss: 0.073 acc: 9.721001\n",
            "[1,  3000] loss: 0.080 acc: 9.669001\n",
            "[1,  4000] loss: 0.072 acc: 9.694000\n",
            "[1,  5000] loss: 0.077 acc: 9.691000\n",
            "[1,  6000] loss: 0.076 acc: 9.706000\n",
            "[1,  7000] loss: 0.078 acc: 9.687000\n",
            "[2,  1000] loss: 0.072 acc: 9.713000\n",
            "[2,  2000] loss: 0.066 acc: 9.737000\n",
            "[2,  3000] loss: 0.073 acc: 9.715000\n",
            "[2,  4000] loss: 0.073 acc: 9.694000\n",
            "[2,  5000] loss: 0.071 acc: 9.710000\n",
            "[2,  6000] loss: 0.074 acc: 9.687000\n",
            "[2,  7000] loss: 0.078 acc: 9.689000\n",
            "[3,  1000] loss: 0.065 acc: 9.726001\n",
            "[3,  2000] loss: 0.067 acc: 9.719001\n",
            "[3,  3000] loss: 0.068 acc: 9.713000\n",
            "[3,  4000] loss: 0.070 acc: 9.708000\n",
            "[3,  5000] loss: 0.069 acc: 9.714001\n",
            "[3,  6000] loss: 0.062 acc: 9.749001\n",
            "[3,  7000] loss: 0.068 acc: 9.732000\n",
            "[4,  1000] loss: 0.063 acc: 9.754001\n",
            "[4,  2000] loss: 0.066 acc: 9.743000\n",
            "[4,  3000] loss: 0.068 acc: 9.723001\n",
            "[4,  4000] loss: 0.070 acc: 9.718000\n",
            "[4,  5000] loss: 0.068 acc: 9.723001\n",
            "[4,  6000] loss: 0.067 acc: 9.732000\n",
            "[4,  7000] loss: 0.068 acc: 9.717000\n",
            "[5,  1000] loss: 0.060 acc: 9.765000\n",
            "[5,  2000] loss: 0.063 acc: 9.750000\n",
            "[5,  3000] loss: 0.063 acc: 9.744000\n",
            "[5,  4000] loss: 0.062 acc: 9.749001\n",
            "[5,  5000] loss: 0.067 acc: 9.730000\n",
            "[5,  6000] loss: 0.066 acc: 9.735001\n",
            "[5,  7000] loss: 0.065 acc: 9.745001\n",
            "[6,  1000] loss: 0.057 acc: 9.752001\n",
            "[6,  2000] loss: 0.062 acc: 9.752001\n",
            "[6,  3000] loss: 0.056 acc: 9.771001\n",
            "[6,  4000] loss: 0.062 acc: 9.737000\n",
            "[6,  5000] loss: 0.068 acc: 9.720000\n",
            "[6,  6000] loss: 0.053 acc: 9.793000\n",
            "[6,  7000] loss: 0.060 acc: 9.766001\n",
            "[7,  1000] loss: 0.051 acc: 9.812000\n",
            "[7,  2000] loss: 0.052 acc: 9.784000\n",
            "[7,  3000] loss: 0.059 acc: 9.758000\n",
            "[7,  4000] loss: 0.058 acc: 9.753000\n",
            "[7,  5000] loss: 0.056 acc: 9.786000\n",
            "[7,  6000] loss: 0.058 acc: 9.770000\n",
            "[7,  7000] loss: 0.058 acc: 9.756001\n",
            "[8,  1000] loss: 0.055 acc: 9.766001\n",
            "[8,  2000] loss: 0.049 acc: 9.807000\n",
            "[8,  3000] loss: 0.053 acc: 9.783000\n",
            "[8,  4000] loss: 0.057 acc: 9.761001\n",
            "[8,  5000] loss: 0.062 acc: 9.762000\n",
            "[8,  6000] loss: 0.055 acc: 9.795000\n",
            "[8,  7000] loss: 0.057 acc: 9.774000\n",
            "[9,  1000] loss: 0.054 acc: 9.778001\n",
            "[9,  2000] loss: 0.048 acc: 9.806001\n",
            "[9,  3000] loss: 0.054 acc: 9.786000\n",
            "[9,  4000] loss: 0.055 acc: 9.790001\n",
            "[9,  5000] loss: 0.056 acc: 9.768001\n",
            "[9,  6000] loss: 0.057 acc: 9.762000\n",
            "[9,  7000] loss: 0.052 acc: 9.791000\n",
            "[10,  1000] loss: 0.045 acc: 9.830001\n",
            "[10,  2000] loss: 0.050 acc: 9.808001\n",
            "[10,  3000] loss: 0.048 acc: 9.811001\n",
            "[10,  4000] loss: 0.048 acc: 9.819000\n",
            "[10,  5000] loss: 0.054 acc: 9.789001\n",
            "[10,  6000] loss: 0.049 acc: 9.805000\n",
            "[10,  7000] loss: 0.051 acc: 9.808001\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "oVZg709ZLzSj",
        "outputId": "4c90dacf-e9b5-47f8-b857-e3f1cab9f0b1"
      },
      "source": [
        "testItem, testLabel = next(iter(train_dl))\n",
        "\n",
        "testItem = testItem[0].unsqueeze(0).to(device)\n",
        "testLabel = labelDict[testLabel[0].item()]\n",
        "\n",
        "output = convModel(testItem)\n",
        "# some funny business to get image from tensor to see if guess is reasonable\n",
        "image = testItem[0].cpu().numpy()[0] * 255\n",
        "cv2_imshow(image)\n",
        "y_pred = labelDict[output.argmax(1).item()]\n",
        "print(\"predicted\", y_pred, \"for\", testLabel)"
      ],
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACSklEQVR4nGWTu2tVaRTFf2vvcy5JdBQNCoIRySAipEkzMoiFFiIIvmF0YDpFtBStRAthQLDQwj9gLBRfhYUMjKKFjShIFAWLUdQwyBhf8YGJuff7tsU596EuOJxis/de31prww8QAgMJUG/BBMIx5GCAvKfuGGpgmIm6/3s0hNGg6qao/x0YC/fcuL8TAY63CXUYCH6JmBhF4IChilCNMHkjGFxtUZIovOy3UO5OcGzwXLQeDztIfSuPjT3f1VligMrybEr//YooGuuuf8kxMeKOQKqW2vKXEeNLMZu3710zYubefCjaQ4TZwVbO5+ey9Oh4zER+u3eRU3ZXSNtepLj288ipN6kV6f+Tw4asy6Bg4GYznq3cPB4p8vSlJRRI8qouKHzB00gfJlspN+PuiSEzpFomgcSmuylFyjkmHq4tS9XSSoYQElciRzNyntgwF6stV22GG2x8E82pW2ee5DR9dg6Y2tOFCdS42GxO/d5ng3++SjNjw0KComOV9R39HDN/9LmL/TnyNq/fV5tmLHkR6f2KAkoWH5/MU0eq53ubRHE4Uux2IUR5IlqnZ3VC4JjrwKdoXl0mChwYmY6Xy5BcklUk7kR69JO1TRn462OcmW3dGNnoZMTlkqLSVHAhT22dv355WREQh+LLv6t60709Wq/HP+7q5HHH7X+GeuPrW1KkT7/JqewWAu9kz9Dg3xGnqmxXU8y8N9746K0Ha+hv1+2b+0NgMsPAKikFqrPX9kfgblDUvV5/FQp1b9ZrLaxnSXWR1r5LfQUPJrsn0hMEogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7FB6AEB7EDD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted K for K\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}